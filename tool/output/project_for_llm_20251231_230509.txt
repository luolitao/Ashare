# Project dump for LLM
# Root: D:\PythonProjects25-12\AShare
# Total files: 69


================================================================================
FILE: AGENTS.md
================================================================================

﻿# Repository Guidelines

## Project Structure and Module Organization
- `ashare/` contains core Python modules (data fetchers, strategies, monitoring, DB schema). Entry points and strategy runners import from here.
- `run_*.py` scripts run individual workflows such as `run_ma5_ma20_trend_strategy.py` or `run_open_monitor.py`.
- `start.py` runs the end-to-end pipeline.
- `config.yaml` holds environment, database, and proxy settings.
- `tool/` contains utility scripts for exporting and network checks (ad-hoc tests).

## Build, Test, and Development Commands
- Install dependencies: `pip install -r requirements.txt`
- Run full pipeline: `python start.py`
- Run a single module: `python run_ma5_ma20_trend_strategy.py` (swap in other `run_*.py` files)
- Open monitor scheduler: `python run_open_monitor_scheduler.py --interval 5`
- Network checks: `python tool/test_baostock_network.py`, `python tool/test_akshare_network.py`

## Coding Style and Naming Conventions
- Python style: 4-space indentation, PEP 8 naming, `snake_case` for functions/variables, `CamelCase` for classes.
- Module names are lowercase with underscores, matching the `ashare/` package.
- Keep configs in `config.yaml` and avoid hardcoding credentials.

## Testing Guidelines
- Run unit tests with `pytest` (install `requirements-dev.txt` first). Use `pytest -m requires_db` to include DB-backed tests.
- DB tests require real MySQL credentials via `MYSQL_HOST`, `MYSQL_USER`, and optional `MYSQL_PASSWORD` / `MYSQL_DB_NAME`.
- Network checks still use `tool/test_*.py` scripts for Baostock/AkShare connectivity.

## Commit and Pull Request Guidelines
- Commit messages are short and descriptive; type prefixes like `feat:` appear in history. Use a concise summary of the change.
- PRs should include a clear description, linked issues (if any), and expected behavior changes. Add screenshots/log snippets for monitoring or strategy output changes.

## Configuration and Data Notes
- `config.yaml` controls database connectivity and optional proxy settings. Keep local secrets out of version control.
- The system writes logs to `ashare.log`; include relevant excerpts when reporting issues.

================================================================================
FILE: ashare/__init__.py
================================================================================

"""A 股数据获取工具包（基于 Baostock）。"""

from .app import AshareApp
from .baostock_core import BaostockDataFetcher
from .baostock_session import BaostockSession
from .universe import AshareUniverseBuilder

__all__ = [
    "AshareApp",
    "AshareUniverseBuilder",
    "BaostockDataFetcher",
    "BaostockSession",
]

================================================================================
FILE: ashare/akshare_fetcher.py
================================================================================

"""Akshare 数据访问封装，用于行为证据信号采集."""

from __future__ import annotations

from typing import Any, Iterable

try:
    import akshare as ak
except ImportError:  # pragma: no cover - 环境未安装 akshare 时延迟失败
    ak = None

import pandas as pd


class AkshareDataFetcher:
    """封装常用的 Akshare 行为证据接口。"""

    def __init__(self) -> None:
        if ak is None:  # pragma: no cover - 运行时缺少依赖
            raise ImportError("akshare 未安装，无法初始化 AkshareDataFetcher")

    def _ensure_df(self, value: Any) -> pd.DataFrame:
        """把 AkShare 返回值尽量规整成 DataFrame；None / 不可转换 -> 空 DF。"""
        if value is None:
            return pd.DataFrame()
        if isinstance(value, pd.DataFrame):
            return value
        try:
            return pd.DataFrame(value)
        except Exception:  # noqa: BLE001
            return pd.DataFrame()

    def get_lhb_detail(self, trade_date: str) -> pd.DataFrame:
        """获取指定交易日的龙虎榜详情。"""
        normalized_date = str(trade_date).replace("-", "")
        raw = ak.stock_lhb_detail_em(start_date=normalized_date, end_date=normalized_date)
        return self._ensure_df(raw)

    def get_margin_detail(self, trade_date: str, exchange: str) -> pd.DataFrame:
        """获取沪深交易所的融资融券明细。"""
        exchange_map = {
            "sse": ak.stock_margin_detail_sse,
            "sh": ak.stock_margin_detail_sse,
            "shanghai": ak.stock_margin_detail_sse,
            "szse": ak.stock_margin_detail_szse,
            "sz": ak.stock_margin_detail_szse,
            "shenzhen": ak.stock_margin_detail_szse,
        }
        key = str(exchange).lower()
        if key not in exchange_map:
            raise ValueError(f"不支持的交易所标识: {exchange}")

        fetch_fn = exchange_map[key]
        raw = fetch_fn(date=str(trade_date).replace("-", ""))
        return self._ensure_df(raw)

    def get_shareholder_count(self, period: str) -> pd.DataFrame:
        """获取全市场股东户数汇总。"""
        raw = ak.stock_zh_a_gdhs(symbol=period)
        return self._ensure_df(raw)

    def get_shareholder_count_detail(self, symbol: str) -> pd.DataFrame:
        """获取单只股票的股东户数明细。"""
        raw = ak.stock_zh_a_gdhs_detail_em(symbol=symbol)
        return self._ensure_df(raw)

    def batch_get_shareholder_count_detail(
        self, symbols: Iterable[str]
    ) -> list[pd.DataFrame]:
        """批量获取股东户数明细，过滤掉空结果；单个失败不影响整体。"""
        frames: list[pd.DataFrame] = []
        for symbol in symbols:
            try:
                df = self.get_shareholder_count_detail(symbol)
            except Exception:  # noqa: BLE001
                # 外部逻辑只关心“能拿到多少”，这里不抛异常，直接跳过
                continue

            if df is not None and not df.empty:
                frames.append(df)

        return frames

    def get_board_industry_spot(self) -> pd.DataFrame:
        """东方财富行业板块当日表现快照。"""

        raw = ak.stock_board_industry_name_em()
        return self._ensure_df(raw)

    def get_board_industry_hist(
        self,
        board_name: str,
        start_date: str,
        end_date: str,
        adjust: str = "hfq",
        period: str = "日k",
    ) -> pd.DataFrame:
        """东方财富行业板块历史行情。"""

        normalized_start = str(start_date).replace("-", "")
        normalized_end = str(end_date).replace("-", "")
        raw = ak.stock_board_industry_hist_em(
            symbol=board_name,
            period=period,
            start_date=normalized_start,
            end_date=normalized_end,
            adjust=adjust,
        )
        return self._ensure_df(raw)

    def get_board_industry_constituents(self, board_name: str) -> pd.DataFrame:
        """东方财富行业板块成份股列表。"""

        raw = ak.stock_board_industry_cons_em(symbol=board_name)
        return self._ensure_df(raw)

================================================================================
FILE: ashare/app.py
================================================================================

"""基于 Baostock 的示例脚本入口."""

from __future__ import annotations

import contextlib
import datetime as dt
import logging
import re
import multiprocessing as mp
import os
import time
from pathlib import Path
from typing import Callable, Iterable, Tuple

import pandas as pd
from sqlalchemy import bindparam, inspect, text

from .akshare_fetcher import AkshareDataFetcher
from .baostock_core import ADJUSTFLAG_NONE, BaostockDataFetcher
from .baostock_session import BaostockSession
from .config import ProxyConfig, get_section
from .db import DatabaseConfig, MySQLWriter
from .external_signal_manager import ExternalSignalManager
from .fundamental_manager import FundamentalDataManager
from .schema_manager import TABLE_A_SHARE_UNIVERSE, SchemaManager
from .universe import AshareUniverseBuilder
from .utils import setup_logger


_worker_session: BaostockSession | None = None
_worker_fetcher: BaostockDataFetcher | None = None


def _init_kline_worker() -> None:
    """在子进程中初始化并复用 Baostock 会话。"""

    global _worker_session, _worker_fetcher
    _worker_session = BaostockSession()
    _worker_session.connect()
    _worker_fetcher = BaostockDataFetcher(_worker_session)


def _reset_worker_session() -> None:
    """在子进程内重新建立 Baostock 会话。"""

    global _worker_session, _worker_fetcher
    if _worker_session is not None:
        with contextlib.suppress(Exception):
            _worker_session.logout()
    _worker_session = BaostockSession()
    _worker_session.connect()
    _worker_fetcher = BaostockDataFetcher(_worker_session)


def _fetch_kline_with_retry(
    fetcher: BaostockDataFetcher,
    session: BaostockSession,
    code: str,
    start_date: str,
    end_date: str,
    freq: str,
    adjustflag: str,
    max_retries: int,
    reset_callback: Callable[[], None] | None = None,
    logger: logging.Logger | None = None,
) -> tuple[str, pd.DataFrame | str]:
    """统一的 K 线重试逻辑，可在主进程与子进程复用。"""

    def _calc_backoff(attempt_index: int) -> float:
        base = 1.0
        max_backoff = 15.0
        return min(max_backoff, base * (2 ** (attempt_index - 1)))

    # 关键优化：不要对“每只股票”都 force_check 探活。
    # force_check=True 会触发一次额外的网络探针（_probe_alive -> query_sz50_stocks），
    # 全市场拉取会把请求量翻倍，显著拖慢速度；并发时更容易触发超时/限流。
    # 这里仅保证已登录即可；会话健康检查与自愈交给 BaostockDataFetcher/_call_with_retry 的节流逻辑。
    session.ensure_alive()

    for attempt in range(1, max_retries + 1):
        try:
            df = fetcher.get_kline(
                code=code,
                start_date=start_date,
                end_date=end_date,
                freq=freq,
                adjustflag=adjustflag,
            )
            return "ok", df
        except Exception as exc:  # noqa: BLE001
            if attempt >= max_retries:
                return "error", str(exc)

            if logger is not None:
                logger.warning("%s 第 %s/%s 次拉取失败：%s", code, attempt, max_retries, exc)
            time.sleep(_calc_backoff(attempt))
            try:
                message = str(exc)
                force_refresh = attempt > 1 or "10054" in message
                session.ensure_alive(
                    force_refresh=force_refresh, force_check=not force_refresh
                )
            except Exception:  # noqa: BLE001
                if reset_callback is not None:
                    reset_callback()
                else:
                    session.reconnect()

    return "error", "达到最大重试次数"


def _worker_fetch_kline(args: tuple[str, str, str, str, str, int]) -> tuple[str, str, pd.DataFrame | str]:
    """子进程中拉取单只股票 K 线，复用会话减少登录开销。"""

    global _worker_session, _worker_fetcher
    code, start_date, end_date, freq, adjustflag, max_retries = args
    if _worker_session is None or _worker_fetcher is None:
        raise RuntimeError("Baostock 会话未初始化。")

    status, payload = _fetch_kline_with_retry(
        fetcher=_worker_fetcher,
        session=_worker_session,
        code=code,
        start_date=start_date,
        end_date=end_date,
        freq=freq,
        adjustflag=adjustflag,
        max_retries=max_retries,
        reset_callback=_reset_worker_session,
    )

    return status, code, payload


class AshareApp:
    """通过脚本方式导出 Baostock 数据的应用."""

    def __init__(
        self,
        output_dir: str | Path = "output",
        top_liquidity_count: int | None = None,
        history_days: int | None = None,
        min_listing_days: int | None = None,
    ) -> None:
        # 保持入口参数兼容性
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # 日志改为写到项目根目录的 ashare.log，不再跟 output 绑在一起
        self.logger = setup_logger()

        baostock_cfg = get_section("baostock")
        self.baostock_per_code_timeout = self._read_int_from_env(
            "ASHARE_BAOSTOCK_PER_CODE_TIMEOUT",
            baostock_cfg.get("per_code_timeout", 30),
        )
        self.baostock_max_retries = self._read_int_from_env(
            "ASHARE_BAOSTOCK_MAX_RETRIES", baostock_cfg.get("max_retries", 2)
        )
        worker_default = baostock_cfg.get("worker_processes", 1)
        worker_min = baostock_cfg.get("min_worker_processes", 1)
        worker_max = baostock_cfg.get("max_worker_processes", mp.cpu_count())
        network_cap = baostock_cfg.get("network_worker_cap", worker_default)
        self.worker_processes_default = self._read_int_from_env(
            "ASHARE_BAOSTOCK_WORKERS", worker_default
        )
        self.worker_processes_min = self._read_int_from_env(
            "ASHARE_BAOSTOCK_MIN_WORKERS", worker_min
        )
        self.worker_processes_max = self._read_int_from_env(
            "ASHARE_BAOSTOCK_MAX_WORKERS", worker_max
        )
        self.network_worker_cap = self._read_int_from_env(
            "ASHARE_BAOSTOCK_NETWORK_WORKER_CAP", network_cap
        )
        self.progress_log_every = self._read_int_from_env(
            "ASHARE_PROGRESS_LOG_EVERY", baostock_cfg.get("progress_log_every", 100)
        )
        self.resume_min_rows_per_code = self._read_int_from_env(
            "ASHARE_RESUME_MIN_ROWS_PER_CODE",
            baostock_cfg.get("resume_min_rows_per_code", 200),
        )
        self.history_flush_batch = self._read_int_from_env(
            "ASHARE_HISTORY_FLUSH_BATCH", baostock_cfg.get("history_flush_batch", 5)
        )
        self.write_chunksize = self._read_int_from_env(
            "ASHARE_HISTORY_WRITE_CHUNKSIZE",
            baostock_cfg.get("history_write_chunksize", 500),
        )
        cleanup_mode_raw = os.getenv(
            "ASHARE_HISTORY_CLEANUP_MODE", baostock_cfg.get("history_cleanup_mode", "window")
        )
        cleanup_mode = str(cleanup_mode_raw).strip().lower()
        if cleanup_mode not in {"window", "skip"}:
            self.logger.warning(
                "history_cleanup_mode=%s 无效，已自动回退为 window 模式。",
                cleanup_mode_raw,
            )
            cleanup_mode = "window"
        self.history_cleanup_mode = cleanup_mode
        self.history_retention_days = self._read_int_from_env(
            "ASHARE_HISTORY_RETENTION_DAYS", baostock_cfg.get("history_retention_days", 0)
        )
        if self.history_cleanup_mode == "skip" and self.history_retention_days > 0:
            self.logger.info(
                "已启用历史数据保留窗口 %s 天，即便追加模式也会定期清理过期数据。",
                self.history_retention_days,
            )
        if self.baostock_max_retries < 1:
            self.baostock_max_retries = 1
        if self.baostock_per_code_timeout <= 0:
            self.baostock_per_code_timeout = 30
        if self.progress_log_every < 1:
            self.progress_log_every = 100

        # 从 config.yaml 读取基础面刷新开关
        app_cfg = get_section("app")
        refresh_flag = app_cfg.get("refresh_fundamentals", False)
        # 下面的数值型参数也从 config.yaml 读取默认值（允许环境变量覆盖）
        if isinstance(refresh_flag, str):
            refresh_flag = refresh_flag.strip().lower() in {
                "1",
                "true",
                "yes",
                "y",
                "on",
            }
        self.refresh_fundamentals: bool = bool(refresh_flag)

        # config.yaml 的默认值（如果写成字符串也要能解析）
        cfg_history_days = app_cfg.get("history_days", 30)
        cfg_history_view_days = app_cfg.get("history_view_days", 0)
        cfg_top_liquidity = app_cfg.get("top_liquidity_count", 100)
        cfg_min_listing = app_cfg.get("min_listing_days", 60)
        try:
            cfg_history_days = int(cfg_history_days)
        except Exception:
            cfg_history_days = 30
        try:
            cfg_history_view_days = int(cfg_history_view_days)
        except Exception:
            cfg_history_view_days = 0
        try:
            cfg_top_liquidity = int(cfg_top_liquidity)
        except Exception:
            cfg_top_liquidity = 100
        try:
            cfg_min_listing = int(cfg_min_listing)
        except Exception:
            cfg_min_listing = 60

        # 日线K线拉取开关（config.yaml: app.fetch_daily_kline）
        # 可用环境变量 ASHARE_FETCH_DAILY_KLINE 覆盖（优先级更高）
        kline_flag = os.getenv("ASHARE_FETCH_DAILY_KLINE")
        if kline_flag is None:
            kline_flag = app_cfg.get("fetch_daily_kline", True)
        if isinstance(kline_flag, str):
            kline_flag = kline_flag.strip().lower() in {
                "1",
                "true",
                "yes",
                "y",
                "on",
            }
        self.fetch_daily_kline: bool = bool(kline_flag)

        meta_flag = os.getenv("ASHARE_FETCH_STOCK_META")
        if meta_flag is None:
            meta_flag = app_cfg.get("fetch_stock_meta", True)
        if isinstance(meta_flag, str):
            meta_flag = meta_flag.strip().lower() in {
                "1",
                "true",
                "yes",
                "y",
                "on",
            }
        self.fetch_stock_meta: bool = bool(meta_flag)

        index_flag = os.getenv("ASHARE_FETCH_INDEX_KLINE")
        if index_flag is None:
            index_flag = app_cfg.get("fetch_index_kline", True)
        if isinstance(index_flag, str):
            index_flag = index_flag.strip().lower() in {
                "1",
                "true",
                "yes",
                "y",
                "on",
            }
        self.fetch_index_kline: bool = bool(index_flag)

        build_dim_flag = os.getenv("ASHARE_BUILD_STOCK_INDUSTRY_DIM")
        if build_dim_flag is None:
            build_dim_flag = app_cfg.get("build_stock_industry_dim", True)
        if isinstance(build_dim_flag, str):
            build_dim_flag = build_dim_flag.strip().lower() in {
                "1",
                "true",
                "yes",
                "y",
                "on",
            }
        self.build_stock_industry_dim: bool = bool(build_dim_flag)

        self.history_days = (
            history_days
            if history_days is not None
            else self._read_int_from_env("ASHARE_HISTORY_DAYS", cfg_history_days)
        )
        # 单独控制“近期自然日视图”窗口（便于你查近期数据）
        self.history_view_days = self._read_int_from_env(
            "ASHARE_HISTORY_VIEW_DAYS", cfg_history_view_days
        )
        if self.history_view_days < 0:
            self.history_view_days = 0

        index_codes_cfg = app_cfg.get("index_codes", [])
        if not isinstance(index_codes_cfg, (list, tuple)):
            index_codes_cfg = []
        self.index_codes = [str(c).strip() for c in index_codes_cfg if str(c).strip()]
        self.index_history_days = self._read_int_from_env(
            "ASHARE_INDEX_HISTORY_DAYS", app_cfg.get("index_history_days", 400)
        )
        if self.index_history_days <= 0:
            self.index_history_days = 400
        resolved_top_liquidity = (
            top_liquidity_count
            if top_liquidity_count is not None
            else self._read_int_from_env("ASHARE_TOP_LIQUIDITY_COUNT", cfg_top_liquidity)
        )
        resolved_min_listing_days = (
            min_listing_days
            if min_listing_days is not None
            else self._read_int_from_env("ASHARE_MIN_LISTING_DAYS", cfg_min_listing)
        )
        self.logger.info(
            "参数配置：history_days=%s, history_view_days=%s, top_liquidity_count=%s, min_listing_days=%s, fetch_daily_kline=%s",
            self.history_days,
            self.history_view_days,
            resolved_top_liquidity,
            resolved_min_listing_days,
            self.fetch_daily_kline,
        )

        self.db_config = DatabaseConfig.from_env()
        self.db_writer = MySQLWriter(self.db_config)

        proxy_config = ProxyConfig.from_env()
        proxy_config.apply_to_environment()
        self.logger.info(
            "代理配置: HTTP=%s, HTTPS=%s", proxy_config.http, proxy_config.https
        )

        self.session = BaostockSession()
        self.fetcher = BaostockDataFetcher(self.session)
        self.universe_builder = AshareUniverseBuilder(
            top_liquidity_count=resolved_top_liquidity,
            min_listing_days=resolved_min_listing_days,
        )
        self.fundamental_manager = FundamentalDataManager(
            self.fetcher, self.db_writer, self.logger
        )
        akshare_cfg = get_section("akshare")
        self.akshare_enabled = akshare_cfg.get("enabled", False)
        board_cfg = akshare_cfg.get("board_industry", {}) if isinstance(akshare_cfg, dict) else {}
        if not isinstance(board_cfg, dict):
            board_cfg = {}
        self.board_industry_enabled = bool(board_cfg.get("enabled", False))
        self.board_spot_enabled = bool(board_cfg.get("spot_enabled", True))
        self.board_hist_enabled = bool(board_cfg.get("hist_enabled", False))
        self.board_constituent_enabled = bool(board_cfg.get("build_stock_board_dim", True))
        self.board_history_days = self._read_int_from_env(
            "ASHARE_BOARD_HISTORY_DAYS", board_cfg.get("history_days", 200)
        )
        self.board_adjust = str(board_cfg.get("adjust", "hfq") or "hfq")
        self.external_signal_manager: ExternalSignalManager | None = None
        if self.akshare_enabled:
            try:
                akshare_fetcher = AkshareDataFetcher()
                self.external_signal_manager = ExternalSignalManager(
                    akshare_fetcher, self.db_writer, self.logger, akshare_cfg
                )
            except ImportError as exc:  # noqa: BLE001
                self.akshare_enabled = False
                self.logger.warning(
                    "Akshare 行为证据层初始化失败，已自动关闭：%s", exc
                )
        else:
            self.logger.info("Akshare 行为证据层已关闭，跳过相关初始化。")

        self.use_baostock: bool = bool(
            self.fetch_daily_kline
            or self.fetch_stock_meta
            or self.refresh_fundamentals
            or self.fetch_index_kline
        )

        # 登录一次会话，后续流程保持复用，减少频繁连接开销
        if self.use_baostock:
            self.session.ensure_alive()
            self.logger.info("Baostock 会话已建立，后续任务将持续复用当前连接。")
        else:
            self.logger.info("已关闭所有 Baostock 拉取开关，本次将以数据库为准执行离线模式。")

    def _save_sample(self, df: pd.DataFrame, table_name: str) -> str:
        self.db_writer.write_dataframe(df, table_name)
        return table_name

    def _create_recent_history_calendar_view(
        self,
        base_table: str,
        window_days: int,
        end_day: dt.date,
        view_name: str | None = None,
    ) -> str:
        """创建一个“最近 N 个自然日”的便捷视图，用于手动查近期数据。"""

        def _is_safe_ident(name: str) -> bool:
            return bool(re.fullmatch(r"[A-Za-z0-9_]+", name))

        view = view_name or f"history_recent_{window_days}_days"
        sanitized_days = max(1, int(window_days))
        if not _is_safe_ident(view) or not _is_safe_ident(base_table):
            raise RuntimeError(f"非法表/视图名：base_table={base_table}, view={view}")

        start_date = (end_day - dt.timedelta(days=sanitized_days - 1)).isoformat()
        end_exclusive = (end_day + dt.timedelta(days=1)).isoformat()

        drop_view_sql = text(f"DROP VIEW IF EXISTS `{view}`")
        drop_table_sql = text(f"DROP TABLE IF EXISTS `{view}`")
        create_view_sql = text(
            """
            CREATE OR REPLACE VIEW `{view}` AS
            SELECT *
            FROM `{base}`
            WHERE `date` >= '{start}'
              AND `date` < '{end_exclusive}'
            """.format(
                view=view,
                base=base_table,
                start=start_date,
                end_exclusive=end_exclusive,
            )
        )

        with self.db_writer.engine.begin() as conn:
            conn.execute(drop_view_sql)
            conn.execute(drop_table_sql)
            conn.execute(create_view_sql)

        self.logger.info(
            "已创建近期自然日视图 %s：[%s, %s)，共 %s 天。",
            view,
            start_date,
            end_exclusive,
            sanitized_days,
        )
        return view

    def _read_int_from_env(self, name: str, default: int) -> int:
        raw_value = os.getenv(name)
        if raw_value is None:
            return default

        try:
            parsed = int(raw_value)
            if parsed > 0:
                return parsed
            self.logger.warning("环境变量 %s 必须为正整数，已回退默认值 %s", name, default)
        except ValueError:
            self.logger.warning("环境变量 %s 解析失败，已回退默认值 %s", name, default)

        return default

    def _choose_worker_processes(self, total_codes: int) -> int:
        """根据配置、CPU 与任务规模动态确定子进程数量。"""

        bounded_max = max(1, min(self.worker_processes_max, mp.cpu_count()))
        network_cap = self.network_worker_cap
        if network_cap <= 0:
            hard_cap = bounded_max
        else:
            hard_cap = max(1, min(network_cap, bounded_max))

        bounded_min = max(1, min(self.worker_processes_min, hard_cap))
        base_default = max(bounded_min, min(self.worker_processes_default, hard_cap))
        adaptive = min(base_default, hard_cap, total_codes or 1)
        if total_codes > 1000 and adaptive < min(hard_cap, bounded_max):
            adaptive = min(hard_cap, bounded_max, adaptive + 1)

        chosen = max(bounded_min, min(hard_cap, adaptive))
        if hard_cap == 1 and chosen != 1:
            chosen = 1
        if hard_cap == 1:
            self.logger.info("Baostock 并发已限制为单进程模式以降低封禁风险。")

        return chosen

    def _probe_daily_kline_availability(
        self, target_date: str, sample_codes: Iterable[str] | None = None
    ) -> bool:
        """探测指定交易日的日线是否已生成，避免无效的全量拉取。"""

        samples = list(sample_codes or ["sh.600000", "sz.000001", "sh.000001"])
        normalized = [self._normalize_stock_code(code) for code in samples if code]
        if not normalized:
            return True

        has_error = False
        self.session.ensure_alive(force_check=True)

        for code in normalized:
            try:
                df = self.fetcher.get_kline(
                    code=code,
                    start_date=target_date,
                    end_date=target_date,
                    freq="d",
                    adjustflag=ADJUSTFLAG_NONE,
                )
            except Exception as exc:  # noqa: BLE001
                has_error = True
                self.logger.debug("日线可用性探针异常（%s）：%s", code, exc)
                continue

            if df.empty:
                self.logger.debug("日线可用性探针：%s %s 返回空。", code, target_date)
                continue

            self.logger.debug("日线可用性探针：%s %s 已返回数据。", code, target_date)
            return True

        if has_error:
            self.logger.warning("日线可用性探针出现异常，继续尝试全量拉取。")
            return True

        return False

    def _get_trading_days_between(
        self, start_day: dt.date, end_day: dt.date
    ) -> list[dt.date]:
        calendar_df = self.fetcher.get_trade_calendar(
            start_day.isoformat(), end_day.isoformat()
        )
        if calendar_df.empty:
            return []

        if "is_trading_day" in calendar_df.columns:
            calendar_df = calendar_df[
                calendar_df["is_trading_day"].astype(str) == "1"
            ]

        trading_days = (
            pd.to_datetime(calendar_df["calendar_date"], errors="coerce")
            .dt.date.dropna()
            .tolist()
        )
        return sorted([day for day in trading_days if day <= end_day])

    def _get_recent_trading_days(
        self,
        end_day: dt.date,
        days: int,
        base_table: str = "history_daily_kline",
    ) -> list[dt.date]:
        if not self.use_baostock:
            return self._get_recent_trading_days_from_db(end_day, days, base_table)

        lookback = max(days * 3, days + 20)
        max_lookback = 365

        while True:
            start_day = end_day - dt.timedelta(days=lookback)
            trading_days = self._get_trading_days_between(start_day, end_day)

            if len(trading_days) >= days or lookback >= max_lookback:
                break

            lookback = min(lookback + days, max_lookback)

        if len(trading_days) < days:
            raise RuntimeError(
                f"在 {lookback} 天的回看范围内未能找到 {days} 个交易日。"
            )

        return trading_days[-days:]

    def _get_recent_trading_days_from_db(
        self, end_day: dt.date, days: int, base_table: str
    ) -> list[dt.date]:
        if not self._table_exists(base_table):
            raise RuntimeError(
                f"离线模式下未找到基础行情表 {base_table}，无法推导交易日。"
            )

        max_days = max(1, days)
        end_date = end_day.isoformat()
        query = text(
            """
            SELECT DISTINCT `date`
            FROM `{table}`
            WHERE `date` <= :end_date
            ORDER BY `date` DESC
            LIMIT {limit}
            """.format(table=base_table, limit=max_days * 2)
        )

        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(query, conn, params={"end_date": end_date})

        if df.empty:
            raise RuntimeError(
                f"离线模式下未能从 {base_table} 推导交易日，查询结果为空。"
            )

        parsed = (
            pd.to_datetime(df["date"], errors="coerce")
            .dropna()
            .dt.date.unique()
            .tolist()
        )
        parsed.sort()

        if len(parsed) < days:
            raise RuntimeError(
                f"离线模式下 {base_table} 仅包含 {len(parsed)} 个有效交易日，"
                f"不足以切出最近 {days} 天窗口。"
            )

        return parsed[-days:]

    def _compute_resume_threshold(
        self,
        end_day: dt.date,
        window_days: int,
        base_table: str = "history_daily_kline",
    ) -> int:
        trading_days = self._get_recent_trading_days(
            end_day, window_days, base_table=base_table
        )
        trading_count = max(1, len(trading_days))
        dynamic_threshold = int(trading_count * 0.8)
        if dynamic_threshold <= 0:
            dynamic_threshold = trading_count
        return max(1, min(self.resume_min_rows_per_code, dynamic_threshold))

    def _table_exists(self, table_name: str) -> bool:
        try:
            inspector = inspect(self.db_writer.engine)
            return inspector.has_table(table_name)
        except Exception:  # noqa: BLE001
            return False

    def _column_exists(self, table_name: str, column: str) -> bool:
        if not table_name or not column:
            return False
        stmt = text(
            """
            SELECT COUNT(*) AS cnt
            FROM information_schema.columns
            WHERE table_schema = :schema AND table_name = :table AND column_name = :column
            """
        )
        try:
            with self.db_writer.engine.begin() as conn:
                row = conn.execute(
                    stmt,
                    {
                        "schema": self.db_writer.config.db_name,
                        "table": table_name,
                        "column": column,
                    },
                ).mappings().first()
        except Exception:  # noqa: BLE001
            return False
        return bool(row and row.get("cnt"))

    def _load_table(self, table_name: str) -> pd.DataFrame:
        """从数据库读取整表；表不存在则返回空 DataFrame。"""

        if not self._table_exists(table_name):
            return pd.DataFrame()

        query = text("SELECT * FROM `{table}`".format(table=table_name))
        with self.db_writer.engine.begin() as conn:
            return pd.read_sql_query(query, conn)

    def _infer_latest_trade_day_from_db(self, base_table: str) -> str:
        """在离线模式下，从历史表推断最近交易日。"""

        if not self._table_exists(base_table):
            return dt.date.today().isoformat()

        query = text(
            "SELECT MAX(`date`) AS max_date FROM `{table}`".format(table=base_table)
        )
        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(query, conn)

        if df.empty or "max_date" not in df.columns:
            return dt.date.today().isoformat()

        max_date = df.loc[0, "max_date"]
        if pd.isna(max_date):
            return dt.date.today().isoformat()

        parsed = pd.to_datetime(max_date, errors="coerce")
        if pd.isna(parsed):
            return dt.date.today().isoformat()

        return parsed.date().isoformat()

    def _build_stock_list_from_history(
        self, base_table: str, trade_day: str
    ) -> pd.DataFrame:
        """在没有 a_share_stock_list 的情况下，用历史行情表兜底生成股票列表。"""

        if not self._table_exists(base_table):
            return pd.DataFrame()

        query = text(
            "SELECT DISTINCT `code` FROM `{table}` WHERE `date` = :trade_day".format(
                table=base_table
            )
        )
        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(query, conn, params={"trade_day": trade_day})

        if df.empty:
            query_all = text(
                "SELECT DISTINCT `code` FROM `{table}`".format(table=base_table)
            )
            with self.db_writer.engine.begin() as conn:
                df = pd.read_sql_query(query_all, conn)

        if df.empty:
            return df

        df["code"] = df["code"].astype(str)
        if "code_name" not in df.columns:
            df["code_name"] = ""
        if "tradeStatus" not in df.columns:
            df["tradeStatus"] = "1"
        return df[["code", "code_name", "tradeStatus"]]

    def _load_index_membership_from_db(self) -> dict[str, set[str]]:
        """从数据库读取指数成分股表，构造 {index_name: set(code)}。"""

        index_membership: dict[str, set[str]] = {}
        for index_name in ("hs300", "zz500", "sz50"):
            table = f"index_{index_name}_members"
            df = self._load_table(table)
            if df.empty or "code" not in df.columns:
                continue
            index_membership[index_name] = set(df["code"].dropna().astype(str))
        return index_membership

    def _load_completed_codes(
        self, table_name: str, min_rows: int, required_end_date: str | None = None
    ) -> set[str]:
        having_conditions = ["COUNT(*) >= :threshold"]
        params: dict[str, object] = {"threshold": min_rows}
        if required_end_date:
            having_conditions.append("MAX(`date`) >= :required_end_date")
            params["required_end_date"] = required_end_date

        having_clause = " AND ".join(having_conditions)
        query = text(
            "SELECT `code` FROM `{table}` GROUP BY `code` HAVING {clause}".format(
                table=table_name, clause=having_clause
            )
        )
        try:
            with self.db_writer.engine.begin() as conn:
                df = pd.read_sql_query(query, conn, params=params)
        except Exception:  # noqa: BLE001
            return set()

        codes = {str(code) for code in df.get("code", []) if pd.notna(code)}
        return codes

    def _flush_history_batch(
        self, batch: list[pd.DataFrame], table_name: str, if_exists: str = "append"
    ) -> None:
        if not batch:
            return

        combined = pd.concat(batch, ignore_index=True)
        if {"code", "date"}.issubset(combined.columns):
            combined = combined.drop_duplicates(subset=["code", "date"])
            combined["trade_date"] = pd.to_datetime(
                combined["date"], errors="coerce"
            ).dt.date

        codes = (
            combined.get("code", pd.Series(dtype=str))
            .dropna()
            .astype(str)
            .unique()
            .tolist()
        )
        min_date = None
        max_date = None
        if "date" in combined.columns:
            date_series = pd.to_datetime(combined["date"], errors="coerce")
            min_date_raw = date_series.min()
            max_date_raw = date_series.max()
            if pd.notna(min_date_raw) and pd.notna(max_date_raw):
                min_date = min_date_raw.date().isoformat()
                max_date = max_date_raw.date().isoformat()

        if codes and min_date and max_date:
            if self.history_cleanup_mode == "skip":
                self.logger.info("清理模式为 skip，直接追加 %s 条记录。", len(combined))
            elif not self._table_exists(table_name):
                self.logger.info("表 %s 不存在，跳过旧数据删除。", table_name)
            else:
                date_column = (
                    "trade_date" if self._column_exists(table_name, "trade_date") else "date"
                )
                delete_stmt = (
                    text(
                        "DELETE FROM `{table}` WHERE `code` IN :codes AND `{date_col}` BETWEEN :start_date AND :end_date".format(
                            table=table_name,
                            date_col=date_column,
                        )
                    ).bindparams(bindparam("codes", expanding=True))
                )
                try:
                    with self.db_writer.engine.begin() as conn:
                        conn.execute(
                            delete_stmt,
                            {
                                "codes": codes,
                                "start_date": min_date,
                                "end_date": max_date,
                            },
                        )
                except Exception as exc:  # noqa: BLE001
                    self.logger.warning("删除旧数据失败，将直接追加：%s", exc)

        self.db_writer.write_dataframe(
            combined,
            table_name,
            if_exists=if_exists,
            chunksize=self.write_chunksize,
            method="multi",
        )

        self._log_memory_usage("历史批量写入")

    def _log_memory_usage(self, prefix: str) -> None:
        try:
            import resource

            usage = resource.getrusage(resource.RUSAGE_SELF)
            memory_mb = usage.ru_maxrss / 1024
            self.logger.debug("%s后内存峰值约 %.2f MB", prefix, memory_mb)
        except Exception:
            return

    def _cleanup_session(self) -> None:
        """统一释放 Baostock 会话，避免遗留连接。"""

        try:
            self.session.logout()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("Baostock 会话登出时发生异常：%s", exc)

    def _purge_old_history(self, table_name: str, reference_date: str) -> None:
        if self.history_retention_days <= 0:
            return
        if not self._table_exists(table_name):
            return

        try:
            end_day = dt.datetime.strptime(reference_date, "%Y-%m-%d").date()
        except ValueError:
            return

        cutoff = (end_day - dt.timedelta(days=self.history_retention_days)).isoformat()
        date_column = (
            "trade_date" if self._column_exists(table_name, "trade_date") else "date"
        )
        purge_stmt = text(
            "DELETE FROM `{table}` WHERE `{date_col}` < :cutoff".format(
                table=table_name,
                date_col=date_column,
            )
        )
        try:
            with self.db_writer.engine.begin() as conn:
                conn.execute(purge_stmt, {"cutoff": cutoff})
            self.logger.info(
                "已按照保留窗口 %s 天清理 %s 之前的历史日线。", self.history_retention_days, cutoff
            )
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("历史保留窗口清理失败：%s", exc)

    def _write_failed_codes(self, failed_codes: list[str]) -> None:
        if not failed_codes:
            return

        failed_path = self.output_dir / "failed_codes.txt"
        existing: set[str] = set()
        if failed_path.exists():
            existing = {line.strip() for line in failed_path.read_text().splitlines() if line.strip()}

        merged = sorted(existing.union(failed_codes))
        failed_path.write_text("\n".join(merged))

    def _fetch_and_store_history(
        self,
        stock_df: pd.DataFrame,
        start_day: str,
        end_date: str,
        base_table: str,
        adjustflag: str = ADJUSTFLAG_NONE,
        resume_threshold: int | None = None,
        probe_date: str | None = None,
        latest_existing_date: str | None = None,
    ) -> tuple[int, list[str], list[str], int]:
        history_frames: list[pd.DataFrame] = []
        success_count = 0
        empty_codes: list[str] = []
        failed_codes: list[str] = []
        skipped = 0
        done_codes: set[str] = set()
        if resume_threshold is not None and resume_threshold > 0:
            done_codes = self._load_completed_codes(
                base_table, resume_threshold, required_end_date=end_date
            )

        codes = [str(code) for code in stock_df.get("code", []) if pd.notna(code)]
        codes_to_fetch = [code for code in codes if code not in done_codes]
        total_attempted = len(codes_to_fetch)
        skipped = len(codes) - len(codes_to_fetch)
        if not codes_to_fetch:
            self.logger.info("历史日线已经完成，跳过拉取。")
            return success_count, empty_codes, failed_codes, skipped

        if (
            probe_date
            and start_day == end_date
            and not self._probe_daily_kline_availability(probe_date)
        ):
            db_hint = (
                f"；数据库仍截至 {latest_existing_date}" if latest_existing_date else ""
            )
            self.logger.info(
                "Baostock 尚未更新到 %s（日线返回全空），本次跳过增量拉取%s",
                probe_date,
                db_hint or "。",
            )
            return success_count, empty_codes, failed_codes, skipped + total_attempted

        ctx = mp.get_context("spawn")
        worker_processes = self._choose_worker_processes(len(codes_to_fetch))
        worker_args = [
            (code, start_day, end_date, "d", adjustflag, self.baostock_max_retries)
            for code in codes_to_fetch
        ]

        self.logger.info(
            "本次历史日线拉取将使用 %s 个并发子进程（候选 %s 支股票）。",
            worker_processes,
            len(codes_to_fetch),
        )
        # feat: 按累计行数阈值批量写库，减少频繁 flush 导致的 I/O 开销
        flush_rows_target = max(5000, int(self.write_chunksize) * 10)
        flush_rows_target = min(flush_rows_target, 20000)
        flush_frames_cap = max(self.history_flush_batch, 50)
        buffer_rows = 0


        def _handle_result(
            status: str, code_value: str, payload: pd.DataFrame | str
        ) -> None:
            nonlocal success_count
            nonlocal buffer_rows

            if status == "ok" and isinstance(payload, pd.DataFrame):
                if payload.empty:
                    empty_codes.append(code_value)
                else:
                    history_frames.append(payload)
                    buffer_rows += len(payload)
                    success_count += 1
            else:
                failed_codes.append(code_value)

        if worker_processes == 1:
            self.session.ensure_alive()
            for idx, code in enumerate(codes_to_fetch, start=1):
                status, payload = _fetch_kline_with_retry(
                    fetcher=self.fetcher,
                    session=self.session,
                    code=code,
                    start_date=start_day,
                    end_date=end_date,
                    freq="d",
                    adjustflag=adjustflag,
                    max_retries=self.baostock_max_retries,
                    reset_callback=self.session.reconnect,
                    logger=self.logger,
                )
                _handle_result(status, code, payload)

                if history_frames and (
                    buffer_rows >= flush_rows_target
                    or len(history_frames) >= flush_frames_cap
                    or idx == len(codes_to_fetch)
                ):
                    self._flush_history_batch(
                        history_frames, base_table, if_exists="append"
                    )
                    history_frames.clear()

                if idx % self.progress_log_every == 0 or idx == len(codes_to_fetch):
                    self.logger.info(
                        "已完成 %s/%s 支股票的拉取，最近处理 %s",
                        idx,
                        len(codes_to_fetch),
                        code,
                    )
        else:
            with ctx.Pool(
                processes=worker_processes,
                initializer=_init_kline_worker,
                maxtasksperchild=200,
            ) as pool:
                for idx, result in enumerate(
                    pool.imap_unordered(_worker_fetch_kline, worker_args), start=1
                ):
                    status, code, payload = result
                    _handle_result(status, code, payload)

                    if history_frames and (
                        buffer_rows >= flush_rows_target
                        or len(history_frames) >= flush_frames_cap
                        or idx == len(codes_to_fetch)
                    ):
                        self._flush_history_batch(
                            history_frames, base_table, if_exists="append"
                        )
                        history_frames.clear()

                    if idx % self.progress_log_every == 0 or idx == len(codes_to_fetch):
                        self.logger.info(
                            "已完成 %s/%s 支股票的拉取，最近处理 %s",
                            idx,
                            len(codes_to_fetch),
                            code,
                        )

        if history_frames:
            self._flush_history_batch(history_frames, base_table, if_exists="append")

        if failed_codes:
            self._write_failed_codes(failed_codes)

        all_empty = (
            success_count == 0
            and len(empty_codes) == total_attempted
            and not failed_codes
            and total_attempted > 0
        )

        if all_empty:
            db_hint = (
                f"；数据库仍截至 {latest_existing_date}" if latest_existing_date else ""
            )
            self.logger.info(
                "Baostock 尚未更新到 %s（日线返回全空），本次跳过增量拉取%s",
                end_date,
                db_hint or "。",
            )
        else:
            self.logger.info(
                "拉取结束：成功 %s/%s，空数据 %s，失败 %s，跳过 %s",
                success_count,
                total_attempted,
                len(empty_codes),
                len(failed_codes),
                skipped,
            )

        if empty_codes:
            self.logger.debug("完全未返回数据的股票：%s", ", ".join(sorted(empty_codes)))
        if failed_codes:
            self.logger.debug("请求失败的股票：%s", ", ".join(sorted(failed_codes)))

        self._purge_old_history(base_table, end_date)

        return success_count, empty_codes, failed_codes, skipped

    def _export_stock_list(self, trade_date: str) -> pd.DataFrame:
        stock_df = self.fetcher.get_stock_list(trade_date)
        if stock_df.empty:
            raise RuntimeError("获取股票列表失败：返回为空。")

        table_name = self._save_sample(stock_df, "a_share_stock_list")
        self.logger.info("已保存 %s 只股票的列表至表 %s", len(stock_df), table_name)
        return stock_df

    def _export_stock_basic(self) -> pd.DataFrame:
        stock_basic_df = self.fetcher.get_stock_basic()
        if stock_basic_df.empty:
            raise RuntimeError("获取证券基本资料失败：返回为空。")

        table_name = self._save_sample(stock_basic_df, "a_share_stock_basic")
        self.logger.info("已保存证券基本资料至表 %s", table_name)
        return stock_basic_df

    def _export_stock_industry(self) -> pd.DataFrame:
        industry_df = self.fetcher.get_stock_industry()
        if industry_df.empty:
            raise RuntimeError("获取行业分类信息失败：返回为空。")

        table_name = self._save_sample(industry_df, "a_share_stock_industry")
        self.logger.info("已保存行业分类信息至表 %s", table_name)
        if self.build_stock_industry_dim:
            self._save_stock_industry_dimension(industry_df)
        return industry_df

    def _save_stock_industry_dimension(self, industry_df: pd.DataFrame) -> None:
        if industry_df.empty:
            return

        dim = industry_df.copy()
        rename_map = {
            "updateDate": "update_date",
            "code_name": "code_name",
            "industry": "industry",
            "industryClassification": "industry_classification",
        }
        dim = dim.rename(columns=rename_map)
        for col in ["update_date"]:
            if col in dim.columns:
                dim[col] = pd.to_datetime(dim[col], errors="coerce")
        dim["source"] = "baostock"
        dim["updated_at"] = dt.datetime.now()

        cols = [
            "update_date",
            "code",
            "code_name",
            "industry",
            "industry_classification",
            "source",
            "updated_at",
        ]
        dim = dim[[c for c in cols if c in dim.columns]]

        with self.db_writer.engine.begin() as conn:
            dim.to_sql(
                "dim_stock_industry",
                conn,
                if_exists="replace",
                index=False,
                chunksize=self.write_chunksize,
            )
        self.logger.info("股票-行业维表 dim_stock_industry 已刷新，共 %s 条", len(dim))

    def _export_index_daily_history(self, end_date: str) -> pd.DataFrame:
        if not self.fetch_index_kline:
            self.logger.info("已关闭指数日线拉取，跳过 history_index_daily_kline。")
            return pd.DataFrame()
        if not self.index_codes:
            self.logger.warning("index_codes 为空，跳过指数日线拉取。")
            return pd.DataFrame()

        end_day = dt.datetime.strptime(end_date, "%Y-%m-%d").date()
        start_day = end_day - dt.timedelta(days=max(1, int(self.index_history_days)))
        frames: list[pd.DataFrame] = []
        for code in self.index_codes:
            try:
                df = self.fetcher.get_kline(
                    code=code,
                    start_date=start_day.isoformat(),
                    end_date=end_day.isoformat(),
                    freq="d",
                    adjustflag=ADJUSTFLAG_NONE,
                )
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取指数 %s 失败：%s", code, exc)
                continue

            if df.empty:
                continue

            df["source"] = "baostock"
            df["created_at"] = dt.datetime.now()
            frames.append(df)

        if not frames:
            self.logger.warning("指数日线拉取结果为空。")
            return pd.DataFrame()

        merged = pd.concat(frames, ignore_index=True)
        merged = merged.drop_duplicates(subset=["date", "code"], keep="last")
        if "date" in merged.columns:
            merged["trade_date"] = pd.to_datetime(merged["date"], errors="coerce").dt.date
        with self.db_writer.engine.begin() as conn:
            if self._table_exists("history_index_daily_kline"):
                date_column = (
                    "trade_date"
                    if self._column_exists("history_index_daily_kline", "trade_date")
                    else "date"
                )
                conn.execute(
                    text(
                        "DELETE FROM history_index_daily_kline WHERE `{date_col}` >= :start_date AND `{date_col}` <= :end_date".format(
                            date_col=date_column
                        )
                    ),
                    {"start_date": start_day.isoformat(), "end_date": end_day.isoformat()},
                )
            merged.to_sql(
                "history_index_daily_kline",
                conn,
                if_exists="append",
                index=False,
                chunksize=self.write_chunksize,
            )
        self.logger.info(
            "指数日线已写入 history_index_daily_kline：%s 条（窗口 %s - %s）",
            len(merged),
            start_day,
            end_day,
        )
        return merged

    @staticmethod
    def _normalize_stock_code(symbol: str) -> str:
        code = str(symbol).strip()
        if not code:
            return code
        if code.startswith(("sh.", "sz.")):
            return code
        if code.startswith("6"):
            return f"sh.{code}"
        if code.startswith(("0", "3")):
            return f"sz.{code}"
        return code

    def _export_board_industry_spot(self) -> pd.DataFrame:
        if not (self.akshare_enabled and self.board_industry_enabled and self.board_spot_enabled):
            self.logger.info("板块快照拉取未开启，跳过 board_industry_spot。")
            return pd.DataFrame()

        try:
            fetcher = AkshareDataFetcher()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("AkShare 初始化失败，无法拉取板块快照：%s", exc)
            return pd.DataFrame()

        try:
            spot = fetcher.get_board_industry_spot()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("拉取行业板块快照失败：%s", exc)
            return pd.DataFrame()

        if spot.empty:
            self.logger.warning("行业板块快照为空。")
            return pd.DataFrame()

        normalized = spot.copy()
        rename_map = {}
        for col in normalized.columns:
            if "代码" in col:
                rename_map[col] = "board_code"
            if "板块" in col and "名称" in col:
                rename_map[col] = "board_name"
            if col in {"涨跌幅", "涨跌幅(%)", "change_rate", "pct_chg"}:
                rename_map[col] = "chg_pct"
            if col in {"成交额", "amount"}:
                rename_map[col] = "amount"
            if "领涨" in col and "涨跌幅" in col:
                rename_map[col] = "leader_pct"
            elif "领涨" in col:
                rename_map[col] = "leader_stock"

        normalized = normalized.rename(columns=rename_map)
        if "board_code" in normalized.columns:
            normalized["board_code"] = normalized["board_code"].astype(str)
        if "chg_pct" in normalized.columns:
            normalized["chg_pct"] = pd.to_numeric(normalized["chg_pct"], errors="coerce")
        normalized["ts"] = dt.datetime.now()

        with self.db_writer.engine.begin() as conn:
            normalized.to_sql(
                "board_industry_spot",
                conn,
                if_exists="append",
                index=False,
                chunksize=self.write_chunksize,
            )
        self.logger.info("行业板块快照已写入 board_industry_spot：%s 条", len(normalized))
        return normalized

    def _export_board_industry_constituents(
        self, spot: pd.DataFrame | None = None
    ) -> pd.DataFrame:
        if not (
            self.akshare_enabled
            and self.board_industry_enabled
            and self.board_constituent_enabled
            and self.board_spot_enabled
        ):
            return pd.DataFrame()

        if spot is None or spot.empty:
            spot = self._export_board_industry_spot()

        if spot.empty:
            self.logger.info("缺少板块快照，跳过成份股维表构建。")
            return pd.DataFrame()

        try:
            fetcher = AkshareDataFetcher()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("AkShare 初始化失败，无法拉取板块成份股：%s", exc)
            return pd.DataFrame()

        frames: list[pd.DataFrame] = []
        for _, row in spot.iterrows():
            board_name = str(row.get("board_name") or "").strip()
            board_code = str(row.get("board_code") or "").strip()
            if not board_name:
                continue
            try:
                cons = fetcher.get_board_industry_constituents(board_name)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取板块 %s 成份股失败：%s", board_name, exc)
                continue

            if cons.empty:
                continue

            cons = cons.copy()
            rename_map = {}
            for col in cons.columns:
                if col in {"代码", "股票代码", "symbol"}:
                    rename_map[col] = "code"
                elif col in {"名称", "股票名称", "股票简称"}:
                    rename_map[col] = "code_name"
            cons = cons.rename(columns=rename_map)
            if "code" not in cons.columns:
                continue
            cons["code"] = cons["code"].astype(str).apply(self._normalize_stock_code)
            cons["board_name"] = board_name
            cons["board_code"] = board_code
            cons["source"] = "akshare"
            cons["updated_at"] = dt.datetime.now()
            frames.append(cons[["code", "board_name", "board_code", "source", "updated_at"]])

        if not frames:
            self.logger.info("未获取到任何板块成份股数据。")
            return pd.DataFrame()

        merged = pd.concat(frames, ignore_index=True)
        merged = merged.drop_duplicates(subset=["code", "board_name"], keep="last")
        merged = merged.sort_values(by=["code", "board_name"])
        merged = merged.dropna(subset=["code", "board_name"])
        merged["code"] = merged["code"].astype(str)

        with self.db_writer.engine.begin() as conn:
            merged.to_sql(
                "dim_stock_board_industry",
                conn,
                if_exists="replace",
                index=False,
                chunksize=self.write_chunksize,
            )
        self.logger.info("板块成份股维表已写入 dim_stock_board_industry：%s 条", len(merged))
        return merged

    def _export_board_industry_history(
        self, end_date: str, spot: pd.DataFrame | None = None
    ) -> pd.DataFrame:
        if not (self.akshare_enabled and self.board_industry_enabled and self.board_hist_enabled):
            self.logger.info("板块历史拉取未开启，跳过 board_industry_hist_daily。")
            return pd.DataFrame()

        try:
            fetcher = AkshareDataFetcher()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("AkShare 初始化失败，无法拉取板块历史：%s", exc)
            return pd.DataFrame()

        if spot is None or spot.empty:
            spot = self._export_board_industry_spot()
        if spot.empty:
            self.logger.warning("没有有效的板块列表，跳过历史行情拉取。")
            return pd.DataFrame()

        start_day = (
            dt.datetime.strptime(end_date, "%Y-%m-%d").date()
            - dt.timedelta(days=max(1, int(self.board_history_days)))
        )
        boards: pd.Series | None = None
        if "board_name" in spot.columns:
            boards = spot["board_name"]
        elif "板块名称" in spot.columns:
            boards = spot["板块名称"]
        if boards is None:
            self.logger.warning("板块快照中缺少 board_name 列，跳过历史拉取。")
            return pd.DataFrame()

        frames: list[pd.DataFrame] = []
        for name in boards.dropna().unique():
            try:
                hist = fetcher.get_board_industry_hist(
                    board_name=str(name),
                    start_date=start_day.isoformat(),
                    end_date=end_date,
                    adjust=self.board_adjust,
                )
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取板块 %s 历史失败：%s", name, exc)
                continue

            if hist.empty:
                continue

            hist = hist.copy()
            hist["board_name"] = name
            hist["source"] = "akshare"
            hist["created_at"] = dt.datetime.now()
            frames.append(hist)

        if not frames:
            self.logger.warning("板块历史拉取结果为空。")
            return pd.DataFrame()

        merged = pd.concat(frames, ignore_index=True)
        merged = merged.rename(columns={"日期": "date"}) if "日期" in merged.columns else merged
        merged = merged.drop_duplicates(subset=["date", "board_name"], keep="last")
        with self.db_writer.engine.begin() as conn:
            if self._table_exists("board_industry_hist_daily"):
                conn.execute(
                    text(
                        "DELETE FROM board_industry_hist_daily WHERE `date` >= :start_date AND `date` <= :end_date"
                    ),
                    {"start_date": start_day.isoformat(), "end_date": end_date},
                )
            merged.to_sql(
                "board_industry_hist_daily",
                conn,
                if_exists="append",
                index=False,
                chunksize=self.write_chunksize,
            )
        self.logger.info(
            "板块历史行情已写入 board_industry_hist_daily：%s 条（窗口 %s - %s）",
            len(merged),
            start_day,
            end_date,
        )
        return merged

    def _export_index_members(self, latest_trade_day: str) -> dict[str, set[str]]:
        index_membership: dict[str, set[str]] = {}
        for index_name in ("hs300", "zz500", "sz50"):
            try:
                members_df = self.fetcher.get_index_members(index_name, latest_trade_day)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("获取 %s 成分股失败: %s", index_name, exc)
                continue

            if members_df.empty:
                self.logger.warning("指数 %s 成分股返回为空，已跳过。", index_name)
                continue

            table_name = self._save_sample(
                members_df, f"index_{index_name}_members"
            )
            self.logger.info("已保存 %s 成分股至表 %s", index_name, table_name)
            index_membership[index_name] = set(members_df.get("code", []))

        return index_membership

    def _export_recent_daily_history(
        self,
        stock_df: pd.DataFrame,
        end_date: str,
        days: int = 30,
        base_table: str = "history_daily_kline",
    ) -> Tuple[pd.DataFrame, str]:
        if stock_df.empty or "code" not in stock_df.columns:
            raise RuntimeError("导出历史日线失败：股票列表为空或缺少 code 列。")

        end_day = dt.datetime.strptime(end_date, "%Y-%m-%d").date()
        recent_trading_days = self._get_recent_trading_days(
            end_day, days, base_table=base_table
        )
        resume_threshold = self._compute_resume_threshold(
            end_day, days, base_table=base_table
        )
        start_day = recent_trading_days[0].isoformat()

        self.logger.info(
            "开始导出 %s 只股票的最近 %s 个交易日历史数据，窗口 [%s, %s]",
            len(stock_df),
            days,
            start_day,
            end_date,
        )

        success_count, empty_codes, failed_codes, skipped = self._fetch_and_store_history(
            stock_df,
            start_day=start_day,
            end_date=end_date,
            base_table=base_table,
            adjustflag=ADJUSTFLAG_NONE,
            resume_threshold=resume_threshold,
        )

        if success_count == 0 and skipped == 0:
            raise RuntimeError("导出历史日线失败：全部股票均未返回数据。")

        recent_df, recent_table = self._slice_recent_window(base_table, end_day, days)
        if not recent_df.empty:
            self.logger.info(
                "已读取最近 %s 个交易日窗口（base=%s），当前样本行数 %s，空数据 %s，失败 %s，跳过 %s",
                days,
                recent_table,
                len(recent_df),
                len(empty_codes),
                len(failed_codes),
                skipped,
            )
        return recent_df, recent_table

    def _slice_recent_window(
        self,
        base_table: str,
        end_day: dt.date,
        window_days: int,
        view_name: str | None = None,
    ) -> Tuple[pd.DataFrame, str]:
        """从基础表切片读取最近 window_days 个交易日的数据（不再创建 history_recent_xxx_days 视图）。"""

        sanitized_days = max(1, int(window_days))

        # 防注入：表名仅允许字母数字下划线
        if not base_table or any((not (ch.isalnum() or ch == "_")) for ch in base_table):
            raise RuntimeError(f"非法表名：{base_table}")

        # 左闭右开：兼容 date 字段为 TEXT 且可能包含时间的情况
        end_exclusive = (end_day + dt.timedelta(days=1)).isoformat()

        start_date_sql = text(
            """
            SELECT MIN(`date`) AS start_date
            FROM (
                SELECT DISTINCT `date`
                FROM `{base}`
                WHERE `date` < :end_exclusive
                ORDER BY `date` DESC
                LIMIT :window_days
            ) AS d
            """.format(base=base_table)
        )
        slice_sql = text(
            """
            SELECT *
            FROM `{base}`
            WHERE `date` >= :start_date
              AND `date` < :end_exclusive
            ORDER BY `code`, `date`
            """.format(base=base_table)
        )

        with self.db_writer.engine.begin() as conn:
            row = conn.execute(
                start_date_sql,
                {"end_exclusive": end_exclusive, "window_days": sanitized_days},
            ).mappings().first()
            start_date = (row or {}).get("start_date")
            if start_date is None:
                raise RuntimeError(
                    f"从表 {base_table} 解析最近 {sanitized_days} 个交易日失败：start_date 为空（表为空或 date 异常）。"
                )
            recent_df = pd.read_sql_query(
                slice_sql,
                conn,
                params={"start_date": str(start_date), "end_exclusive": end_exclusive},
            )

        if recent_df.empty:
            raise RuntimeError(
                f"从表 {base_table} 切片读取最近 {sanitized_days} 个交易日数据失败：结果为空。"
            )

        if "date" not in recent_df.columns:
            raise RuntimeError(
                f"表 {base_table} 缺少 date 列，无法进行时间窗口切片。"
            )

        recent_df["date"] = pd.to_datetime(recent_df["date"])
        self.logger.debug(
            "已从表 %s 切片读取最近 %s 个交易日数据（start=%s, end<%s），不再创建 history_recent_%s_days 视图。",
            base_table,
            sanitized_days,
            str(start_date),
            end_exclusive,
            sanitized_days,
        )
        return recent_df, base_table


    def _refresh_history_calendar_view(
        self,
        base_table: str,
        end_day: dt.date,
        slice_window_days: int | None = None,
    ) -> str | None:
        """按自然日刷新近期查询视图（用于手动 SQL 查询，不影响回测/指标计算口径）。

        - 使用 app.history_view_days 控制自然日窗口（例如 45）。
        - 默认视图名为 history_recent_{N}_days；若与交易日切片视图同名，则自动改为
          history_recent_calendar_{N}_days 以避免覆盖。
        """

        view_days_raw = getattr(self, "history_view_days", 0) or 0
        try:
            view_days = int(view_days_raw)
        except (TypeError, ValueError):  # noqa: PERF203
            view_days = 0

        if view_days <= 0:
            self.logger.info(
                "history_view_days=%s，近期自然日视图未开启，跳过创建。",
                view_days_raw,
            )
            self._last_history_calendar_view = None
            return None

        view_name = f"history_recent_{view_days}_days"
        if slice_window_days is not None and int(slice_window_days) == view_days:
            view_name = f"history_recent_calendar_{view_days}_days"

        try:
            view = self._create_recent_history_calendar_view(
                base_table,
                view_days,
                end_day=end_day,
                view_name=view_name,
            )
            self.logger.debug(
                "近期自然日便捷视图已更新：%s（最近 %s 天）",
                view,
                view_days,
            )
            self._last_history_calendar_view = view
            return view
        except Exception as exc:  # noqa: BLE001
            self.logger.warning(
                "创建近期自然日视图失败（base=%s, view=%s, end=%s, days=%s）: %s",
                base_table,
                view_name,
                end_day.isoformat(),
                view_days,
                exc,
            )
            self._last_history_calendar_view = None
            return None

    def _export_daily_history_incremental(
        self,
        stock_df: pd.DataFrame,
        end_date: str,
        base_table: str = "history_daily_kline",
        window_days: int = 30,
        fetch_enabled: bool = True,
    ) -> Tuple[pd.DataFrame, str]:
        """
        增量更新日线数据，并返回最近 window_days 天的切片。

        - 首次运行或表为空时：调用冷启动逻辑拉取 window_days 天并写入基础表。
        - 后续运行：仅拉取缺失的交易日数据并追加到基础表，然后从基础表切片。
        """

        if stock_df.empty or "code" not in stock_df.columns:
            raise RuntimeError("导出历史日线失败：股票列表为空或缺少 code 列。")

        end_day = dt.datetime.strptime(end_date, "%Y-%m-%d").date()

        with self.db_writer.engine.begin() as conn:
            try:
                existing = pd.read_sql(
                    f"SELECT MAX(`date`) AS max_date FROM `{base_table}`",
                    conn,
                )
                last_date_raw = existing["max_date"].iloc[0]
            except Exception:  # noqa: BLE001
                last_date_raw = None

        last_date_value: dt.date | None = None
        if isinstance(last_date_raw, pd.Timestamp):
            last_date_value = last_date_raw.date()
        elif isinstance(last_date_raw, dt.date):
            last_date_value = last_date_raw
        elif isinstance(last_date_raw, str) and last_date_raw:
            last_date_value = dt.datetime.strptime(last_date_raw, "%Y-%m-%d").date()

        resume_threshold = self._compute_resume_threshold(
            end_day, window_days, base_table=base_table
        )
        done_codes: set[str] = set()

        # 开关关闭：禁止调用 Baostock 日线K线接口，仅从数据库表切片读取
        if not fetch_enabled:
            self.logger.info(
                "日线K线拉取开关已关闭：跳过 Baostock 拉取，仅从表 %s 切片读取最近 %s 个交易日数据。",
                base_table,
                window_days,
            )
            if last_date_value is None:
                raise RuntimeError(
                    f"日线K线拉取已关闭，但表 {base_table} 不存在或为空，无法从数据库读取。"
                )
            if last_date_value < end_day:
                self.logger.warning(
                    "日线K线拉取已关闭：表 %s 仅包含截至 %s 的数据（目标截止 %s），将按数据库最新日期继续。",
                    base_table,
                    last_date_value.isoformat(),
                    end_day.isoformat(),
                )
                end_day = last_date_value
                end_date = end_day.isoformat()

        if fetch_enabled and last_date_value is None:
            self.logger.info(
                "历史表 %s 不存在或为空，执行冷启动：拉取最近 %s 天日线。",
                base_table,
                window_days,
            )
            recent_df, recent_table = self._export_recent_daily_history(
                stock_df, end_date, days=window_days, base_table=base_table
            )
            self._refresh_history_calendar_view(base_table, end_day, slice_window_days=window_days)
            return recent_df, recent_table
        elif fetch_enabled and last_date_value >= end_day:
            done_codes = self._load_completed_codes(
                base_table, resume_threshold, required_end_date=end_date
            )
            if len(done_codes) < len(stock_df):
                self.logger.info(
                    "历史表 %s 已存在但未覆盖全部股票，继续补齐缺口。", base_table
                )
                recent_df, recent_table = self._export_recent_daily_history(
                    stock_df, end_date, days=window_days, base_table=base_table
                )
                self._refresh_history_calendar_view(base_table, end_day, slice_window_days=window_days)
                return recent_df, recent_table
            self.logger.info(
                "历史日线表 %s 已包含截至 %s 的数据，跳过增量拉取。",
                base_table,
                end_date,
            )
            recent_df, recent_table = self._slice_recent_window(base_table, end_day, window_days)
            self._refresh_history_calendar_view(base_table, end_day, slice_window_days=window_days)
            return recent_df, recent_table
        elif fetch_enabled:
            trade_start = last_date_value + dt.timedelta(days=1)
            new_trading_days = self._get_trading_days_between(trade_start, end_day)
            if not new_trading_days:
                self.logger.info(
                    "最近交易日仍为 %s，暂无需要增量的交易日。",
                    last_date_value.isoformat(),
                )
                new_trading_days = []

            start_day = new_trading_days[0].isoformat() if new_trading_days else None
            self.logger.info(
                "开始增量拉取 %s 至 %s 的日线数据（原有截至 %s）。",
                start_day or "无新增交易日",
                end_date,
                last_date_value.isoformat(),
            )

            if start_day is not None:
                self._fetch_and_store_history(
                    stock_df,
                    start_day=start_day,
                    end_date=end_date,
                    base_table=base_table,
                    adjustflag=ADJUSTFLAG_NONE,
                    resume_threshold=None,
                    probe_date=end_date,
                    latest_existing_date=last_date_value.isoformat(),
                )
            else:
                self.logger.info("本次没有任何新的日线数据可写入。")

        recent_df, recent_table = self._slice_recent_window(base_table, end_day, window_days)
        self._refresh_history_calendar_view(base_table, end_day, slice_window_days=window_days)
        return recent_df, recent_table
    def _extract_focus_codes(self, df: pd.DataFrame) -> list[str]:
        if df.empty:
            return []
        if "code" not in df.columns:
            return []
        codes = df["code"].astype(str).dropna().tolist()
        return codes

    def _sync_external_signals(
        self, latest_trade_day: str, focus_df: pd.DataFrame
    ) -> None:
        if self.external_signal_manager is None:
            self.logger.info("Akshare 行为证据层未启用，跳过外部信号同步。")
            return

        focus_codes = self._extract_focus_codes(focus_df)
        try:
            self.external_signal_manager.sync_daily_signals(
                latest_trade_day, focus_codes
            )
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("行为证据同步阶段出现异常: %s", exc)

    def _print_preview(self, interfaces: Iterable[str]) -> None:
        preview = list(interfaces)
        self.logger.info("已发现 %s 个项目组件，前 10 个预览：", len(preview))
        for name in preview[:10]:
            self.logger.info(" - %s", name)

    def _apply_fundamental_filters(
        self, universe_df: pd.DataFrame, fundamentals_df: pd.DataFrame
    ) -> pd.DataFrame:
        if fundamentals_df.empty:
            self.logger.info("未生成财务宽表，跳过基本面过滤。")
            return universe_df

        merged = universe_df.merge(fundamentals_df, on="code", how="left")

        def _select_column(df: pd.DataFrame, candidates: list[str]) -> str | None:
            for col in candidates:
                if col in df.columns:
                    return col
            return None

        def _filter_numeric(
            df: pd.DataFrame, candidates: list[str], predicate, desc: str
        ) -> pd.DataFrame:
            target_col = _select_column(df, candidates)
            if target_col is None:
                self.logger.info("缺少 %s 指标列，跳过该条件。", desc)
                return df

            series = pd.to_numeric(df[target_col], errors="coerce")
            before = len(df)
            df = df[predicate(series)]
            self.logger.info("%s 过滤：%s -> %s", desc, before, len(df))
            return df

        merged = _filter_numeric(
            merged,
            ["profit_roeAvg", "profit_roe", "dupont_dupontROE"],
            lambda s: s > 0,
            "ROE 为正",
        )
        merged = _filter_numeric(
            merged,
            ["balance_liabilityToAsset", "balance_assetLiabRatio"],
            lambda s: s < 0.75,
            "资产负债率 < 75%",
        )
        merged = _filter_numeric(
            merged,
            ["cash_flow_CFOToNP", "cash_flow_CFOToOR", "cash_flow_CFOToGr"],
            lambda s: s > 0,
            "经营现金流为正（按 CFO 比率代理）",
        )
        merged = _filter_numeric(
            merged,
            ["growth_YOYNI", "growth_YOYPNI", "growth_YOYEPSBasic"],
            lambda s: s > 0,
            "净利润或 EPS 同比为正",
        )

        return merged

    def run(self) -> None:
        """执行 Baostock 数据导出与候选池筛选示例。"""

        try:
            # 1) 预览当前模块内可用组件（示例信息输出）
            self._print_preview(
                [
                    "BaostockSession",
                    "BaostockDataFetcher",
                    "AshareUniverseBuilder",
                    "FundamentalDataManager",
                ]
            )

            SchemaManager(self.db_writer.engine, db_name=self.db_writer.config.db_name).ensure_all()

            # 2) 获取最近交易日并导出股票列表/元数据（允许离线）
            if self.use_baostock:
                latest_trade_day = self.fetcher.get_latest_trading_date()
            else:
                latest_trade_day = self._infer_latest_trade_day_from_db(
                    base_table="history_daily_kline"
                )
            self.logger.info("最近交易日：%s", latest_trade_day)

            try:
                self._export_index_daily_history(latest_trade_day)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取指数日线失败：%s", exc)

            # 股票列表（至少要拿到 code）
            if self.fetch_stock_meta:
                try:
                    stock_df = self._export_stock_list(latest_trade_day)
                except RuntimeError as exc:
                    self.logger.error("导出股票列表失败: %s", exc)
                    return
            else:
                stock_df = self._load_table("a_share_stock_list")
                if stock_df.empty:
                    stock_df = self._build_stock_list_from_history(
                        base_table="history_daily_kline",
                        trade_day=latest_trade_day,
                    )
                if stock_df.empty:
                    self.logger.error(
                        "股票列表为空：已关闭 Baostock 元数据拉取，但数据库中也没有可用的 a_share_stock_list/history_daily_kline。"
                    )
                    return

            stock_basic_df = None
            if self.fetch_stock_meta:
                try:
                    stock_basic_df = self._export_stock_basic()
                except RuntimeError as exc:
                    self.logger.warning(
                        "导出证券基本资料失败: %s，将跳过上市状态与上市天数过滤。",
                        exc,
                    )
                    stock_basic_df = None
            else:
                stock_basic_df = self._load_table("a_share_stock_basic")
                if stock_basic_df.empty:
                    self.logger.warning(
                        "已关闭 Baostock 元数据拉取，且数据库中未找到 a_share_stock_basic，将跳过上市状态与上市天数过滤。"
                    )
                    stock_basic_df = None

            industry_df = pd.DataFrame()
            if self.fetch_stock_meta:
                try:
                    industry_df = self._export_stock_industry()
                except RuntimeError as exc:
                    self.logger.warning(
                        "导出行业分类数据失败: %s，将继续主流程（不做行业映射）。",
                        exc,
                    )
                    industry_df = pd.DataFrame()
            else:
                industry_df = self._load_table("a_share_stock_industry")

            board_spot = pd.DataFrame()
            if self.board_industry_enabled:
                try:
                    board_spot = self._export_board_industry_spot()
                except Exception as exc:  # noqa: BLE001
                    self.logger.warning("板块快照刷新失败：%s", exc)
                try:
                    self._export_board_industry_constituents(board_spot)
                except Exception as exc:  # noqa: BLE001
                    self.logger.warning("板块成份股维表刷新失败：%s", exc)
                try:
                    self._export_board_industry_history(latest_trade_day, spot=board_spot)
                except Exception as exc:  # noqa: BLE001
                    self.logger.warning("板块数据刷新失败：%s", exc)

            if self.fetch_stock_meta:
                index_membership = self._export_index_members(latest_trade_day)
            else:
                index_membership = self._load_index_membership_from_db()
            fundamentals_wide = pd.DataFrame()
            try:
                if self.refresh_fundamentals:
                    fundamental_codes: set[str] = set().union(
                        *index_membership.values()
                    )
                    if not fundamental_codes:
                        fallback_count = min(
                            self.universe_builder.top_liquidity_count, len(stock_df)
                        )
                        fundamental_codes = set(
                            stock_df["code"].head(fallback_count)
                        )

                    self.logger.info(
                        "基础面刷新开关已开启，本次将对 %s 支股票刷新季频财务数据。",
                        len(fundamental_codes),
                    )
                    fundamentals_wide = self.fundamental_manager.refresh_all(
                        sorted(fundamental_codes),
                        latest_trade_day,
                        quarterly_lookback=4,
                        report_lookback_years=0,
                        adjust_lookback_years=0,
                        update_reports=False,
                        update_corporate_actions=False,
                        update_macro=False,
                    )
                else:
                    self.logger.info(
                        "基础面刷新开关已关闭，本次仅使用数据库中已有的财务表构建宽表。"
                    )
                    fundamentals_wide = self.fundamental_manager.build_latest_wide()
            except Exception as exc:  # noqa: BLE001
                self.logger.warning(
                    "基础面阶段出现异常，将继续主流程（只使用技术面过滤）: %s",
                    exc,
                )

            # 3) 导出最近 N 日历史日线（增量模式）
            try:
                history_df, history_table = self._export_daily_history_incremental(
                    stock_df,
                    latest_trade_day,
                    base_table="history_daily_kline",
                    window_days=self.history_days,
                    fetch_enabled=self.fetch_daily_kline,
                )
            except RuntimeError as exc:
                self.logger.error(
                    "导出最近 %s 个交易日的日线数据失败: %s",
                    self.history_days,
                    exc,
                )
                return

            # 3.1) 单独创建“最近 N 个自然日”的便捷视图（用于你手动查近期数据）

            # 4) 构建候选池并挑选成交额前 N 名
            try:
                universe_df = self.universe_builder.build_universe(
                    stock_df,
                    history_df,
                    stock_basic_df=stock_basic_df,
                    industry_df=industry_df,
                    index_membership=index_membership,
                )
            except RuntimeError as exc:
                self.logger.error("生成当日候选池失败: %s", exc)
                return

            try:
                universe_df = self._apply_fundamental_filters(
                    universe_df, fundamentals_wide
                )
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("基本面过滤阶段出现异常，保留未过滤结果: %s", exc)

            universe_snapshot_table = TABLE_A_SHARE_UNIVERSE
            self.logger.info(
                "已生成候选池 表=%s 行数=%s",
                universe_snapshot_table,
                len(universe_df),
            )

            # 4.1) 将候选池写入 Universe 表快照（按 date 覆盖；避免 replace 破坏索引/主键）
            if universe_df.empty:
                self.logger.warning("候选池为空，跳过 Universe 表落库与后续选股。")
                return

            if not self._table_exists(universe_snapshot_table):
                self.logger.error(
                    "Universe 表 %s 不存在（SchemaManager 未完成建表？），跳过落库与后续选股。",
                    universe_snapshot_table,
                )
                return

            df = universe_df.copy()
            if "tradestatus" in df.columns and "tradeStatus" not in df.columns:
                df = df.rename(columns={"tradestatus": "tradeStatus"})

            universe_columns = [
                "date",
                "code",
                "code_name",
                "tradeStatus",
                "amount",
                "volume",
                "open",
                "high",
                "low",
                "close",
                "ipoDate",
                "type",
                "status",
                "in_hs300",
                "in_zz500",
                "in_sz50",
            ]
            for col in universe_columns:
                if col not in df.columns:
                    df[col] = None
            df = df[universe_columns]

            df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
            df["ipoDate"] = pd.to_datetime(df["ipoDate"], errors="coerce").dt.date

            dates = sorted(set(df["date"].dropna().astype(str)))
            if not dates:
                self.logger.warning(
                    "候选池缺少有效 date 字段，跳过 Universe 表落库与后续选股。"
                )
                return

            delete_stmt = text(
                f"DELETE FROM `{universe_snapshot_table}` WHERE `date` IN :dates"
            ).bindparams(bindparam("dates", expanding=True))

            with self.db_writer.engine.begin() as conn:
                conn.execute(delete_stmt, {"dates": dates})
                df.to_sql(
                    universe_snapshot_table,
                    conn,
                    if_exists="append",
                    index=False,
                    method="multi",
                    chunksize=1000,
                )

            self.logger.info(
                "Universe 表快照已写入 %s：rows=%s, dates=%s",
                universe_snapshot_table,
                len(df),
                dates,
            )

            try:
                top_liquidity = self.universe_builder.pick_top_liquidity(universe_df)
            except RuntimeError as exc:
                self.logger.error(
                    "挑选成交额前 %s 名失败: %s",
                    self.universe_builder.top_liquidity_count,
                    exc,
                )
                return

            top_liquidity_table = self._save_sample(
                top_liquidity, "a_share_top_liquidity"
            )
            self.logger.info(
                "已将成交额排序结果写入表 %s，可用于筛选高流动性标的。",
                top_liquidity_table,
            )

            self._sync_external_signals(latest_trade_day, top_liquidity)

            # 5) 提示历史日线路径
            self.logger.debug(
                "历史日线窗口数据来源：%s（切片最近 %s 个交易日）", history_table, self.history_days
            )
            recent_view = getattr(self, "_last_history_calendar_view", None)
            if recent_view:
                self.logger.debug(
                    "近期自然日便捷视图已更新：%s（最近 %s 天）",
                    recent_view,
                    self.history_view_days,
                )
        finally:
            self._cleanup_session()
            self.db_writer.dispose()

if __name__ == "__main__":
    AshareApp().run()

================================================================================
FILE: ashare/baostock_core.py
================================================================================

"""Baostock 数据访问层封装。"""

from __future__ import annotations

import logging
import time
from datetime import date, datetime, timedelta
from typing import Any, Callable, Iterable

import baostock as bs
import pandas as pd

from .baostock_session import BaostockSession
from .config import get_section

# adjustflag=3 表示不复权，适用于与实盘价格对齐的场景
ADJUSTFLAG_NONE = "3"


class BaostockDataFetcher:
    """封装常用 Baostock 数据访问接口。"""

    def __init__(
        self,
        session: BaostockSession,
        max_retries: int | None = None,
        retry_backoff: float | None = None,
    ) -> None:
        """保存会话引用并加载接口重试配置。"""

        self.session = session
        cfg = get_section("baostock")
        raw_retry = max_retries if max_retries is not None else cfg.get("max_retries")
        raw_backoff = (
            retry_backoff if retry_backoff is not None else cfg.get("retry_sleep")
        )

        try:
            parsed_retry = int(raw_retry) if raw_retry is not None else 3
        except (TypeError, ValueError):
            parsed_retry = 3

        try:
            parsed_backoff = float(raw_backoff) if raw_backoff is not None else 2.0
        except (TypeError, ValueError):
            parsed_backoff = 2.0

        self.api_max_retries = max(1, parsed_retry)
        self.api_retry_backoff = max(0.5, parsed_backoff)
        self.logger = logging.getLogger(__name__)

    def _ensure_session(self) -> None:
        """确保会话已登录。"""

        self.session.ensure_alive()

    def _calc_backoff(self, attempt_index: int) -> float:
        base = self.api_retry_backoff
        max_sleep = 20.0
        return min(max_sleep, base * (2 ** (attempt_index - 1)))

    def _should_force_refresh(self, error: Exception | None, attempt_index: int) -> bool:
        """判断异常是否需要强制重连，避免无谓的频繁登录。"""

        if error is None:
            return False

        message = str(error)
        auth_markers = ["10001001", "未登录", "not logged", "登录失败"]
        network_markers = ["10054", "reset", "断开", "Connection reset"]

        if any(flag in message for flag in auth_markers):
            return True

        if attempt_index >= self.api_max_retries:
            return True

        return any(flag in message for flag in network_markers)

    def _call_with_retry(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
        """对 Baostock 查询执行重试与会话自愈。"""

        last_error: Exception | None = None
        for attempt in range(1, self.api_max_retries + 1):
            self._ensure_session()
            try:
                result = func(*args, **kwargs)
                if getattr(result, "error_code", "0") == "0":
                    return result
                last_error = RuntimeError(
                    f"Baostock 调用失败: {result.error_code}, {result.error_msg}"
                )
            except Exception as exc:  # noqa: BLE001
                last_error = exc

            if attempt >= self.api_max_retries:
                break

            self.logger.warning(
                "Baostock 调用异常（第 %s 次），即将重试：%s",
                attempt,
                last_error,
            )
            try:
                force_refresh = self._should_force_refresh(last_error, attempt)
                self.session.ensure_alive(
                    force_refresh=force_refresh, force_check=not force_refresh
                )
            except Exception:  # noqa: BLE001
                self.session.logged_in = False
            time.sleep(self._calc_backoff(attempt))

        if last_error:
            raise last_error
        raise RuntimeError("未知的 Baostock 调用异常。")

    def _resultset_to_df(self, rs: Any) -> pd.DataFrame:
        """将 Baostock ResultSet 转换为 DataFrame，并在失败时抛出错误。"""

        if rs.error_code != "0":
            raise RuntimeError(f"Baostock 调用失败: {rs.error_code}, {rs.error_msg}")

        rows: list[Iterable[str]] = []
        while rs.next():
            rows.append(rs.get_row_data())
        return pd.DataFrame(rows, columns=rs.fields)

    def get_trade_calendar(self, start_date: str, end_date: str) -> pd.DataFrame:
        """查询交易日历并过滤出交易日。"""

        rs = self._call_with_retry(bs.query_trade_dates, start_date, end_date)
        df = self._resultset_to_df(rs)
        trading_df = df[df["is_trading_day"] == "1"].reset_index(drop=True)
        return trading_df[["calendar_date", "is_trading_day"]]

    def get_latest_trading_date(self, lookback_days: int = 365) -> str:
        """获取最近一个交易日。"""

        today = date.today()
        start = today - timedelta(days=lookback_days)
        trading_calendar = self.get_trade_calendar(start.isoformat(), today.isoformat())
        if trading_calendar.empty:
            raise ValueError(
                "在最近 {days} 天内未找到交易日，无法确定最近交易日。".format(
                    days=lookback_days
                )
            )

        latest_date = (
            trading_calendar.sort_values("calendar_date")["calendar_date"].iloc[-1]
        )
        return str(latest_date)

    def get_stock_basic(
        self, code: str | None = None, code_name: str | None = None
    ) -> pd.DataFrame:
        """查询证券基本资料。"""

        rs = self._call_with_retry(bs.query_stock_basic, code, code_name)
        df = self._resultset_to_df(rs)
        if df.empty:
            return df

        date_cols = ["ipoDate", "outDate", "endDate"]
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors="coerce")
        for col in ["type", "status"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")
        return df

    def get_stock_industry(self, code: str | None = None) -> pd.DataFrame:
        """查询行业分类信息。"""

        rs = self._call_with_retry(bs.query_stock_industry, code)
        df = self._resultset_to_df(rs)
        if df.empty:
            return df

        if "updateDate" in df.columns:
            df["updateDate"] = pd.to_datetime(df["updateDate"], errors="coerce")
        return df

    def get_index_members(self, index: str, date: str | None = None) -> pd.DataFrame:
        """查询指定指数的成分股列表。"""

        self._ensure_session()
        index_map = {
            "hs300": bs.query_hs300_stocks,
            "zz500": bs.query_zz500_stocks,
            "sz50": bs.query_sz50_stocks,
        }
        index_key = index.lower()
        if index_key not in index_map:
            raise ValueError("暂不支持的指数标识：{value}".format(value=index))

        query_fn = index_map[index_key]
        if date:
            rs = self._call_with_retry(query_fn, date=date)
        else:
            rs = self._call_with_retry(query_fn)
        df = self._resultset_to_df(rs)
        if df.empty:
            return df

        df.insert(0, "index", index_key)
        if "updateDate" in df.columns:
            df["updateDate"] = pd.to_datetime(df["updateDate"], errors="coerce")
        return df

    def get_stock_list(self, trade_date: str, fallback_days: int = 15) -> pd.DataFrame:
        """按交易日获取 A 股列表。

        参数
        ----------
        trade_date : str
            期望的交易日，格式为 "YYYY-MM-DD"。
        fallback_days : int, 默认 15
            如果该日期没有返回股票列表（例如当天数据尚未生成、
            或遇到节假日），则自动向前回退最多 fallback_days 天，
            返回最近一个有数据的交易日的股票列表。

        返回
        ----------
        pd.DataFrame
            含有 code, code_name, tradeStatus 列的股票列表。
            若在 fallback_days 内仍未找到任何数据，则返回空 DataFrame。
        """

        def _query(day: str) -> pd.DataFrame:
            """内部封装一次 query_all_stock 调用。"""

            rs = self._call_with_retry(bs.query_all_stock, day)
            df = self._resultset_to_df(rs)
            return df

        # 1) 先尝试用户指定的日期
        df = _query(trade_date)

        # 2) 如果没有数据，则向前回退，最多 fallback_days 天
        if df.empty:
            current = datetime.strptime(trade_date, "%Y-%m-%d").date()
            for i in range(1, fallback_days + 1):
                prev_day = (current - timedelta(days=i)).isoformat()
                df = _query(prev_day)
                if not df.empty:
                    # 找到了就停止回退，使用这个日期的数据
                    break

        # 3) 如果依然没有数据，直接返回空 DataFrame，由上层决定如何处理
        if df.empty:
            return df

        # 4) 过滤出 A 股主板并只保留常用字段
        prefixes = ("sh.60", "sz.00")
        filtered = df[
            df["code"].str.startswith(prefixes) & (df["tradeStatus"] == "1")
        ].reset_index(drop=True)

        columns = [
            col for col in ["code", "code_name", "tradeStatus"] if col in filtered
        ]
        return filtered[columns]

    def get_kline(
        self,
        code: str,
        start_date: str,
        end_date: str,
        freq: str = "d",
        adjustflag: str = ADJUSTFLAG_NONE,
    ) -> pd.DataFrame:
        """获取 K 线行情数据。"""

        fields = (
            "date,code,open,high,low,close,preclose,volume,amount,"
            "adjustflag,tradestatus,pctChg,isST"
        )
        rs = self._call_with_retry(
            bs.query_history_k_data_plus,
            code,
            fields,
            start_date=start_date,
            end_date=end_date,
            frequency=freq,
            adjustflag=adjustflag,
        )
        df = self._resultset_to_df(rs)
        if df.empty:
            return df

        numeric_cols = [
            "open",
            "high",
            "low",
            "close",
            "preclose",
            "volume",
            "amount",
            "pctChg",
        ]
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")
        return df

    def get_profit_data(self, code: str, year: int, quarter: int) -> pd.DataFrame:
        """获取利润表数据。"""

        rs = self._call_with_retry(bs.query_profit_data, code, year, quarter)
        return self._resultset_to_df(rs)

    def get_growth_data(self, code: str, year: int, quarter: int) -> pd.DataFrame:
        """获取成长能力数据。"""

        rs = self._call_with_retry(bs.query_growth_data, code, year, quarter)
        return self._resultset_to_df(rs)

    def get_balance_data(self, code: str, year: int, quarter: int) -> pd.DataFrame:
        """获取资产负债表数据。"""

        rs = self._call_with_retry(bs.query_balance_data, code, year, quarter)
        return self._resultset_to_df(rs)

    def get_cash_flow_data(self, code: str, year: int, quarter: int) -> pd.DataFrame:
        """获取现金流量表数据。"""

        rs = self._call_with_retry(bs.query_cash_flow_data, code, year, quarter)
        return self._resultset_to_df(rs)

    def get_operation_data(self, code: str, year: int, quarter: int) -> pd.DataFrame:
        """获取营运能力数据。"""

        rs = self._call_with_retry(bs.query_operation_data, code, year, quarter)
        return self._resultset_to_df(rs)

    def get_dupont_data(self, code: str, year: int, quarter: int) -> pd.DataFrame:
        """获取杜邦指标数据。"""

        rs = self._call_with_retry(bs.query_dupont_data, code, year, quarter)
        return self._resultset_to_df(rs)

    def get_performance_express_report(
        self, code: str, start_date: str | None = None, end_date: str | None = None
    ) -> pd.DataFrame:
        """获取业绩快报。"""

        rs = self._call_with_retry(
            bs.query_performance_express_report,
            code,
            start_date,
            end_date,
        )
        return self._resultset_to_df(rs)

    def get_forecast_report(
        self, code: str, start_date: str | None = None, end_date: str | None = None
    ) -> pd.DataFrame:
        """获取业绩预告。"""

        rs = self._call_with_retry(
            bs.query_forecast_report, code, start_date, end_date
        )
        return self._resultset_to_df(rs)

    def get_dividend_data(
        self, code: str, year: int, year_type: str = "report"
    ) -> pd.DataFrame:
        """获取分红送配信息。"""

        rs = self._call_with_retry(bs.query_dividend_data, code, year, year_type)
        return self._resultset_to_df(rs)

    def get_adjust_factor(
        self, code: str, start_date: str | None = None, end_date: str | None = None
    ) -> pd.DataFrame:
        """获取复权因子信息。"""

        rs = self._call_with_retry(bs.query_adjust_factor, code, start_date, end_date)
        return self._resultset_to_df(rs)

    def get_deposit_rate_data(
        self, start_date: str = "", end_date: str = ""
    ) -> pd.DataFrame:
        """获取存款利率数据。"""

        rs = self._call_with_retry(
            bs.query_deposit_rate_data, start_date=start_date, end_date=end_date
        )
        return self._resultset_to_df(rs)

    def get_loan_rate_data(
        self, start_date: str = "", end_date: str = ""
    ) -> pd.DataFrame:
        """获取贷款利率数据。"""

        rs = self._call_with_retry(
            bs.query_loan_rate_data, start_date=start_date, end_date=end_date
        )
        return self._resultset_to_df(rs)

    def get_required_reserve_ratio_data(
        self, start_date: str = "", end_date: str = "", year_type: str = "0"
    ) -> pd.DataFrame:
        """获取存款准备金率数据。"""

        rs = self._call_with_retry(
            bs.query_required_reserve_ratio_data,
            start_date,
            end_date,
            year_type,
        )
        return self._resultset_to_df(rs)

    def get_money_supply_data_month(
        self, start_date: str = "", end_date: str = ""
    ) -> pd.DataFrame:
        """获取月度货币供应量数据。"""

        rs = self._call_with_retry(
            bs.query_money_supply_data_month, start_date=start_date, end_date=end_date
        )
        return self._resultset_to_df(rs)

    def get_money_supply_data_year(
        self, start_date: str = "", end_date: str = ""
    ) -> pd.DataFrame:
        """获取年度货币供应量数据。"""

        rs = self._call_with_retry(
            bs.query_money_supply_data_year, start_date=start_date, end_date=end_date
        )
        return self._resultset_to_df(rs)

    def get_shibor_data(
        self, start_date: str = "", end_date: str = ""
    ) -> pd.DataFrame:
        """获取银行间同业拆借利率（Shibor）。"""

        if not hasattr(bs, "query_shibor_data"):
            raise RuntimeError("当前 Baostock 版本不支持 Shibor 接口。")

        rs = self._call_with_retry(
            bs.query_shibor_data, start_date=start_date, end_date=end_date
        )
        return self._resultset_to_df(rs)


if __name__ == "__main__":
    session = BaostockSession()
    fetcher = BaostockDataFetcher(session)

    latest_date = fetcher.get_latest_trading_date()
    stock_df = fetcher.get_stock_list(latest_date)

    if not stock_df.empty:
        sample_code = stock_df.iloc[0]["code"]
        start_day = (
            datetime.strptime(latest_date, "%Y-%m-%d").date() - timedelta(days=30)
        ).isoformat()
        kline_df = fetcher.get_kline(sample_code, start_day, latest_date)
        print(f"最近 30 天 {sample_code} K 线行数：{len(kline_df)}")
    else:
        print("未获取到股票列表，无法演示 K 线数据查询。")

================================================================================
FILE: ashare/baostock_session.py
================================================================================

"""Baostock 会话管理封装。

该模块提供 `BaostockSession` 类，用于管理 Baostock 的登录与登出，
避免在进程结束时忘记释放会话。
"""

from __future__ import annotations

import atexit
import contextlib
import io
import logging
import os
import socket
import time
from typing import Any

import baostock as bs

from .config import get_section


class BaostockSession:
    """管理 Baostock 登录状态的简单封装。"""

    retry: int = 3
    retry_sleep: float = 3.0
    logged_in: bool = False
    socket_timeout: float | None = None
    alive_check_interval: float = 60.0

    def __init__(self, retry: int | None = None, retry_sleep: float | None = None) -> None:
        """
        初始化会话参数并注册退出时的登出钩子。

        参数允许被覆盖，以便在特殊场景下调整重试策略。
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        cfg = get_section("baostock")

        if retry is None:
            retry = cfg.get("retry", self.retry)
        if retry_sleep is None:
            retry_sleep = cfg.get("retry_sleep", self.retry_sleep)
        socket_timeout_raw = os.getenv(
            "ASHARE_BAOSTOCK_SOCKET_TIMEOUT", cfg.get("socket_timeout")
        )
        alive_interval_raw = os.getenv(
            "ASHARE_BAOSTOCK_KEEPALIVE_INTERVAL",
            cfg.get("keepalive_interval", self.alive_check_interval),
        )

        try:
            self.retry = int(retry)
        except (TypeError, ValueError):
            self.retry = 3

        try:
            self.retry_sleep = float(retry_sleep)
        except (TypeError, ValueError):
            self.retry_sleep = 3.0

        try:
            timeout_value = float(socket_timeout_raw) if socket_timeout_raw else None
        except (TypeError, ValueError):
            timeout_value = None
        self.socket_timeout = timeout_value
        try:
            alive_interval = float(alive_interval_raw)
        except (TypeError, ValueError):
            alive_interval = self.alive_check_interval
        self.alive_check_interval = max(60.0, alive_interval)
        self._last_alive_ts: float = 0.0
        if self.socket_timeout and self.socket_timeout > 0:
            socket.setdefaulttimeout(self.socket_timeout)

        atexit.register(self.logout)

    def connect(self) -> None:
        """
        登录 Baostock，带有限次重试。

        - 如果已经登录则直接返回；
        - 登录失败会按配置的次数与间隔重试；
        - 全部失败后抛出带详细信息的 RuntimeError。
        """
        if self.logged_in:
            return

        last_error_msg = ""
        for attempt in range(1, self.retry + 1):
            with contextlib.redirect_stdout(io.StringIO()) as buf:
                result = bs.login()
            out = buf.getvalue().strip()
            if out:
                self.logger.debug("baostock: %s", out)
            if result.error_code == "0":
                self.logged_in = True
                self._last_alive_ts = time.time()
                return

            last_error_msg = result.error_msg
            if attempt < self.retry:
                time.sleep(self.retry_sleep)

        raise RuntimeError(
            "Baostock 登录失败，已重试 {count} 次，最后一次错误：{msg}".format(
                count=self.retry,
                msg=last_error_msg or "未知错误",
            )
        )

    def logout(self) -> None:
        """登出 Baostock 并重置状态。"""
        try:
            if self.logged_in:
                with contextlib.redirect_stdout(io.StringIO()) as buf:
                    bs.logout()
                out = buf.getvalue().strip()
                if out:
                    self.logger.debug("baostock: %s", out)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("登出失败：%s", exc)
        finally:
            # baostock 的 logout 在部分版本/异常路径下可能不会彻底关闭底层 socket，
            # 在 -Wd 下会看到 ResourceWarning: unclosed <socket.socket ...>。
            # 这里做一次“尽力而为”的反射式清理：只在 baostock 模块对象树里查找 socket 并关闭。
            self._force_close_baostock_sockets()
            # 无论登出是否成功，都要清理状态，避免遗留连接或阻塞退出
            self.logged_in = False
            self._last_alive_ts = 0.0

    def _force_close_baostock_sockets(self) -> None:
        """尽最大努力关闭 baostock 库内部遗留的 socket，避免解释器退出时报 ResourceWarning。

        注意：这里不会扫描全局所有对象（避免误伤 DB/HTTP 等连接），仅遍历 baostock 模块可达对象。
        """
        try:
            root = bs
        except Exception:  # noqa: BLE001
            return

        visited: set[int] = set()
        closed_count = 0

        def _close_sock(sock_obj: socket.socket) -> None:
            nonlocal closed_count
            try:
                # 已关闭的 socket.close() 再调用一般是安全的，但仍做 try/except 兜底
                sock_obj.close()
                closed_count += 1
            except Exception:  # noqa: BLE001
                return

        def _walk(obj: Any, depth: int) -> None:
            if obj is None:
                return
            if depth > 4:
                return

            oid = id(obj)
            if oid in visited:
                return
            visited.add(oid)

            # 直接命中 socket
            if isinstance(obj, socket.socket):
                _close_sock(obj)
                return

            # 基础类型不展开
            if isinstance(obj, (str, bytes, int, float, bool)):
                return

            # 常见容器展开
            if isinstance(obj, (list, tuple, set, frozenset)):
                for item in obj:
                    _walk(item, depth + 1)
                return
            if isinstance(obj, dict):
                for item in obj.values():
                    _walk(item, depth + 1)
                return

            # 常见“连接属性名”优先探测（避免全量遍历 __dict__ 太重）
            for attr in (
                "sock",
                "socket",
                "_sock",
                "_socket",
                "conn",
                "connection",
                "client",
                "_client",
                "tcpCliSock",
                "tcp_socket",
            ):
                try:
                    if hasattr(obj, attr):
                        _walk(getattr(obj, attr), depth + 1)
                except Exception:  # noqa: BLE001
                    continue

            # 最后再尽力遍历对象字典（限制深度 + visited 防爆）
            try:
                obj_dict = getattr(obj, "__dict__", None)
                if isinstance(obj_dict, dict):
                    for v in obj_dict.values():
                        _walk(v, depth + 1)
            except Exception:  # noqa: BLE001
                return

        try:
            _walk(root, 0)
        except Exception:  # noqa: BLE001
            return

        if closed_count:
            self.logger.debug("已强制关闭 baostock 遗留 socket：%s 个", closed_count)

    def ensure_alive(self, force_refresh: bool = False, force_check: bool = False) -> None:
        """确保会话可用，必要时重新登录或主动探测。

        - `force_refresh` 为 ``True`` 时直接重新登录；
        - `force_check` 为 ``True`` 时跳过探测节流，立即执行一次有效性检查。
        """

        if force_refresh:
            self.logger.info("强制刷新会话，重新登录。")
            self.reconnect()
            return

        if not self.logged_in:
            self.logger.info("会话未登录，执行登录。")
            self.connect()
            return

        now = time.time()
        if not force_check and now - self._last_alive_ts < self.alive_check_interval:
            return

        try:
            self._probe_alive()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("会话检查失败: %s", exc)
            self.reconnect()
        else:
            self._last_alive_ts = time.time()

    def _probe_alive(self) -> None:
        """通过轻量查询验证会话可用性。"""

        try:
            rs = bs.query_sz50_stocks()
            if getattr(rs, "error_code", None) != "0":
                raise RuntimeError(
                    f"Baostock 会话失效，错误代码：{getattr(rs, 'error_code', '未知')}，"
                    f"错误信息：{getattr(rs, 'error_msg', '未知')}"
                )
        except Exception as exc:
            self.logger.error("会话验证失败: %s", exc)
            raise RuntimeError(f"Baostock 会话失效，错误详情：{exc}") from exc

    def reconnect(self, max_retries: int = 3) -> None:
        """重新建立 Baostock 连接，并限制最大重试次数。"""

        retries = 0
        while retries < max_retries:
            try:
                self.logger.info("正在尝试重连 Baostock，会话重试次数：%s", retries + 1)
                self.logout()
                self.connect()
                self.logger.info("Baostock 会话重连成功。")
                return
            except Exception as exc:
                retries += 1
                self.logged_in = False
                self._last_alive_ts = 0.0
                self.logger.warning("Baostock 会话重连失败（第 %s 次）：%s", retries, exc)
                if retries < max_retries:
                    time.sleep(2)

        raise RuntimeError("Baostock 会话重连失败，已达到最大重试次数。")


def _demo() -> None:
    """简单示例：登录后查询全部股票并打印数量。"""
    session = BaostockSession()
    session.connect()

    rs = bs.query_all_stock()
    rows = []
    while rs.error_code == "0" and rs.next():
        rows.append(rs.get_row_data())

    print(f"[demo] 全部股票数量：{len(rows)}")


if __name__ == "__main__":
    _demo()

================================================================================
FILE: ashare/baostock_tasks.py
================================================================================

"""Baostock 相关的数据采集任务。"""

from __future__ import annotations

import os
import time
from datetime import datetime, timedelta

import pandas as pd

from .baostock_core import ADJUSTFLAG_NONE, BaostockDataFetcher
from .baostock_session import BaostockSession


def fetch_recent_30d_daily_k_all(
    output_dir: str, session: BaostockSession | None = None
) -> None:
    """采集全市场最近 30 天的日 K 线数据并保存为 CSV。"""

    def _fetch_kline_with_retry(
        code: str,
        start_date: str,
        end_date: str,
        max_retries: int = 3,
    ) -> pd.DataFrame:
        last_exc: Exception | None = None
        for attempt in range(1, max_retries + 1):
            try:
                session.ensure_alive()
                return fetcher.get_kline(
                    code,
                    start_date,
                    end_date,
                    freq="d",
                    adjustflag=ADJUSTFLAG_NONE,
                )
            except Exception as exc:  # noqa: BLE001
                last_exc = exc
                if attempt >= max_retries:
                    break

                wait_time = min(10.0, 2 ** (attempt - 1))
                print(
                    "[%s] 拉取失败（第 %s/%s 次），%s 秒后重试：%s"
                    % (code, attempt, max_retries, wait_time, exc)
                )
                session.ensure_alive(force_refresh=True)
                time.sleep(wait_time)

        raise RuntimeError(f"{code} 拉取失败：{last_exc}")

    external_session = session is not None
    session = session or BaostockSession()
    fetcher = BaostockDataFetcher(session)
    session.ensure_alive()

    end_date = fetcher.get_latest_trading_date()
    end_day = datetime.strptime(end_date, "%Y-%m-%d").date()
    start_date = (end_day - timedelta(days=30)).isoformat()

    stock_df = fetcher.get_stock_list(end_date)
    if stock_df.empty:
        print("未获取到股票列表，终止采集。")
        return

    os.makedirs(output_dir, exist_ok=True)

    total = len(stock_df)
    print(
        "开始采集最近 30 天日 K 线：{count} 只股票，区间 {start} - {end}".format(
            count=total, start=start_date, end=end_date
        )
    )

    failed_codes: list[tuple[str, str]] = []

    try:
        for idx, code in enumerate(stock_df["code"], start=1):
            try:
                kline_df = _fetch_kline_with_retry(code, start_date, end_date)
            except Exception as exc:  # noqa: BLE001
                failed_codes.append((code, str(exc)))
                continue

            normalized_code = code.replace(".", "").upper()
            file_path = os.path.join(output_dir, f"{normalized_code}_recent30d.csv")
            kline_df.to_csv(file_path, index=False)

            if idx % 100 == 0:
                print("已处理 {done}/{total} 只股票".format(done=idx, total=total))
    finally:
        if not external_session:
            session.logout()

    print("采集完成，共处理 {count} 只股票。".format(count=total))
    if failed_codes:
        print("以下股票拉取失败（未写入 CSV）：")
        for code, error_msg in failed_codes:
            print(f"- {code}: {error_msg}")


if __name__ == "__main__":
    fetch_recent_30d_daily_k_all(output_dir="output/kline_daily")

================================================================================
FILE: ashare/calc_metrics.py
================================================================================

from __future__ import annotations

import datetime as dt
from typing import Dict, List, Tuple

import pandas as pd
from sqlalchemy import bindparam, text

from .db import DatabaseConfig, MySQLWriter
from .schema_manager import (
    STRATEGY_CODE_MA5_MA20_TREND,
    TABLE_STRATEGY_TRADE_METRICS,
    SchemaManager,
)
from .utils import setup_logger


class TradeMetricsCalculator:
    def __init__(self) -> None:
        self.logger = setup_logger()
        self.db_writer = MySQLWriter(DatabaseConfig.from_env())
        self.schema_manager = SchemaManager(self.db_writer.engine)
        tables = self.schema_manager.get_table_names()
        self.events_table = tables.signal_events_table
        self.indicator_table = tables.indicator_table

    def _load_events(self) -> pd.DataFrame:
        if not self.schema_manager._table_exists(self.events_table):
            self.logger.warning("事件表 %s 不存在，跳过交易结果计算。", self.events_table)
            return pd.DataFrame()

        stmt = text(
            f"""
            SELECT
              `sig_date`,
              `code`,
              COALESCE(`final_action`, `signal`) AS `action`,
              COALESCE(`final_reason`, `reason`) AS `reason`,
              `strategy_code`
            FROM `{self.events_table}`
            WHERE `strategy_code` = :strategy
            ORDER BY `code`,`sig_date`
            """
        )
        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(stmt, conn, params={"strategy": STRATEGY_CODE_MA5_MA20_TREND})
        df["sig_date"] = pd.to_datetime(df["sig_date"], errors="coerce").dt.date
        df["code"] = df["code"].astype(str)
        return df

    def _load_prices(self, dates: List[dt.date], codes: List[str]) -> pd.DataFrame:
        if not self.schema_manager._table_exists(self.indicator_table):
            return pd.DataFrame()

        stmt = (
            text(
                f"""
                SELECT `trade_date`,`code`,`close`,`atr14`
                FROM `{self.indicator_table}`
                WHERE `trade_date` IN :dates AND `code` IN :codes
                """
            ).bindparams(bindparam("dates", expanding=True), bindparam("codes", expanding=True))
        )
        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(
                stmt,
                conn,
                params={"dates": [d.isoformat() for d in dates], "codes": codes},
            )
        df["trade_date"] = pd.to_datetime(df["trade_date"], errors="coerce").dt.date
        df["code"] = df["code"].astype(str)
        return df

    def _resolve_price_map(self, price_df: pd.DataFrame) -> Dict[Tuple[dt.date, str], Dict[str, float]]:
        if price_df.empty:
            return {}
        price_df = price_df.dropna(subset=["trade_date", "code"]).copy()
        price_df["close"] = pd.to_numeric(price_df["close"], errors="coerce")
        price_df["atr14"] = pd.to_numeric(price_df["atr14"], errors="coerce")
        price_df = price_df.sort_values(["trade_date", "code"]).drop_duplicates(subset=["trade_date", "code"], keep="last")
        return price_df.set_index(["trade_date", "code"])[["close", "atr14"]].to_dict(orient="index")

    def _build_metrics(self, events: pd.DataFrame, price_map: Dict[Tuple[dt.date, str], Dict[str, float]]) -> pd.DataFrame:
        rows: List[Dict[str, object]] = []
        grouped = events.groupby("code", sort=False)
        for code, sub in grouped:
            entry_row: Dict[str, object] | None = None
            for _, row in sub.iterrows():
                action = str(row.get("action") or "").upper()
                sig_date = row.get("sig_date")
                if not isinstance(sig_date, dt.date):
                    continue
                if entry_row is None and action in {"BUY", "BUY_CONFIRM"}:
                    entry_row = row.to_dict()
                    continue
                if entry_row and action in {"SELL"}:
                    entry_date = entry_row.get("sig_date")
                    if not isinstance(entry_date, dt.date):
                        entry_row = None
                        continue
                    entry_price_info = price_map.get((entry_date, code))
                    exit_price_info = price_map.get((sig_date, code))
                    if not entry_price_info or not exit_price_info:
                        entry_row = None
                        continue
                    entry_price = entry_price_info.get("close")
                    atr_at_entry = entry_price_info.get("atr14")
                    exit_price = exit_price_info.get("close")
                    if entry_price is None or exit_price is None:
                        entry_row = None
                        continue
                    pnl_pct = (exit_price - entry_price) / entry_price if entry_price else None
                    atr_denom = (atr_at_entry / entry_price) if entry_price and atr_at_entry else None
                    pnl_atr_ratio = (pnl_pct / atr_denom) if (pnl_pct is not None and atr_denom) else None
                    holding_days = (sig_date - entry_date).days if isinstance(sig_date, dt.date) else None
                    rows.append(
                        {
                            "strategy_code": STRATEGY_CODE_MA5_MA20_TREND,
                            "code": code,
                            "entry_date": entry_date,
                            "entry_price": entry_price,
                            "exit_date": sig_date,
                            "exit_price": exit_price,
                            "atr_at_entry": atr_at_entry,
                            "pnl_pct": pnl_pct,
                            "pnl_atr_ratio": pnl_atr_ratio,
                            "holding_days": holding_days,
                            "exit_reason": row.get("reason"),
                        }
                    )
                    entry_row = None
        return pd.DataFrame(rows)

    def run(self) -> None:
        self.schema_manager.ensure_all()
        events = self._load_events()
        if events.empty:
            self.logger.info("无交易事件，跳过交易指标计算。")
            return

        dates = [d for d in events["sig_date"].dropna().unique().tolist() if isinstance(d, dt.date)]
        codes = events["code"].dropna().astype(str).unique().tolist()
        price_df = self._load_prices(dates, codes)
        price_map = self._resolve_price_map(price_df)
        metrics_df = self._build_metrics(events, price_map)
        if metrics_df.empty:
            self.logger.info("未能生成任何交易指标行。")
            return

        delete_stmt = text(
            f"DELETE FROM `{TABLE_STRATEGY_TRADE_METRICS}` WHERE `strategy_code` = :strategy"
        )
        with self.db_writer.engine.begin() as conn:
            conn.execute(delete_stmt, {"strategy": STRATEGY_CODE_MA5_MA20_TREND})
        self.db_writer.write_dataframe(metrics_df, TABLE_STRATEGY_TRADE_METRICS, if_exists="append")
        self.logger.info("已写入 %s 条交易结果指标。", len(metrics_df))


def run() -> None:
    TradeMetricsCalculator().run()


if __name__ == "__main__":
    run()

================================================================================
FILE: ashare/chip_filter.py
================================================================================

from __future__ import annotations

import datetime as dt
from typing import Iterable, List, Tuple

import numpy as np

import pandas as pd
from sqlalchemy import bindparam, inspect, text

from .db import DatabaseConfig, MySQLWriter
from .schema_manager import TABLE_STRATEGY_CHIP_FILTER
from .utils import setup_logger


class ChipFilter:
    """筹码因子计算：从股东户数明细读取并覆盖写入单表。"""

    def __init__(self) -> None:
        self.logger = setup_logger()
        self.db_writer = MySQLWriter(DatabaseConfig.from_env())
        self.table = TABLE_STRATEGY_CHIP_FILTER

    def _table_exists(self, table: str) -> bool:
        try:
            with self.db_writer.engine.begin() as conn:
                conn.execute(text(f"SELECT 1 FROM `{table}` LIMIT 1"))
            return True
        except Exception:
            return False

    def _resolve_gdhs_columns(
        self, table: str
    ) -> Tuple[str | None, str | None, str | None, str | None]:
        stmt = text(f"SELECT * FROM `{table}` LIMIT 0")
        with self.db_writer.engine.begin() as conn:
            meta_df = pd.read_sql_query(stmt, conn)
        columns = set(meta_df.columns)
        code_col = "code" if "code" in columns else None
        announce_candidates = [
            "公告日期",
            "股东户数公告日期",
            "股东户数统计截止日",
            "period",
        ]
        delta_pct_candidates = [
            "股东户数-增减比例",
            "增减比例",
            "holder_change_ratio",
        ]
        announce_col = next((c for c in announce_candidates if c in columns), None)
        delta_pct_col = next((c for c in delta_pct_candidates if c in columns), None)
        delta_abs_col = None
        if "股东户数-变动数量" in columns:
            delta_abs_col = "股东户数-变动数量"
        elif "holder_change" in columns:
            delta_abs_col = "holder_change"
        return code_col, announce_col, delta_pct_col, delta_abs_col

    def _load_gdhs_detail(
        self, codes: Iterable[str], latest_date: str | dt.date, table: str
    ) -> pd.DataFrame:
        code_col, announce_col, delta_pct_col, delta_abs_col = self._resolve_gdhs_columns(table)
        if code_col is None or announce_col is None:
            return pd.DataFrame()

        select_cols = [code_col, announce_col]
        for col in [delta_pct_col, delta_abs_col]:
            if col:
                select_cols.append(col)

        stmt = (
            text(
                f"""
                SELECT {",".join(f"`{c}`" for c in select_cols)}
                FROM `{table}`
                WHERE `{code_col}` IN :codes
                """
            ).bindparams(bindparam("codes", expanding=True))
        )
        with self.db_writer.engine.begin() as conn:
            try:
                df = pd.read_sql_query(stmt, conn, params={"codes": list(codes)})
            except Exception:
                return pd.DataFrame()

        rename_map = {code_col: "code", announce_col: "announce_date"}
        if delta_pct_col:
            rename_map[delta_pct_col] = "gdhs_delta_pct"
        if delta_abs_col:
            rename_map[delta_abs_col] = "gdhs_delta_raw"
        df = df.rename(columns=rename_map)
        df["code"] = df["code"].astype(str)
        df["announce_date"] = pd.to_datetime(df["announce_date"], errors="coerce")
        latest_ts = pd.to_datetime(latest_date, errors="coerce")
        if pd.isna(latest_ts):
            latest_ts = pd.Timestamp.max
        df = df[df["announce_date"] <= latest_ts]
        df["gdhs_delta_pct"] = pd.to_numeric(df.get("gdhs_delta_pct"), errors="coerce")
        df["gdhs_delta_raw"] = pd.to_numeric(df.get("gdhs_delta_raw"), errors="coerce")
        return df.dropna(subset=["announce_date"]).sort_values(["code", "announce_date"]).reset_index(drop=True)

    def _write_table(self, df: pd.DataFrame) -> None:
        if df.empty or not self._table_exists(self.table):
            return
        sig_dates = df["sig_date"].dropna().unique().tolist()
        try:
            table_columns = {
                col.get("name")
                for col in inspect(self.db_writer.engine).get_columns(self.table)
                if isinstance(col, dict)
            }
        except Exception:
            table_columns = set()
        if sig_dates:
            delete_stmt = (
                text(
                    f"""
                    DELETE FROM `{self.table}`
                    WHERE `sig_date` IN :dates AND `code` IN :codes
                    """
                ).bindparams(bindparam("dates", expanding=True), bindparam("codes", expanding=True))
            )
            with self.db_writer.engine.begin() as conn:
                codes = df["code"].dropna().astype(str).unique().tolist()
                conn.execute(delete_stmt, {"dates": sig_dates, "codes": codes})
        aligned = df.copy()
        if table_columns:
            aligned = aligned[[c for c in aligned.columns if c in table_columns]].copy()
        self.db_writer.write_dataframe(aligned, self.table, if_exists="append")

    def apply(
        self,
        signals: pd.DataFrame,
        *,
        gdhs_table: str = "a_share_gdhs_detail",
        gdhs_summary_table: str | None = "a_share_gdhs",
    ) -> pd.DataFrame:
        if signals.empty:
            return pd.DataFrame()
        detail_exists = self._table_exists(gdhs_table)
        summary_exists = bool(gdhs_summary_table) and self._table_exists(gdhs_summary_table)
        if not detail_exists and not summary_exists:
            return pd.DataFrame()

        sig_df = signals.copy()
        date_col = (
            "date"
            if "date" in sig_df.columns
            else ("sig_date" if "sig_date" in sig_df.columns else None)
        )
        if date_col is None:
            self.logger.warning(
                "ChipFilter: signals 缺少 date/sig_date 列，无法计算筹码过滤。"
            )
            return pd.DataFrame()
        sig_df["sig_date"] = pd.to_datetime(sig_df[date_col], errors="coerce")
        sig_df = sig_df.dropna(subset=["sig_date", "code"])
        sig_df["code"] = sig_df["code"].astype(str)
        sig_df = sig_df.sort_values(["code", "sig_date"], ignore_index=True)
        if sig_df.empty:
            return pd.DataFrame()

        latest_date = sig_df["sig_date"].max()
        codes = sig_df["code"].unique().tolist()
        empty_cols = ["code", "announce_date", "gdhs_delta_pct", "gdhs_delta_raw"]

        detail_df = (
            self._load_gdhs_detail(codes, latest_date, gdhs_table)
            if detail_exists
            else pd.DataFrame(columns=empty_cols)
        )
        summary_df = (
            self._load_gdhs_detail(codes, latest_date, gdhs_summary_table)  # type: ignore[arg-type]
            if summary_exists
            else pd.DataFrame(columns=empty_cols)
        )

        frames: list[pd.DataFrame] = []
        if not detail_df.empty:
            detail_df = detail_df.copy()
            detail_df["__src"] = 0
            frames.append(detail_df)
        if not summary_df.empty:
            summary_df = summary_df.copy()
            summary_df["__src"] = 1
            frames.append(summary_df)

        chip_df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=empty_cols)
        if not chip_df.empty:
            chip_df["code"] = chip_df["code"].astype(str)
            chip_df["announce_date"] = pd.to_datetime(chip_df["announce_date"], errors="coerce")
            chip_df["gdhs_delta_pct"] = pd.to_numeric(chip_df.get("gdhs_delta_pct"), errors="coerce")
            chip_df["gdhs_delta_raw"] = pd.to_numeric(chip_df.get("gdhs_delta_raw"), errors="coerce")
            chip_df = (
                chip_df.dropna(subset=["code", "announce_date"])
                .sort_values(["code", "announce_date", "__src"], ignore_index=True)
                .drop_duplicates(subset=["code", "announce_date"], keep="first")
                .drop(columns=["__src"], errors="ignore")
            )

        chip_df = chip_df.sort_values(["code", "announce_date"], ignore_index=True)

        merged_parts: list[pd.DataFrame] = []
        for code, sig_slice in sig_df.groupby("code", sort=False):
            chip_slice = chip_df[chip_df["code"] == code]

            if chip_slice.empty:
                part = sig_slice.copy()
                part["announce_date"] = pd.NaT
                part["gdhs_delta_pct"] = np.nan
                part["gdhs_delta_raw"] = np.nan
            else:
                part = pd.merge_asof(
                    sig_slice,
                    chip_slice,
                    left_on="sig_date",
                    right_on="announce_date",
                    direction="backward",
                    allow_exact_matches=True,
                )

            part = part.copy()
            part["code"] = code

            # 关键：统一 dtype，避免 concat 时因“空/全 NA 块”的 dtype 推断变化触发 FutureWarning
            part["announce_date"] = pd.to_datetime(part.get("announce_date"), errors="coerce")
            part["gdhs_delta_pct"] = pd.to_numeric(part.get("gdhs_delta_pct"), errors="coerce")
            part["gdhs_delta_raw"] = pd.to_numeric(part.get("gdhs_delta_raw"), errors="coerce")

            # 过滤空块 / 全 NA 块（FutureWarning 的根源之一）
            if part.empty:
                continue
            if not part.notna().any().any():
                continue

            merged_parts.append(part)

        if not merged_parts:
            return pd.DataFrame()

        merged = pd.concat(merged_parts, ignore_index=True)
        if "code" not in merged.columns:
            self.logger.warning("strategy_chip_filter 缺少 code 列，已跳过写入。")
            return pd.DataFrame()

        bad = merged["code"].isna() | (merged["code"].astype(str).str.strip() == "")
        if bad.any():
            self.logger.warning("strategy_chip_filter 丢弃 code 为空的行数=%s", int(bad.sum()))
            merged = merged.loc[~bad].copy()
        if merged.empty:
            return pd.DataFrame()

        merged["gdhs_delta_pct"] = pd.to_numeric(merged.get("gdhs_delta_pct"), errors="coerce")
        merged["vol_ratio"] = pd.to_numeric(merged.get("vol_ratio"), errors="coerce")
        merged["close"] = pd.to_numeric(merged.get("close"), errors="coerce")
        merged["ma20"] = pd.to_numeric(merged.get("ma20"), errors="coerce")
        merged["runup_pct"] = pd.to_numeric(merged.get("runup_pct"), errors="coerce")
        merged["fear_score"] = pd.to_numeric(merged.get("fear_score"), errors="coerce")
        merged["chip_delta"] = merged["gdhs_delta_pct"]

        sig_dt = pd.to_datetime(merged["sig_date"], errors="coerce")
        merged["age_days"] = (sig_dt - merged["announce_date"]).dt.days
        merged["deadzone_hit"] = merged["chip_delta"].abs() <= 5
        merged["stale_hit"] = merged["age_days"] > 120

        chip_reason = pd.Series(pd.NA, index=merged.index, dtype="object")
        chip_score = pd.Series(0.0, index=merged.index, dtype=float)
        chip_penalty = pd.Series(0.0, index=merged.index, dtype=float)
        chip_note = pd.Series(pd.NA, index=merged.index, dtype="object")

        missing_gdhs = merged["chip_delta"].isna() | merged["announce_date"].isna()
        chip_reason = chip_reason.mask(missing_gdhs, "DATA_MISSING_GDHS")
        missing_vol = merged["vol_ratio"].isna()
        vol_ratio_used = merged["vol_ratio"].fillna(1.0)
        chip_reason = chip_reason.mask(
            missing_vol & chip_reason.isna(), "DATA_MISSING_VOL_RATIO_FALLBACK"
        )

        delta_raw = pd.to_numeric(merged.get("gdhs_delta_raw"), errors="coerce")
        outlier_mask = (merged["chip_delta"].abs() > 80) | (delta_raw.abs() > 1_000_000)
        outlier_mask = outlier_mask.fillna(False)
        chip_reason = chip_reason.mask(outlier_mask & chip_reason.isna(), "DATA_OUTLIER_GDHS")

        chip_reason = chip_reason.mask(merged["stale_hit"] & chip_reason.isna(), "DATA_STALE_GDHS")

        can_eval = ~(
            missing_gdhs | outlier_mask | merged["stale_hit"].fillna(False)
        )
        concentrate = can_eval & (merged["chip_delta"] <= -10)
        disperse = can_eval & (merged["chip_delta"] >= 20)
        deadzone = can_eval & merged["deadzone_hit"]

        chip_score.loc[concentrate] += 0.6
        chip_score.loc[disperse] -= 0.6

        chip_reason = chip_reason.mask(concentrate, "CHIP_CONCENTRATE")
        chip_reason = chip_reason.mask(disperse, "CHIP_DISPERSE_STRONG")
        chip_reason = chip_reason.mask(deadzone, "CHIP_NEUTRAL")

        # 量价确认与情绪分歧扣分
        daily_drop_candidates = []
        for col in ["pct_chg", "pct_change", "change_pct", "ret_1d", "ret"]:
            if col in merged.columns:
                daily_drop_candidates.append(pd.to_numeric(merged[col], errors="coerce"))
        daily_drop = next((s for s in daily_drop_candidates if not s.isna().all()), pd.Series(np.nan, index=merged.index))
        weaken_price = (merged["close"] < merged["ma20"]) | (
            (vol_ratio_used > 1.5) & (daily_drop < 0)
        )
        chip_score.loc[can_eval & weaken_price.fillna(False)] -= 0.3

        risk_tag = merged.get("risk_tag")
        if isinstance(risk_tag, pd.Series):
            risk_tag = risk_tag.astype("string")
            mania_mask = risk_tag.str.contains("MANIA", case=False, na=False)
        else:
            mania_mask = pd.Series(False, index=merged.index)

        high_runup = merged["runup_pct"] > 0.2
        chip_score.loc[can_eval & (high_runup | mania_mask)] -= 0.3

        chip_score = chip_score.clip(-1.0, 1.0)
        chip_reason = chip_reason.mask(can_eval & chip_reason.isna(), "CHIP_NEUTRAL")

        data_issue_mask = chip_reason.isin({
            "DATA_MISSING_GDHS",
            "DATA_OUTLIER_GDHS",
            "DATA_STALE_GDHS",
        })
        chip_score = chip_score.where(~data_issue_mask, 0.0)
        chip_note = chip_note.mask(data_issue_mask, chip_reason)

        deadzone_flag = deadzone.fillna(False)
        chip_penalty = chip_penalty.mask(deadzone_flag, chip_penalty + 0.1)
        chip_note = chip_note.mask(deadzone_flag & chip_note.isna(), "DEADZONE_PENALTY")

        merged["chip_score"] = chip_score
        merged["chip_reason"] = chip_reason
        merged["chip_penalty"] = chip_penalty
        merged["chip_note"] = chip_note
        merged["sig_date"] = merged["sig_date"].dt.date
        merged["updated_at"] = dt.datetime.now()

        keep_cols = [
            "sig_date",
            "code",
            "announce_date",
            "gdhs_delta_pct",
            "gdhs_delta_raw",
            "chip_score",
            "chip_reason",
            "vol_ratio",
            "age_days",
            "deadzone_hit",
            "stale_hit",
            "chip_penalty",
            "chip_note",
            "updated_at",
        ]
        result = merged[keep_cols].copy()
        result["announce_date"] = pd.to_datetime(result.get("announce_date"), errors="coerce").dt.date
        try:
            self._write_table(result)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("写入 %s 失败：%s", self.table, exc)
        return result

================================================================================
FILE: ashare/config.py
================================================================================

"""应用运行时的代理配置管理工具."""

from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Dict, Any
from functools import lru_cache
from pathlib import Path

import yaml

# 环境变量中可以用 ASHARE_CONFIG_FILE 指定配置文件路径
CONFIG_FILE_ENV = "ASHARE_CONFIG_FILE"
# 默认的配置文件名，位于项目根目录
DEFAULT_CONFIG_FILENAME = "config.yaml"


@lru_cache()
def load_config() -> Dict[str, Any]:
    """
    读取并缓存 config.yaml 内容。

    优先顺序：
    1. 环境变量 ASHARE_CONFIG_FILE 指定的路径；
    2. 项目根目录下的 config.yaml。
    """
    path_str = os.getenv(CONFIG_FILE_ENV)
    if path_str:
        path = Path(path_str)
    else:
        # ashare/config.py -> ashare 目录 -> 项目根目录
        path = Path(__file__).resolve().parents[1] / DEFAULT_CONFIG_FILENAME

    if not path.is_file():
        # 没有配置文件时，返回空 dict，调用方自己处理默认值
        return {}

    with path.open("r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}

    if not isinstance(data, dict):
        raise ValueError("config.yaml 顶层必须是一个对象（mapping）")

    return data


def get_section(name: str) -> Dict[str, Any]:
    """
    获取配置中的某个子节，例如 app/database/proxy/baostock 等。

    不存在时返回空 dict。
    """
    cfg = load_config()
    section = cfg.get(name) or {}
    if not isinstance(section, dict):
        raise ValueError(f"config.yaml 中 {name} 必须是一个对象（mapping）")
    return section


@dataclass
class ProxyConfig:
    """描述 HTTP/HTTPS 代理配置的简单数据类。"""

    http: str | None = None
    https: str | None = None

    @classmethod
    def from_env(cls) -> "ProxyConfig":
        """
        从环境变量与 config.yaml 加载代理配置。

        优先顺序：
        1. ASHARE_HTTP_PROXY / ASHARE_HTTPS_PROXY；
        2. HTTP_PROXY / HTTPS_PROXY（含小写）；
        3. config.yaml 中 proxy.http / proxy.https。
        """

        http_proxy = (
            os.environ.get("ASHARE_HTTP_PROXY")
            or os.environ.get("HTTP_PROXY")
            or os.environ.get("http_proxy")
        )
        https_proxy = (
            os.environ.get("ASHARE_HTTPS_PROXY")
            or os.environ.get("HTTPS_PROXY")
            or os.environ.get("https_proxy")
        )
        section = get_section("proxy")
        if not http_proxy:
            http_proxy = section.get("http")
        if not https_proxy:
            https_proxy = section.get("https")
        return cls(http=http_proxy, https=https_proxy)

    def as_requests_proxies(self) -> Dict[str, str]:
        """以 requests 可用的格式输出代理配置."""

        proxies: Dict[str, str] = {}
        if self.http:
            proxies["http"] = self.http
        if self.https:
            proxies["https"] = self.https
        return proxies

    def apply_to_environment(self) -> None:
        """将代理配置写入环境变量, 便于底层库读取."""

        if self.http:
            os.environ["HTTP_PROXY"] = self.http
            os.environ["http_proxy"] = self.http
        if self.https:
            os.environ["HTTPS_PROXY"] = self.https
            os.environ["https_proxy"] = self.https

================================================================================
FILE: ashare/db.py
================================================================================

"""MySQL 写入工具与配置."""

from __future__ import annotations

import os
from dataclasses import dataclass
from urllib.parse import quote_plus

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

from .config import get_section


@dataclass
class DatabaseConfig:
    """MySQL 连接参数配置."""

    host: str = "127.0.0.1"
    port: int = 3306
    user: str = "root"
    password: str = ""
    db_name: str = "ashare"
    pool_size: int = 5
    max_overflow: int = 10
    pool_recycle: int = 1800
    pool_timeout: int = 30

    @classmethod
    def from_env(cls) -> "DatabaseConfig":
        """从环境变量与 config.yaml 读取数据库配置。

        优先顺序：
        1. 环境变量 MYSQL_*；
        2. config.yaml 中 database.*；
        3. 类默认值。
        """
        section = get_section("database")

        host = os.getenv("MYSQL_HOST", section.get("host", cls.host))
        port_raw = os.getenv("MYSQL_PORT", section.get("port", cls.port))
        user = os.getenv("MYSQL_USER", section.get("user", cls.user))
        password = os.getenv(
            "MYSQL_PASSWORD",
            section.get("password", cls.password),
        )
        db_name = os.getenv("MYSQL_DB_NAME", section.get("db_name", cls.db_name))
        pool_size_raw = os.getenv(
            "MYSQL_POOL_SIZE", section.get("pool_size", cls.pool_size)
        )
        max_overflow_raw = os.getenv(
            "MYSQL_MAX_OVERFLOW", section.get("max_overflow", cls.max_overflow)
        )
        pool_recycle_raw = os.getenv(
            "MYSQL_POOL_RECYCLE", section.get("pool_recycle", cls.pool_recycle)
        )
        pool_timeout_raw = os.getenv(
            "MYSQL_POOL_TIMEOUT", section.get("pool_timeout", cls.pool_timeout)
        )

        try:
            port = int(port_raw)
        except (TypeError, ValueError):
            port = cls.port

        def _parse_positive_int(raw: str | int | float | None, default: int) -> int:
            try:
                value = int(raw)
                return value if value > 0 else default
            except (TypeError, ValueError):
                return default

        return cls(
            host=host,
            port=port,
            user=user,
            password=password,
            db_name=db_name,
            pool_size=_parse_positive_int(pool_size_raw, cls.pool_size),
            max_overflow=_parse_positive_int(max_overflow_raw, cls.max_overflow),
            pool_recycle=_parse_positive_int(pool_recycle_raw, cls.pool_recycle),
            pool_timeout=_parse_positive_int(pool_timeout_raw, cls.pool_timeout),
        )

    def _credential(self) -> str:
        password = quote_plus(self.password) if self.password else ""
        credential = self.user
        if password:
            credential = f"{credential}:{password}"
        return credential

    def server_url(self) -> str:
        """构造不带库名的连接 URL，便于创建数据库。"""

        return f"mysql+pymysql://{self._credential()}@{self.host}:{self.port}"

    def database_url(self) -> str:
        """构造带库名的连接 URL。"""

        return f"{self.server_url()}/{self.db_name}?charset=utf8mb4"


class MySQLWriter:
    """负责将 DataFrame 写入 MySQL 的工具类."""

    def __init__(self, config: DatabaseConfig) -> None:
        self.config = config
        self.engine = self._create_engine_with_database()

    def _create_engine_with_database(self) -> Engine:
        """确保目标数据库存在，并返回绑定库的 Engine。"""

        server_engine = create_engine(self.config.server_url(), future=True)
        with server_engine.connect() as conn:
            conn.execute(
                text(
                    f"CREATE DATABASE IF NOT EXISTS `{self.config.db_name}` "
                    "DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"
                )
            )
        server_engine.dispose()

        return create_engine(
            self.config.database_url(),
            future=True,
            pool_size=self.config.pool_size,
            max_overflow=self.config.max_overflow,
            pool_timeout=self.config.pool_timeout,
            pool_recycle=self.config.pool_recycle,
            pool_pre_ping=True,
        )

    def write_dataframe(
        self,
        df: pd.DataFrame,
        table_name: str,
        if_exists: str = "replace",
        chunksize: int = 500,
        method: str | None = "multi",
    ) -> None:
        if df.empty:
            raise RuntimeError("待写入的数据为空，已跳过数据库写入。")

        # 可选：提前检查列名大小写冲突
        cols_lower = [c.lower() for c in df.columns]
        if len(cols_lower) != len(set(cols_lower)):
            raise RuntimeError(
                f"DataFrame 列名存在仅大小写不同的重复：{list(df.columns)}"
            )

        allowed_if_exists = {"fail", "replace", "append"}
        if if_exists not in allowed_if_exists:
            raise ValueError(
                "if_exists 仅支持 fail/replace/append，"
                f"当前值为: {if_exists}"
            )

        with self.engine.begin() as conn:
            df.to_sql(
                table_name,
                conn,
                if_exists=if_exists,
                index=False,
                chunksize=chunksize,
                method=method,
            )

    def dispose(self) -> None:
        """释放数据库连接池资源。"""

        self.engine.dispose()

================================================================================
FILE: ashare/env_snapshot_utils.py
================================================================================

"""环境快照通用工具。"""

from __future__ import annotations

import datetime as dt
from typing import Iterable

import pandas as pd
from sqlalchemy import bindparam, text

from .baostock_core import BaostockDataFetcher
from .baostock_session import BaostockSession
from .config import get_section
from .db import DatabaseConfig, MySQLWriter


def _parse_date(val: str) -> dt.date | None:
    try:
        return dt.datetime.strptime(val, "%Y-%m-%d").date()
    except Exception:  # noqa: BLE001
        return None


def _load_trading_calendar(start: dt.date, end: dt.date) -> set[str]:
    try:
        client = BaostockDataFetcher(BaostockSession())
        calendar_df = client.get_trade_calendar(start.isoformat(), end.isoformat())
    except Exception:  # noqa: BLE001
        return set()

    if "is_trading_day" in calendar_df.columns:
        calendar_df = calendar_df[calendar_df["is_trading_day"].astype(str) == "1"]

    dates = (
        pd.to_datetime(calendar_df["calendar_date"], errors="coerce").dt.date.dropna().tolist()
    )
    return {d.isoformat() for d in dates}

# 模块级交易日历缓存：避免重复请求 baostock。
_TRADING_CALENDAR_CACHE: set[str] = set()
_TRADING_CALENDAR_RANGE: tuple[dt.date, dt.date] | None = None

def load_trading_calendar(start: dt.date, end: dt.date) -> set[str]:
    """加载交易日历并做模块级缓存。"""

    global _TRADING_CALENDAR_CACHE, _TRADING_CALENDAR_RANGE
    if (
        _TRADING_CALENDAR_RANGE
        and start >= _TRADING_CALENDAR_RANGE[0]
        and end <= _TRADING_CALENDAR_RANGE[1]
    ):
        return _TRADING_CALENDAR_CACHE

    current_start = start
    current_end = end
    if _TRADING_CALENDAR_RANGE:
        current_start = min(_TRADING_CALENDAR_RANGE[0], start)
        current_end = max(_TRADING_CALENDAR_RANGE[1], end)

    calendar = _load_trading_calendar(current_start, current_end)
    if calendar:
        _TRADING_CALENDAR_CACHE = set(calendar)
        _TRADING_CALENDAR_RANGE = (current_start, current_end)
        return _TRADING_CALENDAR_CACHE
    return set()


def _resolve_latest_closed_week_end(latest_trade_date: str) -> tuple[str, bool]:
    trade_date = _parse_date(latest_trade_date)
    if trade_date is None:
        return latest_trade_date, True

    week_start = trade_date - dt.timedelta(days=trade_date.weekday())
    week_end = week_start + dt.timedelta(days=6)
    calendar = load_trading_calendar(week_start - dt.timedelta(days=60), week_end)

    if calendar:
        last_trade_day = None
        for i in range(7):
            candidate = week_end - dt.timedelta(days=i)
            if candidate.isoformat() in calendar:
                last_trade_day = candidate
                break

        if last_trade_day:
            if trade_date == last_trade_day:
                return trade_date.isoformat(), True

            prev_candidate = week_start - dt.timedelta(days=1)
            for _ in range(30):
                if prev_candidate.isoformat() in calendar:
                    return prev_candidate.isoformat(), False
                prev_candidate -= dt.timedelta(days=1)

    fallback_friday = week_start + dt.timedelta(days=4)
    if trade_date >= fallback_friday:
        return fallback_friday.isoformat(), trade_date == fallback_friday

    prev_friday = fallback_friday - dt.timedelta(days=7)
    return prev_friday.isoformat(), False


def _load_index_codes_from_config() -> list[str]:
    app_cfg = get_section("app") or {}
    codes: Iterable[str] = []
    if isinstance(app_cfg, dict):
        raw = app_cfg.get("index_codes", [])
        if isinstance(raw, (list, tuple)):
            codes = [str(c).strip() for c in raw if str(c).strip()]
    return list(codes)


def resolve_weekly_asof_date(include_current_week: bool) -> str:
    """基于指数日线数据推导周线 asof_date。"""

    codes = _load_index_codes_from_config()
    if not codes:
        raise ValueError("config.yaml 未配置 app.index_codes，无法解析 asof_date。")

    db = MySQLWriter(DatabaseConfig.from_env())
    stmt_latest = (
        text(
            """
            SELECT `code`, MAX(`date`) AS latest_date
            FROM history_index_daily_kline
            WHERE `code` IN :codes
            GROUP BY `code`
            """
        ).bindparams(bindparam("codes", expanding=True))
    )

    with db.engine.begin() as conn:
        latest_date_df = pd.read_sql_query(stmt_latest, conn, params={"codes": codes})

    if latest_date_df.empty:
        raise ValueError("history_index_daily_kline 为空或未找到指定指数。")

    latest_per_code = pd.to_datetime(latest_date_df["latest_date"], errors="coerce").dt.date.dropna()
    if latest_per_code.empty:
        raise ValueError("history_index_daily_kline 为空或未找到指定指数。")

    latest_date_val = min(latest_per_code)
    latest_date_str = pd.to_datetime(latest_date_val).date().isoformat()

    week_end_asof, _ = _resolve_latest_closed_week_end(latest_date_str)
    return latest_date_str if include_current_week else week_end_asof

================================================================================
FILE: ashare/external_signal_manager.py
================================================================================

"""基于 Akshare 的行为证据数据同步管理器."""

from __future__ import annotations

from typing import Any, Callable, Dict, Iterable

import pandas as pd
from sqlalchemy import bindparam, text

from .akshare_fetcher import AkshareDataFetcher
from .db import MySQLWriter


class ExternalSignalManager:
    """负责从 Akshare 拉取龙虎榜、两融与股东户数等信号并入库。"""

    def __init__(
        self,
        fetcher: AkshareDataFetcher,
        db_writer: MySQLWriter,
        logger,
        config: Dict[str, Any] | None = None,
    ) -> None:
        self.fetcher = fetcher
        self.db_writer = db_writer
        self.logger = logger
        self.config = config or {}

    # =========================
    # Utils
    # =========================
    def _to_yyyymmdd(self, value: Any) -> str | None:  # noqa: ANN401
        if pd.isna(value):
            return None
        text_value = str(value).strip()
        if not text_value:
            return None
        if text_value.isdigit() and len(text_value) >= 8:
            return text_value[-8:]

        try:
            parsed = pd.to_datetime(text_value, errors="coerce")
        except Exception:  # noqa: BLE001
            parsed = pd.NaT

        if pd.isna(parsed):
            return None
        return parsed.strftime("%Y%m%d")

    def _yyyymmdd_to_iso(self, yyyymmdd: str) -> str:
        if not yyyymmdd or len(yyyymmdd) != 8:
            return str(yyyymmdd)
        return f"{yyyymmdd[:4]}-{yyyymmdd[4:6]}-{yyyymmdd[6:8]}"

    def _iso_to_yyyymmdd(self, iso_date: str) -> str:
        ymd = self._to_yyyymmdd(iso_date)
        return ymd or str(iso_date).replace("-", "")

    def _shift_yyyymmdd(self, yyyymmdd: str, delta_days: int) -> str:
        base = pd.to_datetime(yyyymmdd, format="%Y%m%d", errors="coerce")
        if pd.isna(base):
            return yyyymmdd
        shifted = base + pd.Timedelta(days=delta_days)
        return shifted.strftime("%Y%m%d")

    def _rename_first(self, df: pd.DataFrame, candidates: list[str], target: str) -> None:
        for col in candidates:
            if col in df.columns and col != target:
                df.rename(columns={col: target}, inplace=True)
                break

    def _ensure_column(self, df: pd.DataFrame, column: str, value: Any) -> None:  # noqa: ANN401
        if column not in df.columns:
            df[column] = value

    def _ensure_df(self, value: Any) -> pd.DataFrame:  # noqa: ANN401
        if value is None:
            return pd.DataFrame()
        if isinstance(value, pd.DataFrame):
            return value
        try:
            return pd.DataFrame(value)
        except Exception:  # noqa: BLE001
            return pd.DataFrame()

    # =========================
    # DB
    # =========================
    def _upsert(
        self,
        df: pd.DataFrame,
        table: str,
        subset: Iterable[str] | None,
        period_delete_by_code: bool = False,
    ) -> None:
        if df.empty:
            self.logger.info("表 %s 本次无新增数据，跳过写入。", table)
            return

        deduped = df.drop_duplicates().copy()
        if subset:
            deduped.drop_duplicates(subset=list(subset), keep="last", inplace=True)

        delete_filters: list[Dict[str, str]] = []
        if "trade_date" in deduped.columns:
            key_cols = ["trade_date"]
            for optional in ("indicator", "exchange", "market"):
                if optional in deduped.columns:
                    key_cols.append(optional)

            for _, row in (
                deduped[key_cols].dropna(subset=["trade_date"]).drop_duplicates().iterrows()
            ):
                delete_filters.append({col: str(row[col]) for col in key_cols})
        elif "period" in deduped.columns:
            if period_delete_by_code and {"period", "code"}.issubset(deduped.columns):
                period_code = (
                    deduped[["period", "code"]]
                    .dropna(subset=["period", "code"])
                    .drop_duplicates()
                )
                for _, row in period_code.iterrows():
                    delete_filters.append({"period": str(row["period"]), "code": str(row["code"])})
            else:
                for period in deduped["period"].dropna().astype(str).unique():
                    delete_filters.append({"period": period})

        if delete_filters:
            try:
                with self.db_writer.engine.begin() as conn:
                    for filter_values in delete_filters:
                        conditions = [f"{col} = :{col}" for col in filter_values]
                        stmt = text(
                            f"DELETE FROM `{table}` WHERE " + " AND ".join(conditions)
                        ).bindparams(*(bindparam(col) for col in filter_values))
                        conn.execute(stmt, filter_values)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning(
                    "删除表 %s 分区时出现异常（可能表不存在），已跳过删除：%s",
                    table,
                    exc,
                )

        self.db_writer.write_dataframe(deduped, table, if_exists="append")
        self.logger.info("表 %s 已写入 %s 行。", table, len(deduped))

    # =========================
    # Normalization
    # =========================
    def _normalize_code_column(self, df: pd.DataFrame) -> None:
        self._rename_first(
            df,
            [
                "code",
                "代码",
                "证券代码",
                "股票代码",
                "SECURITY_CODE",
                "SECURITYCODE",
                "symbol",
                "标的证券代码",
            ],
            "code",
        )
        if "code" in df.columns:
            df["code"] = (
                df["code"].astype(str).str.strip().apply(self._format_code_with_prefix)
            )

    def _format_code_with_prefix(self, code: str) -> str:
        if not code or pd.isna(code):
            return ""
        text_code = str(code).strip().lower()
        if text_code in {"", "nan", "none", "null"}:
            return ""
        if text_code.startswith(("sh.", "sz.", "bj.")):
            return text_code

        numeric_text = text_code
        if "." in text_code and not text_code.startswith(("sh.", "sz.", "bj.")):
            left = text_code.split(".", 1)[0]
            if left.isdigit():
                numeric_text = left

        digits = numeric_text
        if digits.isdigit() and len(digits) > 6:
            digits = digits[-6:]
        if not digits.isdigit():
            return ""
        digits = digits.zfill(6)

        if digits.startswith(("60", "68", "69")):
            return f"sh.{digits}"
        if digits.startswith(("00", "30", "20")):
            return f"sz.{digits}"
        if digits.startswith(("83", "87", "43", "40")):
            return f"bj.{digits}"
        return f"sz.{digits}"

    def _normalize_trade_date(self, df: pd.DataFrame, trade_date: str) -> None:
        if "trade_date" not in df.columns:
            for col in [
                "trade_date",
                "TRADE_DATE",
                "交易日期",
                "日期",
                "上榜日",
                "信用交易日期",
            ]:
                if col in df.columns:
                    df["trade_date"] = df[col]
                    break

        normalized_default = self._to_yyyymmdd(trade_date) or trade_date.replace("-", "")
        self._ensure_column(df, "trade_date", normalized_default)
        df["trade_date"] = df["trade_date"].apply(
            lambda x: self._to_yyyymmdd(x) or normalized_default
        )
        for col in ["上榜日", "信用交易日期", "日期", "交易日期"]:
            if col in df.columns:
                df[col] = df[col].apply(
                    lambda x: self._to_yyyymmdd(x) or normalized_default
                )
        self._coerce_yyyymmdd_date_column(df, "trade_date")
        for col in ["上榜日", "信用交易日期", "日期", "交易日期"]:
            self._coerce_yyyymmdd_date_column(df, col)

    def _normalize_period(self, df: pd.DataFrame, period: str | None = None) -> None:
        self._rename_first(
            df,
            [
                "股东户数统计截止日-本次",
                "股东户数统计截止日",
                "截止日期",
                "报告期",
                "date",
            ],
            "period",
        )
        fallback_period = self._to_yyyymmdd(period) if period else None
        self._ensure_column(df, "period", fallback_period or period or "")
        df["period"] = df["period"].apply(
            lambda x: self._to_yyyymmdd(x)
            or fallback_period
            or ("" if pd.isna(x) else str(x))
        )
        self._coerce_yyyymmdd_date_column(df, "period")

    def _normalize_exchange(self, df: pd.DataFrame, exchange: str) -> None:
        self._rename_first(df, ["exchange", "交易所"], "exchange")
        self._ensure_column(df, "exchange", exchange)

    def _normalize_market(self, df: pd.DataFrame, market: str) -> None:
        self._rename_first(df, ["market", "渠道"], "market")
        self._ensure_column(df, "market", market)

    def _normalize_indicator(self, df: pd.DataFrame, indicator: str) -> None:
        self._rename_first(df, ["indicator", "排行类型"], "indicator")
        self._ensure_column(df, "indicator", indicator)

    def _normalize_symbol(self, code: str) -> str:
        if pd.isna(code):
            return ""
        code = str(code)
        if "." in code:
            return code.split(".", 1)[1]
        return code

    def _coerce_yyyymmdd_date_column(self, df: pd.DataFrame, column: str) -> None:
        if column not in df.columns:
            return
        series = df[column].apply(lambda x: self._to_yyyymmdd(x))
        parsed = pd.to_datetime(series, format="%Y%m%d", errors="coerce")
        df[column] = parsed.dt.date

    def _coerce_any_date_column(self, df: pd.DataFrame, column: str) -> None:
        if column not in df.columns:
            return
        df[column] = pd.to_datetime(df[column], errors="coerce").dt.date

    def _normalize_security_name(self, df: pd.DataFrame) -> None:
        if "证券简称" not in df.columns and "标的证券简称" in df.columns:
            df.rename(columns={"标的证券简称": "证券简称"}, inplace=True)
            return
        if "证券简称" in df.columns and "标的证券简称" in df.columns:
            df["证券简称"] = df["证券简称"].where(
                df["证券简称"].notna(), df["标的证券简称"]
            )
            df.drop(columns=["标的证券简称"], inplace=True)

    # =========================
    # Trade-date backoff (核心：取最近一次可用数据)
    # =========================
    def _iter_recent_trading_dates(
        self,
        requested_trade_date_iso: str,
        max_backoff_days: int,
        skip_today: bool,
    ) -> list[str]:
        """按交易日回退（优先）。

        设计目标：不引入额外配置/开关，也不改调用方签名。
        - 优先使用本地数据库里“指数日线”表的日期序列作为交易日历；
        - 若表不存在/为空/查询失败，则返回空列表，由调用方回退到自然日逻辑。

        返回值：ISO 日期列表（YYYY-MM-DD），按“最近 -> 更早”顺序。
        返回条数与旧逻辑保持一致：最多返回 max_backoff_days + 1 次尝试的候选日期。
        """

        req_iso = str(requested_trade_date_iso or "").strip()
        if not req_iso:
            return []

        # 多取 1 条：如果 req_iso 本身在表里且 skip_today=True，需要丢掉第一条
        limit = int(max_backoff_days) + 2
        if limit <= 0:
            return []

        sql = text(
            f"SELECT DISTINCT `date` AS d FROM `history_index_daily_kline` "
            "WHERE `date` <= :req ORDER BY `date` DESC "
            f"LIMIT {limit}"
        )

        try:
            with self.db_writer.engine.connect() as conn:
                rows = conn.execute(sql, {"req": req_iso}).fetchall()
        except Exception:
            return []

        dates: list[str] = []
        for row in rows:
            if not row:
                continue
            value = row[0]
            if value is None:
                continue
            iso = str(value).strip()
            if iso:
                dates.append(iso)

        if not dates:
            return []

        if skip_today and dates and dates[0] == req_iso:
            dates = dates[1:]

        # 旧逻辑：最多尝试 max_backoff_days + 1 次
        return dates[: int(max_backoff_days) + 1]

    def _iter_recent_dates(
        self,
        requested_trade_date: str,
        max_backoff_days: int,
        skip_today: bool,
    ) -> list[str]:
        """
        返回 ISO 日期列表（YYYY-MM-DD），按“最近 -> 更早”顺序。

        为避免国庆/春节等长假导致“自然日回退不够而全空”，这里默认优先按交易日回退：
        - 能从本地交易日历拿到候选日期 → 用交易日序列；
        - 否则回退到自然日（兼容旧行为）。
        """
        req_ymd = self._to_yyyymmdd(requested_trade_date)
        if not req_ymd:
            return []

        req_iso = self._yyyymmdd_to_iso(req_ymd)

        trading_dates = self._iter_recent_trading_dates(
            requested_trade_date_iso=req_iso,
            max_backoff_days=max_backoff_days,
            skip_today=skip_today,
        )
        if trading_dates:
            return trading_dates

        # fallback：自然日回退（旧逻辑）
        start_offset = 1 if skip_today else 0
        dates: list[str] = []
        for offset in range(start_offset, max_backoff_days + 1 + start_offset):
            ymd = self._shift_yyyymmdd(req_ymd, -offset)
            dates.append(self._yyyymmdd_to_iso(ymd))
        return dates

    def _fetch_trade_date_df(
        self,
        fetch_fn: Callable[[str], Any],
        requested_trade_date: str,
        max_backoff_days: int,
        skip_today: bool,
        case_name: str,
    ) -> tuple[pd.DataFrame, str | None]:
        """
        尝试获取“最近一次可用”的 trade-date 数据：
        - 只要不是 exception，就认为接口可用（success_empty 也可用）
        - 返回 (df, used_date_iso)
        """
        candidates = self._iter_recent_dates(
            requested_trade_date=requested_trade_date,
            max_backoff_days=max_backoff_days,
            skip_today=skip_today,
        )
        if not candidates:
            return pd.DataFrame(), None

        last_exc: Exception | None = None
        for idx, used_iso in enumerate(candidates, start=1):
            try:
                raw = fetch_fn(used_iso)
                df = self._ensure_df(raw)
                if df is None:
                    raise TypeError("akshare returned None")
                # 注意：有些接口在非交易日/未更新时会“成功返回空 DF”；
                # 如果还没到最后一次尝试，则继续回退到更早日期。
                if getattr(df, "empty", False) and idx < len(candidates):
                    self.logger.info(
                        "%s 返回空数据（date=%s, try=%s/%s），继续回退。",
                        case_name, used_iso, idx, len(candidates)
                    )
                    continue
                return df, used_iso
            except Exception as exc:  # noqa: BLE001
                last_exc = exc
                self.logger.warning(
                    "%s 获取失败（date=%s, try=%s/%s）: %s",
                    case_name,
                    used_iso,
                    idx,
                    len(candidates),
                    exc,
                )

        # 全部失败
        if last_exc is not None:
            self.logger.warning("%s 回退尝试全部失败: %s", case_name, last_exc)
        return pd.DataFrame(), None

    # =========================
    # Sync tasks
    # =========================
    def sync_lhb_detail(self, trade_date: str) -> pd.DataFrame:
        ak_cfg = self.config
        lhb_cfg = ak_cfg.get("lhb", {}) if isinstance(ak_cfg.get("lhb", {}), dict) else {}

        # 关键：默认不取当天，直接取“最近一次可用”（通常是上一交易日）
        skip_today = bool(lhb_cfg.get("skip_today", ak_cfg.get("skip_today", True)))
        max_backoff_days = int(lhb_cfg.get("max_backoff_days", ak_cfg.get("max_backoff_days", 5)))

        df, used_iso = self._fetch_trade_date_df(
            fetch_fn=lambda d: self.fetcher.get_lhb_detail(d),
            requested_trade_date=trade_date,
            max_backoff_days=max_backoff_days,
            skip_today=skip_today,
            case_name="龙虎榜详情",
        )

        if df.empty:
            self.logger.info("龙虎榜返回为空（used_date=%s）。", used_iso or trade_date)
            return df

        used_date_for_norm = used_iso or trade_date
        self._normalize_code_column(df)
        self._normalize_trade_date(df, used_date_for_norm)
        if "上榜日" not in df.columns and "trade_date" in df.columns:
            df["上榜日"] = df["trade_date"]
        subset = [col for col in ["code", "trade_date", "上榜日", "上榜原因"] if col in df.columns]
        self._upsert(df, "a_share_lhb_detail", subset=subset or None)
        return df

    def sync_margin_detail(self, trade_date: str, exchanges: Iterable[str]) -> pd.DataFrame:
        ak_cfg = self.config
        margin_cfg = ak_cfg.get("margin", {}) if isinstance(ak_cfg.get("margin", {}), dict) else {}

        skip_today = bool(margin_cfg.get("skip_today", ak_cfg.get("skip_today", True)))
        max_backoff_days = int(margin_cfg.get("max_backoff_days", ak_cfg.get("max_backoff_days", 5)))

        frames: list[pd.DataFrame] = []

        for exchange in exchanges:
            df, used_iso = self._fetch_trade_date_df(
                fetch_fn=lambda d, ex=exchange: self.fetcher.get_margin_detail(d, ex),
                requested_trade_date=trade_date,
                max_backoff_days=max_backoff_days,
                skip_today=skip_today,
                case_name=f"两融明细 {exchange}",
            )

            if df.empty:
                self.logger.info("两融明细 %s 返回为空（used_date=%s）。", exchange, used_iso or trade_date)
                continue

            used_date_for_norm = used_iso or trade_date
            self._normalize_code_column(df)
            self._normalize_trade_date(df, used_date_for_norm)
            self._normalize_exchange(df, exchange)
            self._normalize_security_name(df)
            if "信用交易日期" in df.columns and "trade_date" in df.columns:
                df["信用交易日期"] = df["trade_date"]
            frames.append(df)

        if not frames:
            return pd.DataFrame()

        combined = pd.concat(frames, ignore_index=True)
        subset = [col for col in ["exchange", "trade_date", "code"] if col in combined.columns]
        self._upsert(combined, "a_share_margin_detail", subset=subset or None)
        return combined

    def sync_shareholder_counts(self, trade_date: str, focus_codes: list[str] | None = None) -> pd.DataFrame:
        gdhs_cfg = self.config.get("gdhs", {}) if isinstance(self.config.get("gdhs", {}), dict) else {}
        period = gdhs_cfg.get("period") or "最新"

        try:
            summary_df = self.fetcher.get_shareholder_count(period)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("股东户数汇总获取失败: %s", exc)
            summary_df = pd.DataFrame()

        summary_df = self._ensure_df(summary_df)

        if not summary_df.empty:
            period_from_data = summary_df.get("股东户数统计截止日-本次")
            if period_from_data is not None and not period_from_data.empty:
                period = str(period_from_data.iloc[0])
            self._normalize_code_column(summary_df)
            self._normalize_period(summary_df, period)
            for col in ["股东户数统计截止日-上次", "公告日期"]:
                self._coerce_any_date_column(summary_df, col)
            summary_subset = [col for col in ["code", "period"] if col in summary_df.columns]
            self._upsert(summary_df, "a_share_gdhs", subset=summary_subset or None)
        else:
            self.logger.info("股东户数汇总在 %s 返回为空。", period)

        if not gdhs_cfg.get("detail_enabled", True):
            return summary_df

        if not focus_codes:
            focus_codes = []
        top_n = int(gdhs_cfg.get("detail_top_n", 100))
        normalized_codes = [self._normalize_symbol(code) for code in focus_codes][:top_n]

        try:
            frames = self.fetcher.batch_get_shareholder_count_detail(normalized_codes)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("股东户数明细批量获取失败: %s", exc)
            return summary_df

        if not frames:
            self.logger.info("股东户数明细在指定代码范围内无返回。")
            return summary_df

        detail_df = pd.concat([self._ensure_df(x) for x in frames if x is not None], ignore_index=True)
        if detail_df.empty:
            self.logger.info("股东户数明细拼接后为空。")
            return summary_df

        period_from_detail = detail_df.get("股东户数统计截止日")
        if period_from_detail is not None and not period_from_detail.empty:
            period = str(period_from_detail.iloc[0])

        self._normalize_period(detail_df, period)
        self._normalize_code_column(detail_df)
        self._coerce_any_date_column(detail_df, "股东户数公告日期")
        subset = [col for col in ["code", "period"] if col in detail_df.columns]
        self._upsert(
            detail_df,
            "a_share_gdhs_detail",
            subset=subset or None,
            period_delete_by_code=True,
        )
        return summary_df

    def sync_daily_signals(self, trade_date: str, focus_codes: list[str] | None = None) -> None:
        ak_cfg = self.config
        if not ak_cfg.get("enabled", False):
            self.logger.info("Akshare 行为证据开关关闭，已跳过所有相关采集。")
            return

        # 这里保证：任何一个子任务失败，都不会把“行为证据同步阶段”整体打断
        lhb_cfg = ak_cfg.get("lhb", {})
        if isinstance(lhb_cfg, dict) and lhb_cfg.get("enabled", True):
            try:
                self.sync_lhb_detail(trade_date)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("龙虎榜同步阶段出现异常: %s", exc)
        else:
            self.logger.info("龙虎榜采集已关闭。")

        margin_cfg = ak_cfg.get("margin", {})
        if isinstance(margin_cfg, dict) and margin_cfg.get("enabled", True):
            exchanges = margin_cfg.get("exchanges", ["sse", "szse"])
            try:
                self.sync_margin_detail(trade_date, exchanges)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("两融同步阶段出现异常: %s", exc)
        else:
            self.logger.info("两融采集已关闭。")

        gdhs_cfg = ak_cfg.get("gdhs", {})
        if isinstance(gdhs_cfg, dict) and gdhs_cfg.get("enabled", True):
            try:
                self.sync_shareholder_counts(trade_date, focus_codes)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("股东户数同步阶段出现异常: %s", exc)
        else:
            self.logger.info("股东户数采集已关闭。")

================================================================================
FILE: ashare/fundamental_manager.py
================================================================================

"""财务与宏观数据采集与入库管理模块。"""

from __future__ import annotations

import datetime as dt
from typing import Iterable

import pandas as pd
from sqlalchemy import text

from .baostock_core import BaostockDataFetcher
from .db import MySQLWriter


class FundamentalDataManager:
    """负责财务数据、公司公告与宏观序列的更新与宽表生成。"""

    def __init__(
        self,
        fetcher: BaostockDataFetcher,
        db_writer: MySQLWriter,
        logger,
    ) -> None:
        self.fetcher = fetcher
        self.db_writer = db_writer
        self.logger = logger

    def _recent_quarters(
        self, latest_date: dt.date, lookback_quarters: int
    ) -> list[tuple[int, int]]:
        if lookback_quarters <= 0:
            return []

        month = latest_date.month
        current_quarter = (month - 1) // 3 + 1
        year = latest_date.year

        quarters: list[tuple[int, int]] = []
        for _ in range(lookback_quarters):
            quarters.append((year, current_quarter))
            current_quarter -= 1
            if current_quarter == 0:
                current_quarter = 4
                year -= 1

        quarters.reverse()
        return quarters

    def _load_existing(self, table: str) -> pd.DataFrame:
        with self.db_writer.engine.begin() as conn:
            try:
                return pd.read_sql(text(f"SELECT * FROM `{table}`"), conn)
            except Exception:  # noqa: BLE001
                return pd.DataFrame()

    def _upsert_dataframe(
        self, df: pd.DataFrame, table: str, subset: Iterable[str] | None
    ) -> None:
        if df.empty:
            self.logger.info("表 %s 本次无新增数据，跳过写入。", table)
            return

        existing = self._load_existing(table)
        combined = pd.concat([existing, df], ignore_index=True)
        if subset:
            combined = combined.drop_duplicates(subset=list(subset), keep="last")
        combined.reset_index(drop=True, inplace=True)
        self.db_writer.write_dataframe(combined, table, if_exists="replace")
        self.logger.info("表 %s 已更新，当前总行数：%s", table, len(combined))

    def _fetch_quarterly_table(
        self,
        table: str,
        fetch_fn,
        codes: list[str],
        quarter_pairs: list[tuple[int, int]],
    ) -> None:
        frames: list[pd.DataFrame] = []
        for year, quarter in quarter_pairs:
            for code in codes:
                try:
                    df = fetch_fn(code=code, year=year, quarter=quarter)
                except Exception as exc:  # noqa: BLE001
                    self.logger.warning(
                        "拉取 %s 年 Q%s %s 数据失败: %s", year, quarter, code, exc
                    )
                    continue

                if df.empty:
                    continue

                df["code"] = code
                df["year"] = year
                df["quarter"] = quarter
                frames.append(df)

        if not frames:
            self.logger.warning("%s 在指定季度内无任何返回，跳过写入。", table)
            return

        combined = pd.concat(frames, ignore_index=True)
        self._upsert_dataframe(combined, table, subset=["code", "year", "quarter"])

    def update_quarterly_fundamentals(
        self,
        codes: list[str],
        latest_trade_day: str,
        lookback_quarters: int = 8,
    ) -> None:
        latest_date = dt.datetime.strptime(latest_trade_day, "%Y-%m-%d").date()
        quarter_pairs = self._recent_quarters(latest_date, lookback_quarters)

        task_map = {
            "fundamentals_quarter_profit": self.fetcher.get_profit_data,
            "fundamentals_quarter_growth": self.fetcher.get_growth_data,
            "fundamentals_quarter_balance": self.fetcher.get_balance_data,
            "fundamentals_quarter_cashflow": self.fetcher.get_cash_flow_data,
            "fundamentals_quarter_operation": self.fetcher.get_operation_data,
            "fundamentals_quarter_dupont": self.fetcher.get_dupont_data,
        }

        for table, fetch_fn in task_map.items():
            self.logger.info(
                "开始更新 %s，季度窗口：%s", table, quarter_pairs
            )
            self._fetch_quarterly_table(table, fetch_fn, codes, quarter_pairs)

    def update_company_reports(
        self,
        codes: list[str],
        start_date: str,
        end_date: str,
    ) -> None:
        express_frames: list[pd.DataFrame] = []
        forecast_frames: list[pd.DataFrame] = []

        for code in codes:
            try:
                express_df = self.fetcher.get_performance_express_report(
                    code=code, start_date=start_date, end_date=end_date
                )
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取 %s 业绩快报失败: %s", code, exc)
                express_df = pd.DataFrame()

            if not express_df.empty:
                express_df["code"] = code
                express_frames.append(express_df)

            try:
                forecast_df = self.fetcher.get_forecast_report(
                    code=code, start_date=start_date, end_date=end_date
                )
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取 %s 业绩预告失败: %s", code, exc)
                forecast_df = pd.DataFrame()

            if not forecast_df.empty:
                forecast_df["code"] = code
                forecast_frames.append(forecast_df)

        if express_frames:
            express_combined = pd.concat(express_frames, ignore_index=True)
            subset = [col for col in ["code", "pubDate", "statDate"] if col in express_combined]
            self._upsert_dataframe(
                express_combined,
                "a_share_performance_express",
                subset=subset or None,
            )
        else:
            self.logger.info("本次无业绩快报数据返回。")

        if forecast_frames:
            forecast_combined = pd.concat(forecast_frames, ignore_index=True)
            subset = [col for col in ["code", "profitForcastPubDate", "statDate"] if col in forecast_combined]
            self._upsert_dataframe(
                forecast_combined,
                "a_share_forecast_report",
                subset=subset or None,
            )
        else:
            self.logger.info("本次无业绩预告数据返回。")

    def update_corporate_actions(
        self,
        codes: list[str],
        start_date: str,
        end_date: str,
        start_year: int,
        end_year: int,
    ) -> None:
        dividend_frames: list[pd.DataFrame] = []
        for code in codes:
            for year in range(start_year, end_year + 1):
                try:
                    df = self.fetcher.get_dividend_data(code=code, year=year)
                except Exception as exc:  # noqa: BLE001
                    self.logger.warning("拉取 %s %s 年分红数据失败: %s", code, year, exc)
                    continue
                if df.empty:
                    continue
                df["code"] = code
                df["year"] = year
                dividend_frames.append(df)

        if dividend_frames:
            dividend_combined = pd.concat(dividend_frames, ignore_index=True)
            subset = [col for col in ["code", "dividendDate"] if col in dividend_combined]
            self._upsert_dataframe(
                dividend_combined, "a_share_dividend_events", subset=subset or None
            )
        else:
            self.logger.info("分红数据为空，跳过写入。")

        adjust_frames: list[pd.DataFrame] = []
        for code in codes:
            try:
                df = self.fetcher.get_adjust_factor(
                    code=code, start_date=start_date, end_date=end_date
                )
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取 %s 复权因子失败: %s", code, exc)
                continue
            if df.empty:
                continue
            df["code"] = code
            adjust_frames.append(df)

        if adjust_frames:
            adjust_combined = pd.concat(adjust_frames, ignore_index=True)
            subset = [col for col in ["code", "tradeDate"] if col in adjust_combined]
            self._upsert_dataframe(
                adjust_combined, "a_share_adjust_factor", subset=subset or None
            )
        else:
            self.logger.info("复权因子数据为空，跳过写入。")

    def update_macro_series(self, start_date: str, end_date: str) -> None:
        macro_tasks = {
            "macro_deposit_rate": self.fetcher.get_deposit_rate_data,
            "macro_loan_rate": self.fetcher.get_loan_rate_data,
            "macro_rrr": self.fetcher.get_required_reserve_ratio_data,
            "macro_money_supply_month": self.fetcher.get_money_supply_data_month,
            "macro_money_supply_year": self.fetcher.get_money_supply_data_year,
        }

        for table, fetch_fn in macro_tasks.items():
            try:
                df = fetch_fn(start_date=start_date, end_date=end_date)
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("拉取宏观数据 %s 失败: %s", table, exc)
                continue

            if df.empty:
                self.logger.info("宏观表 %s 返回为空，跳过。", table)
                continue

            self._upsert_dataframe(df, table, subset=None)

        if hasattr(self.fetcher, "get_shibor_data"):
            try:
                df_shibor = self.fetcher.get_shibor_data(
                    start_date=start_date, end_date=end_date
                )
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("Shibor 数据拉取失败: %s", exc)
                return

            if df_shibor.empty:
                self.logger.info("Shibor 数据为空，跳过写入。")
            else:
                self._upsert_dataframe(df_shibor, "macro_shibor", subset=None)
        else:
            self.logger.info("当前 Baostock 版本缺少 Shibor 接口，已跳过。")

    def build_latest_wide(self) -> pd.DataFrame:
        table_prefix = {
            "fundamentals_quarter_profit": "profit",
            "fundamentals_quarter_growth": "growth",
            "fundamentals_quarter_balance": "balance",
            "fundamentals_quarter_cashflow": "cash_flow",
            "fundamentals_quarter_operation": "operation",
            "fundamentals_quarter_dupont": "dupont",
        }

        merged: pd.DataFrame | None = None
        for table, prefix in table_prefix.items():
            df = self._load_existing(table)
            if df.empty:
                continue

            sort_cols = [col for col in ["year", "quarter"] if col in df.columns]
            if sort_cols:
                df = df.sort_values(sort_cols)
            latest = df.groupby("code", as_index=False).tail(1)

            rename_map = {
                col: f"{prefix}_{col}"
                for col in latest.columns
                if col != "code"
            }
            latest = latest.rename(columns=rename_map)

            if merged is None:
                merged = latest
            else:
                merged = merged.merge(latest, on="code", how="outer")

        if merged is None:
            self.logger.warning("未能生成基础宽表，所有基础表均为空。")
            return pd.DataFrame()

        merged = merged.reset_index(drop=True)
        self.db_writer.write_dataframe(merged, "fundamentals_latest_wide")
        self.logger.info("已生成宽表 fundamentals_latest_wide，共 %s 行。", len(merged))
        return merged

    def refresh_all(
        self,
        codes: list[str],
        latest_trade_day: str,
        quarterly_lookback: int = 8,
        report_lookback_years: int = 2,
        adjust_lookback_years: int = 1,
        update_reports: bool = True,
        update_corporate_actions: bool = True,
        update_macro: bool = True,
    ) -> pd.DataFrame:
        if not codes:
            self.logger.warning("股票代码为空，跳过财务与宏观数据更新。")
            return pd.DataFrame()

        self.update_quarterly_fundamentals(
            codes, latest_trade_day, lookback_quarters=quarterly_lookback
        )

        end_date = latest_trade_day

        if update_reports and report_lookback_years > 0:
            start_date = (
                dt.datetime.strptime(latest_trade_day, "%Y-%m-%d").date()
                - dt.timedelta(days=365 * report_lookback_years)
            ).isoformat()
            self.update_company_reports(codes, start_date=start_date, end_date=end_date)

        if update_corporate_actions and adjust_lookback_years > 0:
            adjust_start = (
                dt.datetime.strptime(latest_trade_day, "%Y-%m-%d").date()
                - dt.timedelta(days=365 * adjust_lookback_years)
            ).isoformat()
            adjust_year_start = dt.datetime.strptime(adjust_start, "%Y-%m-%d").year
            adjust_year_end = dt.datetime.strptime(end_date, "%Y-%m-%d").year
            self.update_corporate_actions(
                codes,
                start_date=adjust_start,
                end_date=end_date,
                start_year=adjust_year_start,
                end_year=adjust_year_end,
            )

        if update_macro:
            macro_start = "2000-01-01"
            self.update_macro_series(start_date=macro_start, end_date=end_date)

        return self.build_latest_wide()

================================================================================
FILE: ashare/indicator_utils.py
================================================================================

from __future__ import annotations

import pandas as pd


def consecutive_true(mask: pd.Series) -> pd.Series:
    """计算布尔序列的连续为 True 的累积天数。

    参数
    ----
    mask: pd.Series
        需要计算连续 True 段长度的布尔序列。

    返回
    ----
    pd.Series
        与 mask 等长的整数序列，表示当前位置连续为 True 的天数，False 位置为 0。
    """

    mask = mask.fillna(False).astype(bool)
    changes = (mask != mask.shift()).cumsum()
    streak = mask.groupby(changes).cumcount().add(1)
    return streak.where(mask, 0)

================================================================================
FILE: ashare/ma5_ma20_trend_strategy.py
================================================================================

"""MA5-MA20 顺势趋势波段系统

把“顺势 + MA5/MA20 触发 + 量价/指标过滤 + 风险第一 + 低频交易”落到你的程序里。

数据依赖：
  history_daily_kline（全量日线表；运行时按日期窗口截取，避免依赖 history_recent_xxx_days VIEW）

输出：
  - strategy_indicator_daily：指标明细（按信号计算窗口写入）
  - strategy_signal_events：信号事件（signal/reason/risk/stop 等）
      - signals_write_scope=latest：仅写入最新交易日（默认）
      - signals_write_scope=window：写入本次计算窗口内的全部交易日（用于回填历史/回测）
  - 信号落表 strategy_signal_events；候选过滤由 open_monitor 在 Python 中完成

说明：
  - 本实现先做“日线低频版本”作为选股/清单层。
  - 若要更严格的“分钟线执行层”（例如 60 分钟入场 / 30 分钟离场），建议按需拉分钟线。
"""

from __future__ import annotations

import datetime as dt
import json
from dataclasses import dataclass
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from sqlalchemy import bindparam, text
from sqlalchemy.exc import OperationalError

from .chip_filter import ChipFilter
from .config import get_section
from .db import DatabaseConfig, MySQLWriter
from .env_snapshot_utils import load_trading_calendar
from .indicator_utils import consecutive_true
from .schema_manager import (
    STRATEGY_CODE_MA5_MA20_TREND,
    SchemaManager,
    TABLE_STRATEGY_CHIP_FILTER,
    TABLE_STRATEGY_INDICATOR_DAILY,
    TABLE_STRATEGY_CANDIDATES,
    TABLE_STRATEGY_SIGNAL_EVENTS,
)
from .strategy_candidates import StrategyCandidatesService
from .utils import setup_logger


@dataclass(frozen=True)
class MA5MA20Params:
    """策略参数（支持从 config.yaml 的 strategy_ma5_ma20_trend 节覆盖）。"""

    enabled: bool = False
    lookback_days: int = 365

    # 日线数据来源表：默认直接用全量表（性能更稳），必要时你也可以在 config.yaml 覆盖
    daily_table: str = "history_daily_kline"

    # 放量确认：volume / vol_ma >= threshold
    volume_ratio_threshold: float = 1.5
    volume_ma_window: int = 5

    # 趋势过滤用均线（多头排列）
    trend_ma_short: int = 20
    trend_ma_mid: int = 60
    trend_ma_long: int = 250

    # 回踩买点：close 与 MA20 偏离比例
    pullback_band: float = 0.01

    # KDJ 低位阈值（可选增强：只做 reason 标记，不强制）
    kdj_low_threshold: float = 30.0

    # 输出表/视图
    indicator_table: str = TABLE_STRATEGY_INDICATOR_DAILY
    signal_events_table: str = TABLE_STRATEGY_SIGNAL_EVENTS

    # signals 写入范围：
    # - latest：仅写入最新交易日（默认，低开销）
    # - window：写入本次计算窗口内的全部交易日（用于回填历史/回测）
    signals_write_scope: str = "latest"
    valid_days: int = 3

    @classmethod
    def from_config(cls) -> "MA5MA20Params":
        sec = get_section("strategy_ma5_ma20_trend")
        if not sec:
            return cls()
        kwargs = {}
        indicator_table = sec.get("indicator_table")
        if indicator_table is None:
            indicator_table = sec.get("signals_indicator_table")
        if indicator_table is not None:
            kwargs["indicator_table"] = str(indicator_table).strip()
        events_table = sec.get("signal_events_table")
        if events_table is None:
            events_table = sec.get("signals_table")
        if events_table is not None:
            kwargs["signal_events_table"] = str(events_table).strip()
        for k in cls.__dataclass_fields__.keys():  # type: ignore[attr-defined]
            if k in sec:
                kwargs[k] = sec[k]
        return cls(**kwargs)


def _ensure_datetime(series: pd.Series) -> pd.Series:
    if np.issubdtype(series.dtype, np.datetime64):
        return series
    return pd.to_datetime(series, errors="coerce")


def _normalize_list_date(series: pd.Series) -> pd.Series:
    raw = series.copy()
    as_str = raw.astype(str)
    digit_mask = as_str.str.fullmatch(r"\d{8}")
    parsed_digits = pd.to_datetime(as_str.where(digit_mask), format="%Y%m%d", errors="coerce")
    parsed_general = pd.to_datetime(raw, errors="coerce")
    parsed = parsed_digits.combine_first(parsed_general)
    parsed = parsed.where(~raw.isna(), pd.NaT)
    return parsed


def _to_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    out = df.copy()
    for c in cols:
        if c in out.columns:
            out[c] = pd.to_numeric(out[c], errors="coerce")
    return out


def _ema(s: pd.Series, span: int) -> pd.Series:
    return s.ewm(span=span, adjust=False).mean()


def _macd(
    close: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9
) -> Tuple[pd.Series, pd.Series, pd.Series]:
    """MACD: DIF/DEA/HIST(2*(DIF-DEA))"""
    dif = _ema(close, fast) - _ema(close, slow)
    dea = _ema(dif, signal)
    hist = 2 * (dif - dea)
    return dif, dea, hist


def _kdj(
    high: pd.Series, low: pd.Series, close: pd.Series, n: int = 9
) -> Tuple[pd.Series, pd.Series, pd.Series]:
    """标准 KDJ(9,3,3) 迭代实现。"""
    low_n = low.rolling(n, min_periods=1).min()
    high_n = high.rolling(n, min_periods=1).max()
    denom = (high_n - low_n).replace(0, np.nan)
    rsv = ((close - low_n) / denom * 100.0).fillna(0.0)

    k_vals: List[float] = []
    d_vals: List[float] = []
    k_prev = 50.0
    d_prev = 50.0
    for v in rsv.to_numpy():
        k_now = (2.0 / 3.0) * k_prev + (1.0 / 3.0) * float(v)
        d_now = (2.0 / 3.0) * d_prev + (1.0 / 3.0) * k_now
        k_vals.append(k_now)
        d_vals.append(d_now)
        k_prev, d_prev = k_now, d_now

    k = pd.Series(k_vals, index=close.index, name="kdj_k")
    d = pd.Series(d_vals, index=close.index, name="kdj_d")
    j = (3 * k - 2 * d).rename("kdj_j")
    return k, d, j


def _atr(high: pd.Series, low: pd.Series, preclose: pd.Series, n: int = 14) -> pd.Series:
    tr1 = (high - low).abs()
    tr2 = (high - preclose).abs()
    tr3 = (low - preclose).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    return tr.rolling(n, min_periods=1).mean().rename("atr14")


def _rsi(close: pd.Series, n: int = 14) -> pd.Series:
    delta = close.diff()
    gain = delta.clip(lower=0)
    loss = (-delta).clip(lower=0)
    avg_gain = gain.ewm(alpha=1 / n, adjust=False, min_periods=n).mean()
    avg_loss = loss.ewm(alpha=1 / n, adjust=False, min_periods=n).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.rename("rsi14")


def _split_exchange_symbol(code: str) -> Tuple[str, str]:
    code_s = str(code or "").strip()
    if not code_s:
        return "", ""
    if "." in code_s:
        ex, sym = code_s.split(".", 1)
        return ex.lower(), sym
    return "", code_s


class MA5MA20StrategyRunner:
    """从 MySQL 读取日线 → 计算指标 → 生成 MA5-MA20 信号 → 写回 MySQL。"""

    def __init__(self) -> None:
        self.logger = setup_logger()
        self.params = MA5MA20Params.from_config()
        self.db_writer = MySQLWriter(DatabaseConfig.from_env())
        self.candidates_service = StrategyCandidatesService(self.db_writer, self.logger)
        self.indicator_window = self._resolve_indicator_window()
        self._fundamentals_cache: pd.DataFrame | None = None
        self._stock_basic_cache: pd.DataFrame | None = None

    def _resolve_indicator_window(self) -> int:
        try:
            lookback = int(self.params.lookback_days)
        except Exception:
            self.logger.warning(
                "lookback_days=%s 解析失败，将回退默认 365 天。",
                self.params.lookback_days,
            )
            return 365
        if lookback <= 0:
            self.logger.warning(
                "lookback_days=%s 无效，需为正整数，将回退默认 365 天。",
                self.params.lookback_days,
            )
            return 365
        return lookback

    def _daily_table_name(self) -> str:
        tbl = (getattr(self.params, "daily_table", "") or "").strip()
        if tbl:
            return tbl
        return f"history_recent_{int(self.params.lookback_days)}_days"

    def _get_latest_trade_date(self) -> dt.date:
        tbl = self._daily_table_name()
        stmt = text(f"SELECT MAX(`date`) AS max_date FROM `{tbl}`")
        with self.db_writer.engine.begin() as conn:
            row = conn.execute(stmt).mappings().first()
        if not row or not row.get("max_date"):
            raise RuntimeError(f"{tbl} 为空，无法运行策略。请先运行 python start.py")
        v = row["max_date"]
        ts = pd.to_datetime(v, errors="coerce")
        if pd.isna(ts):
            raise RuntimeError(f"无法解析最新交易日：{v!r}")
        return ts.date()

    def _load_daily_kline(self, codes: List[str], end_date: dt.date) -> pd.DataFrame:
        """读取最近 indicator_window 天的日线数据，用于指标计算。"""
        tbl = self._daily_table_name()
        lookback = int(self.indicator_window)
        start_date = end_date - dt.timedelta(days=int(lookback * 2))  # 给交易日留冗余

        # 你的 history_daily_kline 的 date/code 是 TEXT，统一用 ISO 字符串做范围过滤（避免类型隐式转换）
        start_date_s = start_date.isoformat()
        end_date_s = end_date.isoformat()

        # 只取策略计算必需列，降低 I/O
        select_cols = "`date`,`code`,`high`,`low`,`close`,`preclose`,`volume`,`amount`"

        codes = [str(c) for c in (codes or []) if str(c).strip()]
        use_in = bool(codes) and len(codes) <= 2000

        with self.db_writer.engine.begin() as conn:
            if not codes:
                stmt = text(
                    f"""
                    SELECT {select_cols}
                    FROM `{tbl}`
                    WHERE `date` BETWEEN :start_date AND :end_date
                    ORDER BY `code`,`date`
                    """
                )
                df = pd.read_sql(stmt, conn, params={"start_date": start_date_s, "end_date": end_date_s})
            elif use_in:
                # IN 列表分批，避免 SQL 过长/解析慢
                chunk_size = 800
                stmt = (
                    text(
                        f"""
                        SELECT {select_cols}
                        FROM `{tbl}`
                        WHERE `code` IN :codes AND `date` BETWEEN :start_date AND :end_date
                        ORDER BY `code`,`date`
                        """
                    )
                    .bindparams(bindparam("codes", expanding=True))
                )
                parts: List[pd.DataFrame] = []
                for i in range(0, len(codes), chunk_size):
                    part_codes = codes[i : i + chunk_size]
                    part = pd.read_sql(
                        stmt,
                        conn,
                        params={"codes": part_codes, "start_date": start_date_s, "end_date": end_date_s},
                    )
                    if not part.empty:
                        parts.append(part)
                df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()
            else:
                # 极端情况：codes 很多时退化为按日期读取（不建议用 all 做日线指标计算）
                stmt = text(
                    f"""
                    SELECT {select_cols}
                    FROM `{tbl}`
                    WHERE `date` BETWEEN :start_date AND :end_date
                    ORDER BY `code`,`date`
                    """
                )
                df = pd.read_sql(stmt, conn, params={"start_date": start_date_s, "end_date": end_date_s})

        if df.empty:
            raise RuntimeError(f"未能从 {tbl} 读取到任何日线数据。")

        df["date"] = _ensure_datetime(df["date"])
        df["code"] = df["code"].astype(str)

        df = _to_numeric(
            df,
            ["high", "low", "close", "preclose", "volume", "amount"],
        )
        df = df.dropna(subset=["date", "code", "close", "high", "low", "preclose"]).copy()

        df = df.sort_values(["code", "date"]).reset_index(drop=True)
        df = df.groupby("code", group_keys=False).tail(lookback).reset_index(drop=True)
        return df

    def _load_fundamentals_latest(self) -> pd.DataFrame:
        table = "fundamentals_latest_wide"
        try:
            with self.db_writer.engine.begin() as conn:
                return pd.read_sql(text(f"SELECT * FROM `{table}`"), conn)
        except Exception as exc:  # noqa: BLE001
            self.logger.info("未能读取 %s（将跳过基本面标签）：%s", table, exc)
            return pd.DataFrame()

    def _load_stock_basic(self) -> pd.DataFrame:
        table = "a_share_stock_basic"
        try:
            with self.db_writer.engine.begin() as conn:
                try:
                    df = pd.read_sql(
                        text("SELECT `code`,`code_name`,`ipoDate` FROM `a_share_stock_basic`"),
                        conn,
                    )
                except OperationalError as exc:
                    if "1054" in str(exc) or "Unknown column" in str(exc):
                        self.logger.info(
                            "读取 %s 字段 ['code', 'code_name', 'ipoDate'] 失败，将回退基础字段：%s",
                            table,
                            exc,
                        )
                        return pd.read_sql(
                            text(f"SELECT `code`,`code_name` FROM `{table}`"), conn
                        )
                    raise

                if "ipoDate" in df.columns:
                    df["ipoDate"] = _normalize_list_date(df["ipoDate"])
                return df
        except Exception as exc:  # noqa: BLE001
            self.logger.info(
                "读取 %s 失败，将跳过 ST 标签与板块限幅识别：%s",
                table,
                exc,
            )
            return pd.DataFrame()

    def _compute_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """为每个 code 计算均线、量能、MACD、KDJ、ATR。"""
        df = df.sort_values(["code", "date"]).copy()

        # 用 groupby().rolling() 替代 transform(lambda...)，减少 Python 层开销
        g_close = df.groupby("code", sort=False)["close"]
        g_vol = df.groupby("code", sort=False)["volume"]

        df["ma5"] = g_close.rolling(5, min_periods=5).mean().reset_index(level=0, drop=True)
        df["ma10"] = g_close.rolling(10, min_periods=10).mean().reset_index(level=0, drop=True)
        df["ma20"] = g_close.rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)
        df["ma60"] = g_close.rolling(60, min_periods=60).mean().reset_index(level=0, drop=True)
        df["ma250"] = g_close.rolling(250, min_periods=250).mean().reset_index(level=0, drop=True)

        vol_win = int(self.params.volume_ma_window)
        df["vol_ma"] = g_vol.rolling(vol_win, min_periods=vol_win).mean().reset_index(level=0, drop=True)
        df["vol_ratio"] = df["volume"] / df["vol_ma"]
        df["avg_volume_20"] = g_vol.rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)

        def _add_oscillators(sub: pd.DataFrame) -> pd.DataFrame:
            group_code = getattr(sub, "name", None)
            close = sub["close"]
            high = sub["high"]
            low = sub["low"]
            preclose = sub["preclose"]

            dif, dea, hist = _macd(close)
            k, d, j = _kdj(high, low, close)
            atr = _atr(high, low, preclose)
            rsi = _rsi(close)

            sub = sub.copy()
            sub["macd_dif"] = dif
            sub["macd_dea"] = dea
            sub["macd_hist"] = hist
            sub["prev_macd_hist"] = hist.shift(1)
            sub["kdj_k"] = k
            sub["kdj_d"] = d
            sub["kdj_j"] = j
            sub["atr14"] = atr
            sub["rsi14"] = rsi
            # pandas>=2.2, include_groups=False 会去掉分组列，主动恢复 code
            if "code" not in sub.columns:
                sub["code"] = group_code
            return sub

        # pandas 新版本会提示 groupby.apply 将来不再包含分组列，提前兼容
        try:
            df = (
                df.groupby("code", group_keys=False)
                .apply(_add_oscillators, include_groups=False)  # pandas>=2.2
                .reset_index(drop=True)
            )
        except TypeError:
            # 兼容旧 pandas（没有 include_groups 参数）
            df = df.groupby("code", group_keys=False).apply(_add_oscillators).reset_index(drop=True)
        return df

    def _select_column(
        self, df: pd.DataFrame, candidates: List[str], *, contains: str | None = None
    ) -> str | None:
        for col in candidates:
            if col in df.columns:
                return col
        if contains:
            contains_lower = contains.lower()
            for col in df.columns:
                if contains_lower in col.lower():
                    return col
        return None

    def _build_fundamental_risk_map(self, fundamentals: pd.DataFrame) -> Dict[str, Dict[str, str]]:
        if fundamentals.empty:
            return {}

        netprofit_col = self._select_column(
            fundamentals,
            ["profit_netProfit", "profit_netprofit", "profit_net_profit"],
            contains="netprofit",
        )
        yoy_col = self._select_column(
            fundamentals,
            ["growth_YOYNI", "growth_YOYPNI", "growth_YOYEPSBasic"],
            contains="yoy",
        )

        if netprofit_col is None and yoy_col is None:
            self.logger.info(
                "fundamentals_latest_wide 缺少净利润/同比列，跳过基本面风险标签。"
            )
            return {}

        merged = fundamentals[
            [c for c in ["code", netprofit_col, yoy_col] if c in fundamentals.columns]
        ].copy()
        if merged.empty:
            return {}

        for col in [netprofit_col, yoy_col]:
            if col and col in merged.columns:
                merged[col] = pd.to_numeric(merged[col], errors="coerce")

        risk_map: Dict[str, Dict[str, str]] = {}
        for _, row in merged.iterrows():
            code = str(row.get("code") or "").strip()
            if not code:
                continue

            tags: list[str] = []
            notes: list[str] = []

            netprofit_val = row.get(netprofit_col) if netprofit_col else None
            if (
                netprofit_val is not None
                and not pd.isna(netprofit_val)
                and float(netprofit_val) <= 0
            ):
                tags.append("NO_PROFIT")
                notes.append(f"净利润为 {float(netprofit_val):.2f}，缺乏业绩支撑")

            yoy_val = row.get(yoy_col) if yoy_col else None
            if yoy_val is not None and not pd.isna(yoy_val) and float(yoy_val) < 0:
                tags.append("WEAK_GROWTH")
                notes.append(f"净利润同比 {float(yoy_val):.2f}% 为负")

            if tags:
                risk_map[code] = {
                    "tag": "|".join(tags),
                    "note": "；".join(notes),
                }

        return risk_map

    def _build_board_masks(
        self, codes: pd.Series, stock_basic: pd.DataFrame
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        parts = codes.map(_split_exchange_symbol)
        code_parts = pd.DataFrame(parts.tolist(), columns=["exchange", "symbol"]).fillna("")

        symbols = code_parts["symbol"].astype(str)
        exchanges = code_parts["exchange"].astype(str)

        mask_bj = (exchanges == "bj") | symbols.str.startswith(("43", "83"))
        mask_growth = symbols.str.startswith(
            ("300", "301", "302", "303", "688", "689")
        )

        mask_st = pd.Series(False, index=codes.index)
        if not stock_basic.empty:
            cols_lower = {c.lower(): c for c in stock_basic.columns}
            code_col = cols_lower.get("code")
            name_col = cols_lower.get("code_name") or cols_lower.get("name")
            if code_col and name_col and name_col in stock_basic.columns:
                base = stock_basic[[code_col, name_col]].dropna().copy()
                base[code_col] = base[code_col].astype(str)
                base[name_col] = base[name_col].astype(str)
                base = base.drop_duplicates(subset=[code_col], keep="last")
                names = base.set_index(code_col)[name_col]

                def _lookup_name(raw_code: str) -> str:
                    direct = names.get(raw_code, "")
                    if isinstance(direct, pd.Series):
                        direct = direct.iloc[0] if not direct.empty else ""
                    if direct:
                        return str(direct)
                    ex, sym = _split_exchange_symbol(raw_code)
                    if sym and sym in names:
                        return str(names.get(sym, ""))
                    if ex and f"{ex}.{sym}" in names:
                        return str(names.get(f"{ex}.{sym}", ""))
                    return ""

                mask_st = codes.map(lambda c: _lookup_name(str(c))).str.upper().str.contains("ST")

        return mask_growth, mask_bj, mask_st

    def _generate_signals(
        self,
        df: pd.DataFrame,
        fundamentals: pd.DataFrame | None = None,
        stock_basic: pd.DataFrame | None = None,
    ) -> pd.DataFrame:
        """生成 BUY/SELL/HOLD 信号，并给出 reason。"""
        p = self.params

        def _prev_state(mask: pd.Series) -> pd.Series:
            """分组取前一日布尔状态，兼容 pandas 新版的 fillna 行为。"""

            return (
                mask.groupby(df["code"])
                .shift(1)
                .astype("boolean")
                .fillna(False)
                .infer_objects(copy=False)
                .astype(bool)
            )

        # 1) 趋势过滤：多头排列 + 价格站上中长均线
        trend_ok = (
            (df["close"] > df["ma60"])
            & (df["close"] > df["ma250"])
            & (df["ma20"] > df["ma60"])
            & (df["ma60"] > df["ma250"])
        )

        # 2) MA5/MA20 金叉/死叉
        ma5_gt_ma20 = (df["ma5"] > df["ma20"]).astype(bool)
        prev_ma5_gt_ma20 = _prev_state(ma5_gt_ma20)
        cross_up = ma5_gt_ma20 & (~prev_ma5_gt_ma20)
        cross_down = (~ma5_gt_ma20) & prev_ma5_gt_ma20
        prev_macd_hist = pd.to_numeric(df.get("prev_macd_hist"), errors="coerce")
        hist_cross_up = (df["macd_hist"] > 0) & (prev_macd_hist <= 0)
        hist_cross_down = (df["macd_hist"] < 0) & (prev_macd_hist >= 0)

        # 3) 放量确认
        vol_ok = df["vol_ratio"] >= float(p.volume_ratio_threshold)

        # 4) MACD 过滤（DIF 上穿 DEA 或 HIST>0）
        macd_gt = (df["macd_dif"] > df["macd_dea"]).astype(bool)
        prev_macd_gt = _prev_state(macd_gt)
        macd_cross_up = macd_gt & (~prev_macd_gt)
        macd_ok = macd_cross_up | (df["macd_hist"] > 0)
        macd_event = np.select(
            [hist_cross_up, hist_cross_down],
            ["MACD_HIST_CROSS_UP", "MACD_HIST_CROSS_DOWN"],
            default="",
        )

        # 5) KDJ 低位金叉（可选增强：只作为 reason 标记）
        kdj_gt = (df["kdj_k"] > df["kdj_d"]).astype(bool)
        prev_kdj_gt = _prev_state(kdj_gt)
        kdj_cross_up = kdj_gt & (~prev_kdj_gt)
        kdj_low = df["kdj_k"] <= float(p.kdj_low_threshold)
        kdj_ok = kdj_cross_up & kdj_low

        # 6) 趋势回踩（close 接近 MA20 + MA5 向上）
        pullback_band = float(p.pullback_band)
        pullback_near = ((df["close"] - df["ma20"]).abs() / df["ma20"]) <= pullback_band
        ma5_up = (df["ma5"] - df["ma5"].groupby(df["code"]).shift(1)) > 0

        buy_cross = trend_ok & cross_up & vol_ok & macd_ok
        buy_pullback = trend_ok & pullback_near & ma5_up & macd_ok
        buy_macd_confirm = hist_cross_up & (df["close"] >= df["ma20"])
        pattern_series = self._detect_price_patterns(df)
        buy_pattern = (pattern_series == "W_BOTTOM_CONFIRMED") & (df["close"] > df["ma20"])

        # 卖出：死叉 或 跌破 MA20 且放量（趋势破坏）
        sell_without_hist = cross_down | ((df["close"] < df["ma20"]) & vol_ok)
        sell = sell_without_hist | hist_cross_down
        prev_ma5 = df.groupby("code")["ma5"].shift(1)
        prev_ma20 = df.groupby("code")["ma20"].shift(1)
        dead_cross = (df["ma5"] < df["ma20"]) & (prev_ma5 > prev_ma20) & (df["vol_ratio"] < 1.0)
        reduce_mask = (df["ma5"] < df["ma20"]) & (df["vol_ratio"] < 1.0) & (~dead_cross)

        # 用 pandas Series 拼接原因，避免 numpy.ndarray 没有 strip 的问题
        reason = pd.Series("", index=df.index, dtype="object")
        reason = reason.mask(buy_cross, "MA5上穿MA20（金叉）+放量+MACD")

        def _append(base: pd.Series, cond: pd.Series, text_: str) -> pd.Series:
            add = np.where(base.eq(""), text_, base + "|" + text_)
            return base.mask(cond, pd.Series(add, index=base.index, dtype="object"))

        reason = _append(reason, buy_pullback, "趋势回踩MA20")
        reason = _append(reason, kdj_ok & (buy_cross | buy_pullback), "KDJ低位金叉")
        reason = _append(reason, buy_macd_confirm, "MACD柱翻红")
        reason = _append(reason, buy_pattern, "W底突破")

        # 卖出原因优先覆盖
        reason = reason.mask(sell_without_hist, "MA5下穿MA20（死叉）或跌破MA20放量")
        reason = reason.mask(hist_cross_down, "MACD柱翻绿/死叉")
        reason = reason.mask(dead_cross, "死叉+缩量清")
        reason = reason.mask(reduce_mask, "弱势缩量减仓")
        reason = reason.mask(reason.eq(""), "观望")

        signal = np.select(
            [sell | dead_cross, reduce_mask, buy_cross | buy_pullback | buy_pattern, buy_macd_confirm],
            ["SELL", "REDUCE", "BUY", "BUY_CONFIRM"],
            default="HOLD",
        )

        out = df.copy()
        out["signal"] = signal
        out["reason"] = reason.to_numpy()
        # 风险参考：2*ATR 作为“初始止损价”参考（你也可以换成 MA20 跌破止损）
        out["stop_ref"] = out["close"] - 2.0 * out["atr14"]

        # 妖股/过热风险：短期涨幅+涨停次数+乖离
        out["ret_10"] = out.groupby("code", sort=False)["close"].pct_change(periods=10)
        out["ret_20"] = out.groupby("code", sort=False)["close"].pct_change(periods=20)
        pct_change = np.where(
            out["preclose"] > 0,
            (out["close"] - out["preclose"]) / out["preclose"],
            np.nan,
        )
        out["ma20_bias"] = np.where(
            out["ma20"] != 0, (out["close"] - out["ma20"]) / out["ma20"], np.nan
        )

        def _format_pct(value: float | None) -> str:
            if value is None or pd.isna(value):
                return "N/A"
            return f"{value*100:.2f}%"
        code_series = out["code"].astype(str)
        stock_basic_df = stock_basic if stock_basic is not None else pd.DataFrame()
        mask_growth, mask_bj, mask_st = self._build_board_masks(code_series, stock_basic_df)
        list_date_col = self._select_column(
            stock_basic_df,
            ["ipoDate"],
        )
        code_col = self._select_column(stock_basic_df, ["code"], contains="code")
        listing_days = pd.Series(pd.NA, index=out.index, dtype="Int64")
        if list_date_col and code_col and list_date_col in stock_basic_df.columns:
            base = stock_basic_df[[code_col, list_date_col]].dropna().copy()
            base[code_col] = base[code_col].astype(str)
            base[list_date_col] = pd.to_datetime(base[list_date_col], errors="coerce")
            base = base.dropna(subset=[list_date_col]).drop_duplicates(subset=[code_col])
            list_date_map = base.set_index(code_col)[list_date_col]
            mapped_dates = out["code"].map(list_date_map)
            listing_days = (out["date"] - mapped_dates).dt.days
        out["listing_days"] = listing_days
        limit_up_threshold = pd.Series(
            np.where(mask_bj, 0.295, np.where(mask_growth, 0.195, 0.097)),
            index=out.index,
            dtype=float,
        )

        mania_ret = (out["ret_20"] >= 0.35).fillna(False)
        limit_up_daily = pct_change >= limit_up_threshold
        out["limit_up_cnt_20"] = (
            limit_up_daily.groupby(out["code"], sort=False)
            .rolling(20, min_periods=1)
            .sum()
            .reset_index(level=0, drop=True)
        )
        mania_limit = (out["limit_up_cnt_20"] >= 2).fillna(False)
        mania_bias = (out["ma20_bias"] >= 0.15).fillna(False)
        mania_mask = mania_ret | mania_limit | mania_bias

        ret20_fmt = out["ret_20"].apply(_format_pct)
        bias_fmt = out["ma20_bias"].apply(_format_pct)
        mania_notes_ret = np.where(mania_ret, "20日涨幅 " + ret20_fmt + " 过高", "")
        mania_notes_limit = np.where(
            mania_limit, "20日内涨停 " + out["limit_up_cnt_20"].fillna(0).astype(int).astype(str) + " 次", ""
        )
        mania_notes_bias = np.where(mania_bias, "MA20 乖离 " + bias_fmt + " 偏离成本", "")
        mania_notes = [
            "；".join([v for v in items if v])
            for items in zip(mania_notes_ret, mania_notes_limit, mania_notes_bias)
        ]

        yearline_enabled = out["ma250"].notna()
        if "listing_days" in out.columns and out["listing_days"].notna().any():
            yearline_enabled = yearline_enabled & out["listing_days"].ge(250).fillna(False)
        below_ma250_mask = (
            (out["close"] < out["ma250"]) & yearline_enabled
        ).astype("boolean")
        above_ma250_mask = (
            (out["close"] >= out["ma250"]) & yearline_enabled
        ).astype("boolean")
        prev_below_ma250 = (
            below_ma250_mask.groupby(out["code"], sort=False).shift(1).fillna(False).astype("boolean")
        )
        below_ma250_streak = (
            below_ma250_mask.groupby(out["code"], sort=False)
            .transform(consecutive_true)
            .astype("Int64")
        )
        above_ma250_streak = (
            above_ma250_mask.groupby(out["code"], sort=False)
            .transform(consecutive_true)
            .astype("Int64")
        )
        ma250_slope = out.groupby("code", sort=False)["ma250"].diff()
        yearline_break_confirmed = below_ma250_streak >= 3
        yearline_break_warn = (~yearline_break_confirmed) & (
            below_ma250_mask & (ma250_slope < 0)
        )
        yearline_reclaim_confirmed = (above_ma250_streak >= 2) & prev_below_ma250

        yearline_state = np.select(
            [
                yearline_break_confirmed,
                yearline_reclaim_confirmed & yearline_enabled,
                below_ma250_mask,
                yearline_enabled,
            ],
            [
                "BREAK_CONFIRMED",
                "RECLAIM_CONFIRMED",
                "BELOW_1_2D",
                "ABOVE",
            ],
            default="NO_DATA",
        )

        yearline_tags_confirmed = np.where(
            yearline_break_confirmed, "YEARLINE_BREAK_CONFIRMED", ""
        )
        yearline_tags_warn = np.where(yearline_break_warn, "YEARLINE_BREAK_WARN", "")
        yearline_notes_confirmed = np.where(
            yearline_break_confirmed, "年线连续3日收盘跌破，趋势破位风险", ""
        )
        yearline_notes_warn = np.where(
            yearline_break_warn,
            "年线下穿且年线走弱，警惕有效跌破",
            "",
        )

        fund_df = fundamentals if fundamentals is not None else pd.DataFrame()
        fund_risk_map = self._build_fundamental_risk_map(fund_df)
        fund_tags = code_series.map(lambda c: (fund_risk_map.get(c) or {}).get("tag", ""))
        fund_notes = code_series.map(lambda c: (fund_risk_map.get(c) or {}).get("note", ""))

        mania_tags = np.where(mania_mask, "MANIA", "")
        st_tags = np.where(mask_st.fillna(False), "ST", "")
        st_notes = np.where(mask_st.fillna(False), "风险警示 ST", "")

        risk_tags = [
            "|".join([t for t in (mt, stt, ft, ytc, ytw) if t])
            for mt, stt, ft, ytc, ytw in zip(
                mania_tags,
                st_tags,
                fund_tags,
                yearline_tags_confirmed,
                yearline_tags_warn,
            )
        ]
        risk_notes = [
            "；".join([v for v in (mn, sn, fn, ync, ynw) if v])
            for mn, sn, fn, ync, ynw in zip(
                mania_notes,
                st_notes,
                fund_notes,
                yearline_notes_confirmed,
                yearline_notes_warn,
            )
        ]

        out["yearline_state"] = yearline_state
        out["risk_tag"] = risk_tags
        out["risk_note"] = risk_notes
        out["runup_pct"] = out.groupby("code", sort=False)["close"].pct_change(periods=5)
        fear_score = ((out["kdj_k"] - 50.0) / 50.0) * (1 - out["vol_ratio"])
        out["fear_score"] = fear_score.clip(-3, 3)
        wave_type = np.select(
            [
                (out["rsi14"] < 30) & (out["ma20_bias"] < -0.05),
                out["ma20_bias"].abs() < 0.02,
            ],
            ["OVERSOLD_REVERT", "RANGE"],
            default="TREND_PULLBACK",
        )
        out["wave_type"] = wave_type
        out["pattern_tag"] = pattern_series
        out["macd_event"] = macd_event
        out["hist_cross_up"] = hist_cross_up
        out["hist_cross_down"] = hist_cross_down

        out = self._attach_chip_factors(out)
        out = self._decide_final_action(out)

        extras: List[str] = []
        for _, row in out.iterrows():
            payload = {
                "raw_signal": row.get("raw_signal"),
                "macd_event": row.get("macd_event"),
                "hist_cross_up": bool(row.get("hist_cross_up", False)),
                "hist_cross_down": bool(row.get("hist_cross_down", False)),
                "chip_score": row.get("chip_score"),
                "chip_reason": row.get("chip_reason"),
                "gdhs_delta_pct": row.get("gdhs_delta_pct"),
                "gdhs_announce_date": (
                    row.get("gdhs_announce_date").isoformat()
                    if isinstance(row.get("gdhs_announce_date"), (dt.date, pd.Timestamp))
                    else None
                ),
                "age_days": row.get("age_days"),
                "deadzone_hit": bool(row.get("deadzone_hit", False)),
                "stale_hit": bool(row.get("stale_hit", False)),
                "chip_penalty": row.get("chip_penalty"),
                "chip_note": row.get("chip_note"),
                "gate": row.get("gate_tag"),
                "fear_score": row.get("fear_score"),
                "wave_type": row.get("wave_type"),
                "pattern_tag": row.get("pattern_tag"),
                "runup_pct": row.get("runup_pct"),
            }
            payload = {k: v for k, v in payload.items() if v is not None and not pd.isna(v)}
            extras.append(json.dumps(payload, ensure_ascii=False))
        out["extra_json"] = extras
        return out

    def _detect_price_patterns(self, df: pd.DataFrame) -> pd.Series:
        if df.empty:
            return pd.Series(dtype="object")

        pattern = pd.Series("", index=df.index, dtype="object")
        grouped = df.groupby("code", sort=False)
        for code, sub in grouped:
            lows = pd.to_numeric(sub.get("low"), errors="coerce")
            highs = pd.to_numeric(sub.get("high"), errors="coerce")
            closes = pd.to_numeric(sub.get("close"), errors="coerce")
            vol_ratio = pd.to_numeric(sub.get("vol_ratio"), errors="coerce")

            neckline = highs.rolling(15, min_periods=5).max()
            valley_recent = lows.rolling(20, min_periods=10).min()
            valley_prev = valley_recent.shift(5)
            valley_close = (valley_recent.notna()) & (valley_prev.notna())
            w_bottom = valley_close & ((valley_recent - valley_prev).abs() / valley_prev.clip(lower=1e-6) <= 0.03)
            confirm = closes > neckline
            if not vol_ratio.isna().all():
                confirm = confirm & (vol_ratio > 1.2)
            pattern.loc[sub.index] = np.where(confirm & w_bottom, "W_BOTTOM_CONFIRMED", "")

            flag_slope = closes.diff().rolling(5, min_periods=5).mean()
            flag_std = closes.rolling(5, min_periods=5).std()
            flag_range_ok = (flag_std / closes.replace(0, np.nan)) < 0.02
            flag_body = flag_slope.abs() < (closes.abs() * 0.001)
            breakout = closes > closes.rolling(10, min_periods=5).max()
            if not vol_ratio.isna().all():
                breakout = breakout & (vol_ratio > 1.2)
            pattern.loc[sub.index] = np.where(
                breakout & flag_range_ok & flag_body,
                "FLAG_BREAKOUT",
                pattern.loc[sub.index],
            )
        return pattern

    def _attach_chip_factors(self, signals: pd.DataFrame) -> pd.DataFrame:
        def _fill_vol_ratio(sig_df: pd.DataFrame, dates: List[dt.date], code_list: List[str]) -> pd.DataFrame:
            if "vol_ratio" in sig_df.columns and not sig_df["vol_ratio"].isna().any():
                return sig_df
            try:
                vr_stmt = text(
                    """
                    SELECT `trade_date`AS date, code, vol_ratio
                    FROM strategy_indicator_daily
                    WHERE `trade_date` IN :dates AND code IN :codes
                    """
                ).bindparams(bindparam("dates", expanding=True), bindparam("codes", expanding=True))
                with self.db_writer.engine.begin() as conn:
                    vr_df = pd.read_sql_query(
                        vr_stmt,
                        conn,
                        params={"dates": dates, "codes": code_list},
                    )
            except Exception:
                vr_df = pd.DataFrame()

            if vr_df.empty:
                return sig_df

            # 关键：merge 键 dtype 必须一致，否则会报：
            # ValueError: You are trying to merge on datetime64[ns] and object columns for key 'date'
            sig_df = sig_df.copy()
            vr_df = vr_df.copy()
            if "date" in sig_df.columns:
                sig_df["date"] = pd.to_datetime(sig_df["date"], errors="coerce")
            sig_df["code"] = sig_df["code"].astype(str)
            vr_df["date"] = pd.to_datetime(vr_df["date"], errors="coerce")
            vr_df["code"] = vr_df["code"].astype(str)
            sig_df = sig_df.dropna(subset=["date", "code"])
            vr_df = vr_df.dropna(subset=["date", "code"])
            if sig_df.empty or vr_df.empty:
                return sig_df
            merged_sig = sig_df.merge(vr_df, on=["date", "code"], how="left", suffixes=("", "_ind"))
            merged_sig["vol_ratio"] = pd.to_numeric(merged_sig["vol_ratio"], errors="coerce").fillna(
                pd.to_numeric(merged_sig.get("vol_ratio_ind"), errors="coerce")
            )
            if "vol_ratio_ind" in merged_sig.columns:
                merged_sig = merged_sig.drop(columns=["vol_ratio_ind"])
            return merged_sig

        if signals.empty:
            signals["chip_score"] = np.nan
            signals["gdhs_delta_pct"] = np.nan
            signals["gdhs_announce_date"] = pd.NaT
            signals["chip_reason"] = None
            signals["chip_penalty"] = 0.0
            signals["chip_note"] = None
            signals["age_days"] = np.nan
            signals["deadzone_hit"] = False
            signals["stale_hit"] = False
            return signals

        signals = signals.copy()
        signals["date"] = pd.to_datetime(signals["date"], errors="coerce")
        signals = signals.dropna(subset=["date", "code"])
        signals["code"] = signals["code"].astype(str)
        if signals.empty:
            return signals

        codes = signals["code"].unique().tolist()
        sig_dates = signals["date"].dt.date.unique().tolist()
        chip_table_exists = self._table_exists(TABLE_STRATEGY_CHIP_FILTER)
        chip_df = pd.DataFrame()

        if chip_table_exists:
            stmt = (
                text(
                    f"""
                    SELECT *
                    FROM `{TABLE_STRATEGY_CHIP_FILTER}`
                    WHERE `sig_date` IN :dates AND `code` IN :codes
                    """
                ).bindparams(bindparam("dates", expanding=True), bindparam("codes", expanding=True))
            )
            with self.db_writer.engine.begin() as conn:
                try:
                    chip_df = pd.read_sql_query(
                        stmt,
                        conn,
                        params={"dates": sig_dates, "codes": codes},
                    )
                except Exception:
                    chip_df = pd.DataFrame()

        if chip_df.empty:
            signals = _fill_vol_ratio(signals, sig_dates, codes)
            chip_cols = [
                "date",
                "code",
                "vol_ratio",
                "close",
                "ma20",
                "runup_pct",
                "fear_score",
                "pct_chg",
                "pct_change",
                "change_pct",
                "ret_1d",
                "ret",
            ]
            chip_inputs = [c for c in chip_cols if c in signals.columns]
            chip_df = ChipFilter().apply(signals[chip_inputs].copy())

        stale_mask = pd.Series(False, index=chip_df.index)
        if not chip_df.empty:
            chip_reason_col = chip_df.get("chip_reason")
            if chip_reason_col is None:
                chip_reason_col = pd.Series(pd.NA, index=chip_df.index, dtype="object")
            vol_source = (
                chip_df["vol_ratio"]
                if "vol_ratio" in chip_df.columns
                else pd.Series(np.nan, index=chip_df.index)
            )
            vol_col = pd.to_numeric(vol_source, errors="coerce")
            chip_df["chip_reason"] = chip_reason_col
            stale_mask = (
                chip_reason_col.isna()
                | (chip_reason_col == "")
                | chip_reason_col.eq("DATA_MISSING")
                | vol_col.isna()
            )
        if not chip_df.empty and stale_mask.any():
            stale_rows = chip_df.loc[stale_mask, ["sig_date", "code"]].dropna()
            if not stale_rows.empty:
                stale_dates = pd.to_datetime(stale_rows["sig_date"], errors="coerce").dt.date.dropna().unique().tolist()
                stale_codes = stale_rows["code"].astype(str).unique().tolist()
                re_sig = signals[signals["date"].dt.date.isin(stale_dates) & signals["code"].isin(stale_codes)].copy()
                refreshed = pd.DataFrame()
                if not re_sig.empty:
                    re_sig = _fill_vol_ratio(re_sig, stale_dates, stale_codes)
                    refresh_cols = [
                        "date",
                        "code",
                        "vol_ratio",
                        "close",
                        "ma20",
                        "runup_pct",
                        "fear_score",
                        "pct_chg",
                        "pct_change",
                        "change_pct",
                        "ret_1d",
                        "ret",
                    ]
                    refresh_inputs = [c for c in refresh_cols if c in re_sig.columns]
                    refreshed = ChipFilter().apply(re_sig[refresh_inputs].copy())
                if not refreshed.empty:
                    refreshed = refreshed.copy()
                    refreshed["sig_date"] = pd.to_datetime(refreshed["sig_date"], errors="coerce")
                    chip_df = chip_df[~stale_mask].copy()
                    concat_frames = []
                    for df in (chip_df, refreshed):
                        if df is None or df.empty:
                            continue
                        if not df.notna().any().any():
                            continue
                        concat_frames.append(df)
                    chip_df = pd.concat(concat_frames, ignore_index=True) if concat_frames else pd.DataFrame()
                    chip_df = chip_df.drop_duplicates(subset=["sig_date", "code"], keep="last")

        if chip_df.empty:
            signals["chip_score"] = np.nan
            signals["gdhs_delta_pct"] = np.nan
            signals["gdhs_announce_date"] = pd.NaT
            signals["chip_reason"] = "DATA_MISSING_GDHS"
            signals["chip_penalty"] = 0.0
            signals["chip_note"] = "DATA_MISSING_GDHS"
            signals["age_days"] = np.nan
            signals["deadzone_hit"] = False
            signals["stale_hit"] = False
            return signals

        chip_df = chip_df.rename(columns={"announce_date": "gdhs_announce_date"})
        chip_df["sig_date"] = pd.to_datetime(chip_df["sig_date"], errors="coerce")
        chip_df["code"] = chip_df["code"].astype(str)
        merged = pd.merge(
            signals,
            chip_df,
            how="left",
            left_on=["date", "code"],
            right_on=["sig_date", "code"],
            suffixes=("", "_chip"),
        )
        merged["gdhs_delta_pct"] = pd.to_numeric(merged.get("gdhs_delta_pct"), errors="coerce")
        merged["gdhs_announce_date"] = pd.to_datetime(
            merged.get("gdhs_announce_date"), errors="coerce"
        )
        merged["chip_score"] = pd.to_numeric(merged.get("chip_score"), errors="coerce")
        merged["chip_reason"] = merged.get("chip_reason")
        merged["chip_penalty"] = pd.to_numeric(merged.get("chip_penalty"), errors="coerce").fillna(0.0)
        merged["chip_note"] = merged.get("chip_note")
        merged["age_days"] = pd.to_numeric(merged.get("age_days"), errors="coerce")
        merged["deadzone_hit"] = merged.get("deadzone_hit", False)
        merged["stale_hit"] = merged.get("stale_hit", False)
        gdhs_missing = merged["gdhs_announce_date"].isna() & merged["gdhs_delta_pct"].isna()
        chip_all_missing = gdhs_missing & merged["chip_score"].isna()
        vol_missing = merged["vol_ratio"].isna()
        missing_reason_mask = merged["chip_reason"].isna() | (merged["chip_reason"] == "")
        missing_reason = np.select(
            [
            missing_reason_mask & (gdhs_missing | chip_all_missing),
            missing_reason_mask & vol_missing,
        ],
            ["DATA_MISSING_GDHS", "DATA_MISSING_VOL_RATIO"],
            default=None,
        )
        missing_reason_series = pd.Series(missing_reason, index=merged.index, dtype="object")
        missing_reason_mask = missing_reason_series.notna()
        merged.loc[missing_reason_mask, "chip_reason"] = missing_reason_series[missing_reason_mask].values
        merged.loc[missing_reason_mask, "chip_score"] = 0.0
        merged.loc[missing_reason_mask, "chip_note"] = merged.loc[missing_reason_mask, "chip_reason"]
        return merged

    def _decide_final_action(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            return df

        out = df.copy()
        out["raw_signal"] = out["signal"]
        final_action = pd.Series(out["signal"], index=out.index, dtype="object")
        final_reason = pd.Series(out["reason"], index=out.index, dtype="object")
        base_cap = 0.5
        final_cap = pd.Series(base_cap, index=out.index, dtype=float)
        gate_tag = pd.Series("", index=out.index, dtype="object")

        required_cols = ["close", "ma20", "ma5", "macd_hist"]
        missing_core = out[required_cols].isna().any(axis=1)
        final_action = final_action.mask(missing_core, "HOLD")
        final_reason = final_reason.mask(missing_core, "DATA_MISSING")
        final_cap = final_cap.mask(missing_core, 0.0)
        vol_missing = out["vol_ratio"].isna()
        final_reason = final_reason.mask(
            vol_missing & ~missing_core, "VOL_RATIO_FALLBACK"
        )
        final_cap = final_cap.mask(vol_missing & ~missing_core, 0.2)
        vol_ratio_filled = pd.to_numeric(out.get("vol_ratio"), errors="coerce").fillna(1.0)

        rsi = pd.to_numeric(out.get("rsi14"), errors="coerce")
        bias = pd.to_numeric(out.get("ma20_bias"), errors="coerce")
        env_gate = out.get("env_final_gate_action")
        oversold = (rsi < 30) & (bias < -0.05)
        final_cap = final_cap.mask(oversold, 0.2)
        gate_tag = gate_tag.mask(oversold, "C_GO")
        if oversold.any():
            base_reason = final_reason.fillna("").astype(str)
            c_go_reason = np.where(
                base_reason.eq(""),
                "C_GO",
                base_reason + "|C_GO",
            )
            final_reason = final_reason.mask(oversold, pd.Series(c_go_reason, index=final_reason.index))

        if env_gate is not None:
            env_gate_upper = out["env_final_gate_action"].astype(str).str.upper()
            gate_stop = env_gate_upper == "STOP"
            gate_wait = env_gate_upper == "WAIT"
            entry_mask = ~final_action.isin(["SELL", "REDUCE"])
            final_cap = final_cap.mask(gate_stop | gate_wait, 0.0)
            final_action = final_action.mask(
                (gate_stop | gate_wait) & entry_mask,
                "WAIT",
            )
            final_reason = final_reason.mask((gate_stop | gate_wait) & entry_mask, "ENV_GATE")

        runup_pct = pd.to_numeric(out.get("runup_pct"), errors="coerce")
        runup_exit = runup_pct > 0.15
        final_action = final_action.mask(runup_exit, "REDUCE")
        final_reason = final_reason.mask(runup_exit, "RUNUP>15%")
        final_cap = final_cap.mask(runup_exit, 0.0)

        prev_hist = pd.to_numeric(out.get("prev_macd_hist"), errors="coerce")
        macd_hist = pd.to_numeric(out.get("macd_hist"), errors="coerce")
        hist_diff = macd_hist - prev_hist
        macd_shrink = (
            hist_diff.groupby(out["code"], sort=False)
            .rolling(3, min_periods=3)
            .apply(lambda x: (x < 0).all(), raw=True)
            .reset_index(level=0, drop=True)
        )
        out["macd_shrink"] = macd_shrink
        macd_shrink_series = out.get("macd_shrink")
        if isinstance(macd_shrink_series, pd.Series) and macd_shrink_series.dtype != bool:
            macd_shrink_flag = (macd_shrink_series < 0).fillna(False)
        elif isinstance(macd_shrink_series, pd.Series):
            macd_shrink_flag = macd_shrink_series.fillna(False)
        else:
            macd_shrink_flag = pd.Series(False, index=out.index, dtype=bool)

        hist_cross_down_flag = out.get("hist_cross_down", False)
        if isinstance(hist_cross_down_flag, pd.Series):
            hist_cross_down_flag = hist_cross_down_flag.fillna(False)
        else:
            hist_cross_down_flag = bool(hist_cross_down_flag)

        exit_flag = macd_shrink_flag | hist_cross_down_flag
        soft_stop = (out["close"] > out["ma20"]) & (vol_ratio_filled < 1.0) & (
            exit_flag
        )
        final_action = final_action.mask(soft_stop, "SELL")
        final_reason = final_reason.mask(soft_stop, "动能衰减/缩量")
        final_cap = final_cap.mask(soft_stop, 0.0)

        reduce_mask = (out["ma5"] < out["ma20"]) & (vol_ratio_filled < 1.0)
        final_action = final_action.mask(reduce_mask & ~soft_stop, "REDUCE")
        final_reason = final_reason.mask(reduce_mask & ~soft_stop, "弱势缩量减仓")
        final_cap = final_cap.mask(reduce_mask, 0.0)

        chip_score = pd.to_numeric(out.get("chip_score"), errors="coerce")
        chip_reason = out.get("chip_reason")
        chip_reason = chip_reason.replace("", pd.NA) if isinstance(chip_reason, pd.Series) else chip_reason
        chip_penalty = pd.to_numeric(out.get("chip_penalty"), errors="coerce").fillna(0.0)
        chip_age_days = pd.to_numeric(out.get("age_days"), errors="coerce")
        chip_stale = out.get("stale_hit")
        if isinstance(chip_stale, pd.Series):
            chip_stale_flag = chip_stale.fillna(False)
            if chip_stale_flag.dtype != bool:
                chip_stale_flag = chip_stale_flag.astype(bool)
        else:
            chip_stale_flag = pd.Series(bool(chip_stale), index=out.index)
        chip_reason_str = (
            chip_reason.astype("string")
            if isinstance(chip_reason, pd.Series)
            else pd.Series("", index=out.index, dtype="string")
        )
        chip_effective = (
            (~chip_stale_flag)
            & chip_age_days.notna()
            & (chip_age_days <= 45)
            & ~chip_reason_str.str.startswith("DATA_", na=False)
            & ~chip_reason_str.str.contains("OUTLIER", case=False, na=False)
        )
        raw_signal = out.get("raw_signal")
        raw_signal_upper = raw_signal.astype(str).str.upper() if isinstance(raw_signal, pd.Series) else None

        entry_mask = ~final_action.isin(["SELL", "REDUCE"])
        if raw_signal_upper is not None:
            sell_raw = raw_signal_upper == "SELL"
            final_action = final_action.mask(sell_raw, "SELL")
            final_cap = final_cap.mask(sell_raw, 0.0)

            buy_confirm_raw = raw_signal_upper == "BUY_CONFIRM"
            vol_guard = vol_ratio_filled < 1.0
            structure_weak = (out["ma5"] < out["ma20"]) | (out["macd_hist"] <= 0)
            degrade_wait = (
                buy_confirm_raw
                & entry_mask
                & chip_effective
                & (chip_score <= -0.5)
                & (structure_weak | vol_guard)
            )
            # 筹码/数据异常：避免把 BUY_CONFIRM “硬拦”成 WAIT（会直接从信号池消失）
            # 改为降级为 BUY + 小仓位，让信号还能进入候选池，但仓位明显受控
            final_action = final_action.mask(degrade_wait, "BUY")
            wait_reason = chip_reason.fillna("CHIP_WEAK") if isinstance(chip_reason, pd.Series) else "CHIP_WEAK"
            final_reason = final_reason.mask(degrade_wait, wait_reason)
            final_cap = final_cap.mask(degrade_wait, 0.2)

            weak_confirm = buy_confirm_raw & entry_mask & ~degrade_wait
            confirm_cap = 0.2
            final_cap = final_cap.mask(weak_confirm, np.minimum(final_cap, confirm_cap))

        hold_like = entry_mask & ~final_action.isin(["WAIT"])
        weak_chip = chip_effective & (chip_score <= -0.5)
        final_cap = final_cap.mask(hold_like & weak_chip, np.minimum(final_cap, 0.2))

        chip_missing = (
            chip_reason.astype("string").str.startswith("DATA_MISSING").fillna(False)
            if isinstance(chip_reason, pd.Series)
            else pd.Series(False, index=out.index)
        )
        if isinstance(chip_reason, pd.Series):
            missing_reason_fill = chip_reason
        else:
            missing_reason_fill = pd.Series(chip_reason, index=out.index)
        final_reason = final_reason.mask(chip_missing & final_reason.isna(), missing_reason_fill)

        entry_mask = ~final_action.isin(["SELL", "REDUCE"])
        final_cap = final_cap.mask(
            entry_mask & chip_effective, np.maximum(0.0, final_cap - chip_penalty)
        )
        vol_ratio = pd.to_numeric(out.get("vol_ratio"), errors="coerce")
        low_vol_entry = vol_ratio_filled < 1.5
        final_cap = final_cap.mask(entry_mask & low_vol_entry & (final_cap > 0), 0.3)

        exit_mask = final_action.isin(["SELL", "REDUCE"])
        final_cap = final_cap.mask(exit_mask, 0.0)

        out["final_action"] = final_action.fillna("HOLD")
        out["final_reason"] = final_reason.fillna("观望")
        out["final_cap"] = final_cap
        out["gate_tag"] = gate_tag.replace("", None)
        return out

    def _clear_table(self, table: str) -> None:
        try:
            with self.db_writer.engine.begin() as conn:
                conn.execute(text(f"TRUNCATE TABLE `{table}`"))
        except Exception:
            pass

    def _table_exists(self, table: str) -> bool:
        if not table:
            return False
        try:
            with self.db_writer.engine.begin() as conn:
                conn.execute(text(f"SELECT 1 FROM `{table}` LIMIT 1"))
            return True
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("检查表 %s 是否存在失败：%s", table, exc)
            return False

    def _write_indicator_daily(
        self,
        latest_date: dt.date,
        signals: pd.DataFrame,
        codes: List[str],
        *,
        scope_override: str | None = None,
    ) -> None:
        table = self.params.indicator_table
        scope = (
            (scope_override or getattr(self.params, "signals_write_scope", "latest"))
            or "latest"
        ).strip().lower()
        if scope not in {"latest", "window"}:
            self.logger.warning("signals_write_scope=%s 无效，已回退为 latest。", scope)
            scope = "latest"

        base = signals.copy()
        codes_clean = [str(c) for c in (codes or []) if str(c).strip()]
        if codes_clean:
            base = base[base["code"].astype(str).isin(codes_clean)].copy()
        if base.empty:
            self.logger.warning("指标写入为空（scope=%s），已跳过。", scope)
            return

        if scope == "latest":
            base = base[base["date"].dt.date == latest_date].copy()
        if base.empty:
            self.logger.warning("signals_write_scope=%s 下无任何指标行，已跳过写入。", scope)
            return

        keep_cols = [
            "date",
            "code",
            "close",
            "volume",
            "amount",
            "avg_volume_20",
            "ma5",
            "ma10",
            "ma20",
            "ma60",
            "ma250",
            "vol_ratio",
            "macd_dif",
            "macd_dea",
            "macd_hist",
            "prev_macd_hist",
            "kdj_k",
            "kdj_d",
            "kdj_j",
            "atr14",
            "rsi14",
            "ret_10",
            "ret_20",
            "limit_up_cnt_20",
            "ma20_bias",
            "yearline_state",
        ]
        indicator_df = base[keep_cols].copy()
        indicator_df = indicator_df.rename(columns={"date": "trade_date"})
        indicator_df["trade_date"] = pd.to_datetime(indicator_df["trade_date"]).dt.date
        indicator_df["code"] = indicator_df["code"].astype(str)
        # 防止 DataFrame 自身重复键（trade_date, code）导致一次写入就撞主键
        indicator_df = indicator_df.drop_duplicates(subset=["trade_date", "code"], keep="last").copy()

        if scope == "latest":
            delete_stmt = (
                text(f"DELETE FROM `{table}` WHERE `trade_date` = :d AND `code` IN :codes")
                .bindparams(bindparam("codes", expanding=True))
            )
            del_codes = indicator_df["code"].tolist()
            with self.db_writer.engine.begin() as conn:
                conn.execute(delete_stmt, {"d": latest_date, "codes": del_codes})
        else:
            start_d = min(indicator_df["trade_date"])
            end_d = max(indicator_df["trade_date"])
            delete_by_date_only = (not codes_clean) or (len(codes_clean) > 2000)
            with self.db_writer.engine.begin() as conn:
                if delete_by_date_only:
                    delete_stmt = text(
                        f"DELETE FROM `{table}` WHERE `trade_date` BETWEEN :start_date AND :end_date"
                    )
                    conn.execute(delete_stmt, {"start_date": start_d, "end_date": end_d})
                else:
                    delete_stmt = (
                        text(
                            f"""
                            DELETE FROM `{table}`
                            WHERE `trade_date` BETWEEN :start_date AND :end_date
                              AND `code` IN :codes
                            """
                        )
                        .bindparams(bindparam("codes", expanding=True))
                    )
                    chunk_size = 800
                    for i in range(0, len(codes_clean), chunk_size):
                        part_codes = codes_clean[i : i + chunk_size]
                        conn.execute(
                            delete_stmt,
                            {"start_date": start_d, "end_date": end_d, "codes": part_codes},
                        )

            self.logger.info(
                "signals_write_scope=window：指标已覆盖写入 %s~%s（%s）。",
                start_d,
                end_d,
                "all-codes" if delete_by_date_only else f"{len(codes_clean)} codes",
            )

        self.db_writer.write_dataframe(indicator_df, table, if_exists="append")

    def _write_signal_events(
        self,
        latest_date: dt.date,
        signals: pd.DataFrame,
        codes: List[str],
        *,
        scope_override: str | None = None,
    ) -> None:
        table = self.params.signal_events_table
        scope = (
            (scope_override or getattr(self.params, "signals_write_scope", "latest"))
            or "latest"
        ).strip().lower()
        if scope not in {"latest", "window"}:
            self.logger.warning("signals_write_scope=%s 无效，已回退为 latest。", scope)
            scope = "latest"

        base = signals.copy()
        codes_clean = [str(c) for c in (codes or []) if str(c).strip()]
        if codes_clean:
            base = base[base["code"].astype(str).isin(codes_clean)].copy()
        if base.empty:
            self.logger.warning("信号事件写入为空（scope=%s），已跳过。", scope)
            return

        if scope == "latest":
            base = base[base["date"].dt.date == latest_date].copy()
        if base.empty:
            self.logger.warning("signals_write_scope=%s 下无任何信号事件，已跳过写入。", scope)
            return

        keep_cols = [
            "date",
            "code",
            "signal",
            "final_action",
            "final_reason",
            "final_cap",
            "reason",
            "risk_tag",
            "risk_note",
            "stop_ref",
            "macd_event",
            "chip_score",
            "gdhs_delta_pct",
            "gdhs_announce_date",
            "chip_reason",
            "chip_penalty",
            "chip_note",
            "age_days",
            "deadzone_hit",
            "stale_hit",
            "fear_score",
            "wave_type",
            "extra_json",
        ]
        events_df = base[keep_cols].copy()
        events_df = events_df.rename(columns={"date": "sig_date"})
        events_df["sig_date"] = pd.to_datetime(events_df["sig_date"]).dt.date
        events_df["code"] = events_df["code"].astype(str)
        events_df["strategy_code"] = STRATEGY_CODE_MA5_MA20_TREND
        valid_days = None
        try:
            valid_days = int(getattr(self.params, "valid_days", 3))
            events_df["valid_days"] = valid_days
        except Exception:
            self.logger.warning(
                "valid_days=%s 解析失败，将跳过有效期字段写入。",
                getattr(self.params, "valid_days", None),
            )
            events_df["valid_days"] = pd.NA
            valid_days = None

        expires_cache: dict[dt.date, dt.date | None] = {}

        def _resolve_expires_on(sig_date: dt.date) -> dt.date | None:
            if sig_date in expires_cache:
                return expires_cache[sig_date]
            if valid_days is None:
                expires_cache[sig_date] = None
                return None
            if valid_days <= 0:
                expires_cache[sig_date] = sig_date
                return sig_date
            end_date = sig_date + dt.timedelta(days=max(valid_days * 3, 10))
            calendar = load_trading_calendar(sig_date, end_date)
            if calendar:
                trading_dates = []
                for raw in calendar:
                    if not isinstance(raw, str):
                        continue
                    try:
                        trade_date = dt.datetime.strptime(raw, "%Y-%m-%d").date()
                    except Exception:
                        continue
                    if sig_date <= trade_date <= end_date:
                        trading_dates.append(trade_date)
                trading_dates.sort()
                if sig_date not in trading_dates:
                    trading_dates.insert(0, sig_date)
                if len(trading_dates) > valid_days:
                    expires_cache[sig_date] = trading_dates[valid_days]
                    return trading_dates[valid_days]
            expires_cache[sig_date] = sig_date + dt.timedelta(days=valid_days)
            return expires_cache[sig_date]

        events_df["expires_on"] = events_df["sig_date"].apply(
            lambda d: _resolve_expires_on(d) if pd.notna(d) else pd.NA
        )

        # 防止同一批信号里存在重复键，导致唯一约束/主键冲突（strategy_code, sig_date, code）
        events_df = events_df.drop_duplicates(subset=["strategy_code", "sig_date", "code"], keep="last").copy()
        for col in ["risk_tag", "risk_note", "reason", "chip_reason"]:
            if col in events_df.columns:
                events_df[col] = (
                    events_df[col]
                    .fillna("")
                    .astype(str)
                    .str.slice(0, 250)
                )
        for col in ["final_reason", "macd_event", "wave_type"]:
            if col in events_df.columns:
                events_df[col] = (
                    events_df[col]
                    .fillna("")
                    .astype(str)
                    .str.slice(0, 255)
                )
        if "chip_note" in events_df.columns:
            events_df["chip_note"] = (
                events_df["chip_note"]
                .fillna("")
                .astype(str)
                .str.slice(0, 255)
                .replace("", pd.NA)
            )
        events_df["gdhs_announce_date"] = pd.to_datetime(
            events_df.get("gdhs_announce_date"), errors="coerce"
        ).dt.date
        events_df["extra_json"] = None
        if "extra_json" in base.columns:
            events_df["extra_json"] = base["extra_json"].fillna("").astype(str)
        if "chip_score" in events_df.columns:
            events_df["chip_score"] = pd.to_numeric(events_df["chip_score"], errors="coerce")
        if "chip_penalty" in events_df.columns:
            events_df["chip_penalty"] = pd.to_numeric(events_df["chip_penalty"], errors="coerce")
        if "age_days" in events_df.columns:
            events_df["age_days"] = pd.to_numeric(events_df["age_days"], errors="coerce").astype("Int64")
        for flag_col in ["deadzone_hit", "stale_hit"]:
            if flag_col in events_df.columns:
                events_df[flag_col] = events_df[flag_col].astype(bool)

        if scope == "latest":
            delete_stmt = (
                text(
                    f"""
                    DELETE FROM `{table}`
                    WHERE `sig_date` = :d AND `code` IN :codes AND `strategy_code` = :strategy
                    """
                ).bindparams(bindparam("codes", expanding=True))
            )
            del_codes = events_df["code"].tolist()
            with self.db_writer.engine.begin() as conn:
                conn.execute(
                    delete_stmt,
                    {
                        "d": latest_date,
                        "codes": del_codes,
                        "strategy": STRATEGY_CODE_MA5_MA20_TREND,
                    },
                )
        else:
            start_d = min(events_df["sig_date"])
            end_d = max(events_df["sig_date"])
            delete_by_date_only = (not codes_clean) or (len(codes_clean) > 2000)
            with self.db_writer.engine.begin() as conn:
                if delete_by_date_only:
                    delete_stmt = text(
                        f"""
                        DELETE FROM `{table}`
                        WHERE `sig_date` BETWEEN :start_date AND :end_date
                          AND `strategy_code` = :strategy
                        """
                    )
                    conn.execute(
                        delete_stmt,
                        {"start_date": start_d, "end_date": end_d, "strategy": STRATEGY_CODE_MA5_MA20_TREND},
                    )
                else:
                    delete_stmt = (
                        text(
                            f"""
                            DELETE FROM `{table}`
                            WHERE `sig_date` BETWEEN :start_date AND :end_date
                              AND `strategy_code` = :strategy
                              AND `code` IN :codes
                            """
                        ).bindparams(bindparam("codes", expanding=True))
                    )
                    chunk_size = 800
                    for i in range(0, len(codes_clean), chunk_size):
                        part_codes = codes_clean[i : i + chunk_size]
                        conn.execute(
                            delete_stmt,
                            {
                                "start_date": start_d,
                                "end_date": end_d,
                                "codes": part_codes,
                                "strategy": STRATEGY_CODE_MA5_MA20_TREND,
                            },
                        )
            self.logger.info(
                "signals_write_scope=window：事件已覆盖写入 %s~%s（%s）。",
                start_d,
                end_d,
                "all-codes" if delete_by_date_only else f"{len(codes_clean)} codes",
            )

        self.db_writer.write_dataframe(events_df, table, if_exists="append")

    def _precompute_chip_filter(self, signals: pd.DataFrame) -> None:
        """策略运行结束后触发筹码预计算，盘前准备 ready signals。"""

        if signals.empty:
            return

        chip_inputs = [
            "date",
            "sig_date",
            "code",
            "vol_ratio",
            "close",
            "ma20",
            "runup_pct",
            "fear_score",
            "pct_chg",
            "pct_change",
            "change_pct",
            "ret_1d",
            "ret",
        ]
        sig_for_chip = signals[[c for c in chip_inputs if c in signals.columns]].copy()
        if "sig_date" not in sig_for_chip.columns and "date" in sig_for_chip.columns:
            sig_for_chip = sig_for_chip.rename(columns={"date": "sig_date"})
        if "sig_date" not in sig_for_chip.columns:
            return
        sig_for_chip["sig_date"] = pd.to_datetime(sig_for_chip["sig_date"], errors="coerce")
        if "date" not in sig_for_chip.columns:
            sig_for_chip["date"] = sig_for_chip["sig_date"]
        sig_for_chip = sig_for_chip.dropna(subset=["sig_date", "code"])
        if sig_for_chip.empty:
            return

        try:
            ChipFilter().apply(sig_for_chip)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("筹码预计算失败（已跳过，不影响信号写入）：%s", exc)

    def _refresh_ready_signals_table(self, sig_dates: List[dt.date]) -> None:
        if not sig_dates:
            return

        schema_manager = SchemaManager(self.db_writer.engine)
        table_names = schema_manager.get_table_names()
        ready_table = table_names.ready_signals_view
        if not ready_table or not self._table_exists(ready_table):
            self.logger.warning("ready_signals 表 %s 不存在，跳过刷新。", ready_table)
            return

        select_sql, column_order = schema_manager.build_ready_signals_select(
            self.params.signal_events_table,
            self.params.indicator_table,
            TABLE_STRATEGY_CHIP_FILTER,
        )
        if not select_sql or not column_order:
            self.logger.warning("ready_signals 物化 SQL 未生成，跳过刷新。")
            return

        sig_dates_clean = sorted({d for d in sig_dates if isinstance(d, dt.date)})
        if not sig_dates_clean:
            return

        filtered_select = f"""{select_sql}
              AND e.`strategy_code` = :strategy_code
              AND e.`sig_date` IN :sig_dates
            """
        columns_clause = ", ".join(f"`{col}`" for col in column_order)
        delete_stmt = (
            text(
                f"""
                DELETE FROM `{ready_table}`
                WHERE `strategy_code` = :strategy_code AND `sig_date` IN :sig_dates
                """
            ).bindparams(bindparam("sig_dates", expanding=True))
        )
        insert_stmt = text(
            f"""
            INSERT INTO `{ready_table}` ({columns_clause})
            {filtered_select}
            """
        ).bindparams(bindparam("sig_dates", expanding=True))

        payload = {
            "strategy_code": STRATEGY_CODE_MA5_MA20_TREND,
            "sig_dates": sig_dates_clean,
        }
        with self.db_writer.engine.begin() as conn:
            conn.execute(delete_stmt, payload)
            conn.execute(insert_stmt, payload)
        self.logger.info(
            "ready_signals 表 %s 已刷新（sig_dates=%s）。",
            ready_table,
            ",".join([d.isoformat() for d in sig_dates_clean]),
        )

    def run(self, *, force: bool = False) -> None:
        """执行 MA5-MA20 策略。

        - 默认遵循 config.yaml: strategy_ma5_ma20_trend.enabled。
        - 当 force=True 时，即便 enabled=false 也会执行（用于单独运行脚本）。
        """

        if (not force) and (not self.params.enabled):
            self.logger.info("strategy_ma5_ma20_trend.enabled=false，已跳过 MA5-MA20 策略运行。")
            return

        if force and (not self.params.enabled):
            self.logger.info("strategy_ma5_ma20_trend.enabled=false，但 force=True，仍将执行 MA5-MA20 策略。")

        self.logger.debug(
            "MA5-MA20 参数：lookback_days=%s indicator_window=%s",
            self.params.lookback_days,
            self.indicator_window,
        )

        daily_tbl = self._daily_table_name()
        latest_date = self._get_latest_trade_date()
        self.candidates_service.refresh(latest_date)

        with self.db_writer.engine.begin() as conn:
            candidates_df = pd.read_sql_query(
                text(
                    f"""
                    SELECT `code`, `is_liquidity`, `has_signal`
                    FROM `{TABLE_STRATEGY_CANDIDATES}`
                    WHERE `asof_trade_date` = :d
                    """
                ),
                conn,
                params={"d": latest_date},
            )

        if candidates_df.empty:
            self.logger.warning(
                "strategy_candidates=%s 为空，已跳过 MA5-MA20 策略运行。",
                latest_date,
            )
            return

        candidates_df["code"] = candidates_df["code"].astype(str)
        candidate_codes = candidates_df["code"].dropna().unique().tolist()
        liquidity_codes = (
            candidates_df.loc[candidates_df["is_liquidity"] == 1, "code"]
            .dropna()
            .unique()
            .tolist()
        )
        signal_codes = (
            candidates_df.loc[candidates_df["has_signal"] == 1, "code"]
            .dropna()
            .unique()
            .tolist()
        )
        candidate_set = set(candidate_codes)
        liquidity_set = set(liquidity_codes)
        signal_set = set(signal_codes)
        both_set = liquidity_set & signal_set
        snapshot_only_set = candidate_set - liquidity_set
        self.logger.info(
            "MA5-MA20 策略：日线表=%s candidates_total=%s liquidity=%s signal=%s both=%s snapshot_only=%s",
            daily_tbl,
            len(candidate_set),
            len(liquidity_set),
            len(signal_set),
            len(both_set),
            len(snapshot_only_set),
        )

        daily = self._load_daily_kline(candidate_codes, latest_date)
        self.logger.info("MA5-MA20 策略：读取日线 %s 行（%s 只股票）。", len(daily), daily["code"].nunique())

        ind = self._compute_indicators(daily)
        fundamentals = (
            self._load_fundamentals_latest()
            if self._fundamentals_cache is None
            else self._fundamentals_cache
        )
        stock_basic = (
            self._load_stock_basic()
            if self._stock_basic_cache is None
            else self._stock_basic_cache
        )
        self._fundamentals_cache = fundamentals
        self._stock_basic_cache = stock_basic
        sig = self._generate_signals(ind, fundamentals, stock_basic)
        sig = sig.dropna(subset=["date", "code"])
        sig["date"] = pd.to_datetime(sig["date"], errors="coerce")
        sig["code"] = sig["code"].astype(str)
        sig = (
            sig.sort_values(["code", "date"])
            .drop_duplicates(subset=["code", "date"], keep="last")
            .reset_index(drop=True)
        )
        sig["code"] = sig["code"].astype(str)
        sig_for_write = sig.copy()
        snapshot_only_codes = sorted(snapshot_only_set)
        liquidity_codes = sorted(liquidity_set)
        if snapshot_only_codes:
            snapshot_mask = sig_for_write["code"].isin(snapshot_only_codes)
            latest_mask = sig_for_write["date"].dt.date == latest_date
            sig_for_write.loc[snapshot_mask, "signal"] = "SNAPSHOT"
            sig_for_write.loc[snapshot_mask, "final_action"] = "SNAPSHOT"
            sig_for_write.loc[snapshot_mask, "final_reason"] = "SNAPSHOT_ONLY"
            sig_for_write.loc[snapshot_mask, "reason"] = "SNAPSHOT_ONLY"
            sig_for_write = pd.concat(
                [sig_for_write[~snapshot_mask], sig_for_write[snapshot_mask & latest_mask]],
                ignore_index=True,
            )

        self._write_indicator_daily(latest_date, sig, liquidity_codes)
        if snapshot_only_codes:
            self._write_indicator_daily(
                latest_date,
                sig,
                snapshot_only_codes,
                scope_override="latest",
            )

        self._write_signal_events(latest_date, sig_for_write, liquidity_codes)
        if snapshot_only_codes:
            self._write_signal_events(
                latest_date,
                sig_for_write,
                snapshot_only_codes,
                scope_override="latest",
            )

        self._precompute_chip_filter(sig_for_write)
        sig_dates = sig_for_write["date"].dropna().dt.date.unique().tolist()
        self._refresh_ready_signals_table(sig_dates)

        latest_sig = sig[sig["date"].dt.date == latest_date]
        dup_count = int(latest_sig.duplicated(subset=["code", "date"]).sum())
        self.logger.info(
            "MA5-MA20 策略自检：latest_sig 行数=%s，唯一 code 数=%s，重复(code,date)=%s",
            len(latest_sig),
            latest_sig["code"].nunique(),
            dup_count,
        )
        action_col = "final_action" if "final_action" in latest_sig.columns else "signal"
        action_series = latest_sig[action_col].fillna("HOLD").astype(str)
        counts = action_series.value_counts(dropna=False)
        order = ["BUY", "BUY_CONFIRM", "SELL", "REDUCE", "HOLD", "WAIT"]
        ordered_counts = {k: int(counts.get(k, 0)) for k in order}
        other_count = int(counts.drop(labels=order, errors="ignore").sum())
        self.logger.info(
            "MA5-MA20 策略完成：最终动作(final_action)统计（最新交易日：%s）："
            "BUY=%s, BUY_CONFIRM=%s, SELL=%s, REDUCE=%s, HOLD=%s, WAIT=%s, OTHER=%s",
            latest_date,
            ordered_counts["BUY"],
            ordered_counts["BUY_CONFIRM"],
            ordered_counts["SELL"],
            ordered_counts["REDUCE"],
            ordered_counts["HOLD"],
            ordered_counts["WAIT"],
            other_count,
        )

================================================================================
FILE: ashare/market_indicator_builder.py
================================================================================

from __future__ import annotations

import datetime as dt
import json
import logging
from typing import Any, Dict, List

import pandas as pd
from sqlalchemy import bindparam, text

from .indicator_utils import consecutive_true
from .ma5_ma20_trend_strategy import _atr, _macd
from .market_regime import MarketRegimeClassifier
from .weekly_env_builder import WeeklyEnvironmentBuilder


class MarketIndicatorBuilder:
    """负责日线/周线指标的构建。"""

    def __init__(
        self,
        *,
        env_builder: WeeklyEnvironmentBuilder,
        logger: logging.Logger,
    ) -> None:
        self.env_builder = env_builder
        self.logger = logger
        self.market_regime = MarketRegimeClassifier()
        self.regime_confirm_days = 2

    @property
    def index_codes(self) -> list[str]:
        return self.env_builder.index_codes

    def compute_weekly_indicator(
        self,
        asof_trade_date: str,
        *,
        checked_at: dt.datetime | None = None,
    ) -> list[dict[str, Any]]:
        env_context = self.env_builder.build_environment_context(
            asof_trade_date, checked_at=checked_at
        )
        weekly_scenario = env_context.get("weekly_scenario") if isinstance(env_context, dict) else {}
        if not isinstance(weekly_scenario, dict):
            weekly_scenario = {}

        weekly_asof = weekly_scenario.get("weekly_asof_trade_date") or asof_trade_date
        weekly_gate_policy = env_context.get("weekly_gate_policy")
        weekly_zone_id = env_context.get("weekly_zone_id")
        weekly_zone_score = env_context.get("weekly_zone_score")
        weekly_exp_return_bucket = env_context.get("weekly_exp_return_bucket")
        weekly_zone_reason = env_context.get("weekly_zone_reason")
        weekly_money_proxy = env_context.get("weekly_money_proxy")
        weekly_tags = env_context.get("weekly_tags")
        weekly_note = env_context.get("weekly_note")
        weekly_plan_json = weekly_scenario.get("weekly_plan_json")

        benchmark_code = self.env_builder.benchmark_code
        return [
            {
                "weekly_asof_trade_date": weekly_asof,
                "benchmark_code": benchmark_code,
                "weekly_scene_code": weekly_scenario.get("weekly_scene_code"),
                "weekly_phase": weekly_scenario.get("weekly_phase"),
                "weekly_structure_status": weekly_scenario.get("weekly_structure_status"),
                "weekly_pattern_status": weekly_scenario.get("weekly_pattern_status"),
                "weekly_risk_score": weekly_scenario.get("weekly_risk_score"),
                "weekly_risk_level": weekly_scenario.get("weekly_risk_level"),
                "weekly_gate_policy": weekly_gate_policy,
                "weekly_plan_a_exposure_cap": weekly_scenario.get("weekly_plan_a_exposure_cap"),
                "weekly_key_levels_str": weekly_scenario.get("weekly_key_levels_str"),
                "weekly_zone_id": weekly_zone_id,
                "weekly_zone_score": weekly_zone_score,
                "weekly_exp_return_bucket": weekly_exp_return_bucket,
                "weekly_zone_reason": weekly_zone_reason,
                "weekly_money_proxy": weekly_money_proxy,
                "weekly_tags": weekly_tags,
                "weekly_note": weekly_note,
                "weekly_plan_json": weekly_plan_json,
            }
        ]

    def resolve_weekly_asof_dates(
        self, start_date: dt.date, end_date: dt.date
    ) -> list[dt.date]:
        primary_code = (self.index_codes or ["sh.000001"])[0]
        stmt = text(
            """
            SELECT CAST(`date` AS CHAR) AS trade_date
            FROM history_index_daily_kline
            WHERE `code` = :code AND `date` BETWEEN :start_date AND :end_date
            ORDER BY `date`
            """
        )
        try:
            with self.env_builder.db_writer.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={
                        "code": primary_code,
                        "start_date": start_date.isoformat(),
                        "end_date": end_date.isoformat(),
                    },
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("读取周线交易日失败：%s", exc)
            return []

        if df.empty or "trade_date" not in df.columns:
            return []

        dates = pd.to_datetime(df["trade_date"], errors="coerce").dropna()
        if dates.empty:
            return []

        weekly_end = dates.groupby(dates.dt.to_period("W-FRI")).max()
        return weekly_end.dt.date.dropna().tolist()

    def compute_daily_indicators(
        self, start_date: dt.date, end_date: dt.date
    ) -> list[dict[str, Any]]:
        index_codes = [c for c in self.index_codes if c]
        benchmark_code = self.env_builder.benchmark_code or "sh.000001"
        if benchmark_code and benchmark_code not in index_codes:
            index_codes.append(benchmark_code)
        if not index_codes:
            return []

        lookback_start = start_date - dt.timedelta(days=400)
        stmt = (
            text(
                """
                SELECT `code`,`date`,`open`,`high`,`low`,`close`,`volume`,`amount`
                FROM history_index_daily_kline
                WHERE `code` IN :codes AND `date` BETWEEN :start_date AND :end_date
                ORDER BY `code`, `date`
                """
            ).bindparams(bindparam("codes", expanding=True))
        )
        try:
            with self.env_builder.db_writer.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={
                        "codes": index_codes,
                        "start_date": lookback_start.isoformat(),
                        "end_date": end_date.isoformat(),
                    },
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("读取指数日线指标失败：%s", exc)
            return []

        if df.empty:
            return []

        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.dropna(subset=["date"])
        if df.empty:
            return []

        for col in ["open", "high", "low", "close", "volume", "amount"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        results: list[pd.DataFrame] = []
        for code, group in df.groupby("code", sort=False):
            grp = group.sort_values("date").copy()
            close = grp["close"]
            grp["ma20"] = close.rolling(20, min_periods=20).mean()
            grp["ma60"] = close.rolling(60, min_periods=60).mean()
            grp["ma250"] = close.rolling(250, min_periods=250).mean()
            std20 = close.rolling(20, min_periods=20).std()
            grp["bb_mid"] = grp["ma20"]
            grp["bb_upper"] = grp["bb_mid"] + 2 * std20
            grp["bb_lower"] = grp["bb_mid"] - 2 * std20
            bb_range = grp["bb_upper"] - grp["bb_lower"]
            grp["bb_width"] = bb_range / grp["bb_mid"]
            grp["bb_pos"] = (close - grp["bb_lower"]) / bb_range
            zero_range = bb_range == 0
            if zero_range.any():
                grp.loc[zero_range, "bb_pos"] = None
            zero_mid = grp["bb_mid"] == 0
            if zero_mid.any():
                grp.loc[zero_mid, "bb_width"] = None
            dif, dea, hist = _macd(close)
            grp["macd_hist"] = hist
            preclose = close.shift(1)
            grp["atr14"] = _atr(grp["high"], grp["low"], preclose)
            grp["dev_ma20_atr"] = (close - grp["ma20"]) / grp["atr14"]

            rolling_low = close.rolling(
                self.market_regime.breakdown_window, min_periods=1
            ).min().shift(1)
            grp["rolling_low"] = rolling_low
            ma250_valid = grp["ma250"].notna()
            below_ma250 = (close < grp["ma250"]) & ma250_valid
            prev_below = below_ma250.shift(1, fill_value=False)
            above_ma250 = (close >= grp["ma250"]) & ma250_valid

            grp["below_ma250_streak"] = consecutive_true(below_ma250)
            grp["reclaim_streak"] = consecutive_true(above_ma250)
            grp["break_confirmed"] = (
                grp["below_ma250_streak"] >= self.market_regime.effective_breakdown_days
            )
            grp["reclaim_confirmed"] = (
                (grp["reclaim_streak"] >= self.market_regime.effective_reclaim_days)
                & prev_below
            )

            daily_ret = close.pct_change()
            grp["pullback_mode"] = None
            grp.loc[
                daily_ret <= -0.03, "pullback_mode"
            ] = "FAST_DROP"

            status = pd.Series("RISK_ON", index=grp.index, dtype="object")
            status = status.mask(close.isna(), "UNKNOWN")
            status = status.mask(grp["ma60"].isna() | grp["ma250"].isna(), "UNKNOWN")

            risk_off_threshold = -0.5
            risk_off_condition = (
                (close < grp["ma60"])
                & (grp["macd_hist"] < 0)
                & (grp["dev_ma20_atr"] < risk_off_threshold)
            )
            grp["risk_off_flag"] = risk_off_condition

            status = status.mask(
                (status != "UNKNOWN") & grp["break_confirmed"], "BEAR_CONFIRMED"
            )
            status = status.mask(
                (status != "UNKNOWN")
                & (grp["rolling_low"].notna())
                & (close < grp["rolling_low"]),
                "BREAKDOWN",
            )
            status = status.mask(
                (status == "RISK_ON") & risk_off_condition,
                "RISK_OFF",
            )
            pullback_mask = (grp["ma60"].notna()) & (close < grp["ma60"])
            status = status.mask((status == "RISK_ON") & pullback_mask, "PULLBACK")
            pullback_mask = (grp["ma20"].notna()) & (close < grp["ma20"])
            status = status.mask((status == "RISK_ON") & pullback_mask, "PULLBACK")
            grp["status"] = status

            score = (
                (close > grp["ma20"]).astype(int)
                + (close > grp["ma60"]).astype(int)
                + (close > grp["ma250"]).astype(int)
            )
            grp["score_raw"] = score
            grp["code"] = str(code)
            results.append(grp)

        merged = pd.concat(results, ignore_index=True) if results else pd.DataFrame()
        if merged.empty:
            return []

        merged["trade_date"] = merged["date"].dt.date
        merged["above_ma20"] = (merged["close"] > merged["ma20"]) & merged["ma20"].notna()
        merged["above_ma60"] = (merged["close"] > merged["ma60"]) & merged["ma60"].notna()
        day_summary = (
            merged.groupby("trade_date")[["status", "score_raw", "pullback_mode"]]
            .apply(self._resolve_daily_regime)
            .reset_index()
        )
        day_summary = day_summary.sort_values("trade_date").reset_index(drop=True)
        if "regime" in day_summary.columns:
            day_summary["regime_raw"] = day_summary["regime"]
            day_summary = self._apply_regime_hysteresis(
                day_summary, confirm_days=self.regime_confirm_days
            )
        if "position_hint" in day_summary.columns:
            day_summary["position_hint_raw"] = day_summary["position_hint"]
            pos = pd.to_numeric(day_summary["position_hint"], errors="coerce")
            day_summary["position_hint"] = pos.ewm(alpha=0.7, adjust=False).mean()

        breadth_summary = (
            merged.groupby("trade_date")
            .apply(self._resolve_breadth_metrics, include_groups=False)
            .reset_index()
        )
        merged = merged.merge(day_summary, on="trade_date", how="left")

        merged = merged[
            (merged["trade_date"] >= start_date) & (merged["trade_date"] <= end_date)
        ]
        day_summary = day_summary[
            (day_summary["trade_date"] >= start_date)
            & (day_summary["trade_date"] <= end_date)
        ]
        if merged.empty:
            return []

        benchmark_daily = merged.loc[merged["code"] == benchmark_code, :].copy()
        benchmark_daily = benchmark_daily[
            [
                "trade_date",
                "ma20",
                "ma60",
                "ma250",
                "macd_hist",
                "atr14",
                "dev_ma20_atr",
                "bb_mid",
                "bb_upper",
                "bb_lower",
                "bb_width",
                "bb_pos",
            ]
        ]

        daily_env = day_summary.merge(benchmark_daily, on="trade_date", how="left")
        daily_env = daily_env.merge(breadth_summary, on="trade_date", how="left")

        component_fields = [
            "code",
            "ma20",
            "ma60",
            "ma250",
            "macd_hist",
            "atr14",
            "dev_ma20_atr",
            "score_raw",
            "status",
        ]

        def _safe_value(value: Any) -> Any:
            try:
                if pd.isna(value):
                    return None
            except Exception:
                pass
            if isinstance(value, (dt.datetime, dt.date)):
                return value.isoformat()
            return value

        components_map: dict[dt.date, str] = {}
        for trade_date, group in merged.groupby("trade_date", sort=False):
            components = [
                {field: _safe_value(row.get(field)) for field in component_fields}
                for _, row in group.iterrows()
            ]
            components_map[trade_date] = json.dumps(
                components, ensure_ascii=False, separators=(",", ":")
            )

        daily_env["components_json"] = daily_env["trade_date"].map(components_map)

        invalid_ma_mask = daily_env["ma60"].isna() | daily_env["ma250"].isna()
        if invalid_ma_mask.any():
            daily_env.loc[invalid_ma_mask, "regime"] = "UNKNOWN"
            daily_env.loc[invalid_ma_mask, "position_hint"] = None
            daily_env.loc[invalid_ma_mask, "score"] = None

        zone_payloads = [self._resolve_daily_zone(row) for _, row in daily_env.iterrows()]
        zone_df = pd.DataFrame(zone_payloads)
        daily_env = pd.concat([daily_env.reset_index(drop=True), zone_df], axis=1)

        rows: list[dict[str, Any]] = []
        for _, row in daily_env.iterrows():
            rows.append(
                {
                    "asof_trade_date": row.get("trade_date"),
                    "benchmark_code": benchmark_code,
                    "regime": row.get("regime"),
                    "score": row.get("score"),
                    "position_hint": row.get("position_hint"),
                    "ma20": row.get("ma20"),
                    "ma60": row.get("ma60"),
                    "ma250": row.get("ma250"),
                    "macd_hist": row.get("macd_hist"),
                    "atr14": row.get("atr14"),
                    "dev_ma20_atr": row.get("dev_ma20_atr"),
                    "bb_mid": row.get("bb_mid"),
                    "bb_upper": row.get("bb_upper"),
                    "bb_lower": row.get("bb_lower"),
                    "bb_width": row.get("bb_width"),
                    "bb_pos": row.get("bb_pos"),
                    "cycle_phase": None,
                    "breadth_pct_above_ma20": row.get("breadth_pct_above_ma20"),
                    "breadth_pct_above_ma60": row.get("breadth_pct_above_ma60"),
                    "breadth_risk_off_ratio": row.get("breadth_risk_off_ratio"),
                    "dispersion_score": row.get("dispersion_score"),
                    "daily_zone_id": row.get("daily_zone_id"),
                    "daily_zone_score": row.get("daily_zone_score"),
                    "daily_cap_multiplier": row.get("daily_cap_multiplier"),
                    "daily_zone_reason": row.get("daily_zone_reason"),
                    "components_json": row.get("components_json"),
                }
            )
        return rows

    @staticmethod
    def _resolve_daily_regime(group: pd.DataFrame) -> pd.Series:
        statuses = group["status"].dropna().astype(str).tolist()
        score = group["score_raw"].mean() if not group.empty else None
        pullback_fast = (
            group["pullback_mode"].fillna("").astype(str).eq("FAST_DROP").any()
        )

        regime = "RISK_ON"
        if statuses and all(val == "UNKNOWN" for val in statuses):
            regime = "UNKNOWN"
            score = None
        elif "BEAR_CONFIRMED" in statuses:
            regime = "BEAR_CONFIRMED"
        elif "BREAKDOWN" in statuses:
            regime = "BREAKDOWN"
        elif "RISK_OFF" in statuses:
            regime = "RISK_OFF"
        elif "PULLBACK" in statuses:
            regime = "PULLBACK"

        position_hint_map = {
            "RISK_ON": 0.8,
            "PULLBACK": 0.4,
            "RISK_OFF": 0.1,
            "BREAKDOWN": 0.0,
            "BEAR_CONFIRMED": 0.0,
            "UNKNOWN": None,
        }
        position_hint = position_hint_map.get(regime)
        if regime == "PULLBACK" and pullback_fast and position_hint is not None:
            position_hint = min(position_hint, 0.3)

        return pd.Series(
            {
                "score": score,
                "regime": regime,
                "position_hint": position_hint,
                "pullback_fast": pullback_fast,
            }
        )

    @staticmethod
    def _apply_regime_hysteresis(
        day_summary: pd.DataFrame, *, confirm_days: int
    ) -> pd.DataFrame:
        if day_summary.empty or "regime" not in day_summary.columns:
            return day_summary

        confirm_days = max(int(confirm_days), 1)
        severe = {"BREAKDOWN", "BEAR_CONFIRMED"}
        position_hint_map = {
            "RISK_ON": 0.8,
            "PULLBACK": 0.4,
            "RISK_OFF": 0.1,
            "BREAKDOWN": 0.0,
            "BEAR_CONFIRMED": 0.0,
            "UNKNOWN": None,
        }

        current = None
        pending = None
        pending_count = 0

        smoothed = []
        for _, row in day_summary.iterrows():
            raw = str(row.get("regime") or "").upper()
            if raw == "UNKNOWN":
                smoothed.append(current or "UNKNOWN")
                continue

            if current is None:
                current = raw
                pending = None
                pending_count = 0
                smoothed.append(current)
                continue

            if raw == current:
                pending = None
                pending_count = 0
                smoothed.append(current)
                continue

            if raw in severe:
                current = raw
                pending = None
                pending_count = 0
                smoothed.append(current)
                continue

            if pending != raw:
                pending = raw
                pending_count = 1
            else:
                pending_count += 1

            if pending_count >= confirm_days:
                current = pending
                pending = None
                pending_count = 0

            smoothed.append(current)

        day_summary = day_summary.copy()
        day_summary["regime"] = smoothed

        def _recalc_hint(row: pd.Series) -> float | None:
            regime = str(row.get("regime") or "").upper()
            hint = position_hint_map.get(regime)
            if regime == "PULLBACK" and bool(row.get("pullback_fast")) and hint is not None:
                hint = min(hint, 0.3)
            return hint

        day_summary["position_hint"] = day_summary.apply(_recalc_hint, axis=1)
        return day_summary

    @staticmethod
    def _resolve_breadth_metrics(group: pd.DataFrame) -> pd.Series:
        valid_ma20 = group["ma20"].notna()
        valid_ma60 = group["ma60"].notna()

        breadth_ma20 = (
            float(group.loc[valid_ma20, "above_ma20"].sum()) / float(valid_ma20.sum())
            if valid_ma20.any()
            else None
        )
        breadth_ma60 = (
            float(group.loc[valid_ma60, "above_ma60"].sum()) / float(valid_ma60.sum())
            if valid_ma60.any()
            else None
        )

        risk_off_mask = (
            group["risk_off_flag"].fillna(False)
            & group["ma60"].notna()
            & group["macd_hist"].notna()
            & group["dev_ma20_atr"].notna()
        )
        denom = (
            group["ma60"].notna()
            & group["macd_hist"].notna()
            & group["dev_ma20_atr"].notna()
        )
        breadth_risk_off = (
            float(risk_off_mask.sum()) / float(denom.sum()) if denom.any() else None
        )

        dispersion = None
        if group["dev_ma20_atr"].notna().sum() >= 2:
            try:
                val = float(group["dev_ma20_atr"].std())
            except Exception:
                val = None
            if val is not None and not pd.isna(val):
                dispersion = val

        return pd.Series(
            {
                "breadth_pct_above_ma20": breadth_ma20,
                "breadth_pct_above_ma60": breadth_ma60,
                "breadth_risk_off_ratio": breadth_risk_off,
                "dispersion_score": dispersion,
            }
        )

    @staticmethod
    def _resolve_daily_zone(row: pd.Series) -> dict[str, Any]:
        def _as_float(val: Any) -> float | None:
            try:
                if pd.isna(val):
                    return None
            except Exception:
                pass
            try:
                return float(val)
            except Exception:
                return None

        regime = str(row.get("regime") or "").upper()
        bb_pos = _as_float(row.get("bb_pos"))
        bb_width = _as_float(row.get("bb_width"))

        zone_id = "DZ_NEUTRAL"
        zone_score = 50
        cap_multiplier = 1.0
        reason_parts: list[str] = []

        if regime == "UNKNOWN":
            zone_id = "DZ_UNKNOWN"
            zone_score = 50
            cap_multiplier = 1.0
            reason_parts.append("regime=UNKNOWN")
        elif regime in {"BREAKDOWN", "BEAR_CONFIRMED"}:
            zone_id = "DZ_BREAKDOWN"
            zone_score = 10
            cap_multiplier = 0.2
            reason_parts.append(f"regime={regime}")
        elif bb_pos is not None:
            if bb_pos <= 0.15:
                zone_id = "DZ_LOW_EDGE"
                zone_score = 40
                cap_multiplier = 0.6
                reason_parts.append(f"bb_pos={bb_pos:.2f}")
            elif bb_pos >= 0.85:
                if bb_width is not None and bb_width >= 0.12:
                    zone_id = "DZ_OVERHEAT"
                    zone_score = 30
                    cap_multiplier = 0.5
                    reason_parts.append(f"bb_pos={bb_pos:.2f}")
                    reason_parts.append(f"bb_width={bb_width:.2f}")
                else:
                    zone_id = "DZ_HIGH_EDGE"
                    zone_score = 60
                    cap_multiplier = 0.9
                    reason_parts.append(f"bb_pos={bb_pos:.2f}")
            else:
                reason_parts.append(f"bb_pos={bb_pos:.2f}")
        else:
            reason_parts.append("bb_pos=NA")

        if regime and regime != "UNKNOWN":
            reason_parts.append(f"regime={regime}")

        return {
            "daily_zone_id": zone_id,
            "daily_zone_score": zone_score,
            "daily_cap_multiplier": cap_multiplier,
            "daily_zone_reason": ";".join(reason_parts)[:255] if reason_parts else None,
        }

================================================================================
FILE: ashare/market_indicator_runner.py
================================================================================

from __future__ import annotations

import datetime as dt
import logging
from typing import Any, Optional

import pandas as pd
from sqlalchemy import text

from .market_indicator_builder import MarketIndicatorBuilder
from .open_monitor_repo import OpenMonitorRepository


class MarketIndicatorRunner:
    """日线/周线指标回填入口。"""

    def __init__(
        self,
        *,
        repo: OpenMonitorRepository,
        builder: MarketIndicatorBuilder,
        logger: logging.Logger,
    ) -> None:
        self.repo = repo
        self.builder = builder
        self.logger = logger

    def run_weekly_indicator(
        self,
        *,
        start_date: str | None = None,
        end_date: str | None = None,
        mode: str = "incremental",
    ) -> dict[str, Any]:
        benchmark_code = str(self.repo.params.weekly_benchmark_code or "").strip()
        end_dt = self._resolve_end_date(end_date, benchmark_code)
        if end_dt is None:
            raise ValueError("无法解析 weekly 指标的 end_date。")

        start_dt = self._resolve_start_date(
            start_date,
            end_dt,
            mode=mode,
            latest_date=self.repo.get_latest_weekly_indicator_date(),
        )
        weekly_dates = self.builder.resolve_weekly_asof_dates(start_dt, end_dt)
        if not weekly_dates:
            return {
                "written": 0,
                "start_date": start_dt.isoformat(),
                "end_date": end_dt.isoformat(),
            }

        written = 0
        for asof_date in weekly_dates:
            rows = self.builder.compute_weekly_indicator(
                asof_date.isoformat(), checked_at=dt.datetime.now()
            )
            written += self.repo.upsert_weekly_indicator(rows)
        return {
            "written": written,
            "start_date": start_dt.isoformat(),
            "end_date": end_dt.isoformat(),
        }

    def run_daily_indicator(
        self,
        *,
        start_date: str | None = None,
        end_date: str | None = None,
        mode: str = "incremental",
    ) -> dict[str, Any]:
        benchmark_code = str(self.repo.params.index_code or "sh.000001").strip()
        end_dt = self._resolve_end_date(end_date, benchmark_code)
        if end_dt is None:
            raise ValueError("无法解析 daily 指标的 end_date。")

        latest_daily = self.repo.get_latest_daily_market_env_date(
            benchmark_code=benchmark_code
        )
        latest_daily_date = (
            dt.date.fromisoformat(latest_daily) if latest_daily else None
        )
        start_dt = self._resolve_start_date(
            start_date,
            end_dt,
            mode=mode,
            latest_date=latest_daily_date,
        )
        rows = self.builder.compute_daily_indicators(start_dt, end_dt)
        rows = self.repo.attach_cycle_phase_from_weekly(rows)
        if rows:
            dates = [row.get("asof_trade_date") for row in rows]
            counts = pd.Series(dates).value_counts()
            if (counts > 1).any():
                self.logger.warning("daily env 输出存在重复交易日：%s", counts[counts > 1])
        written = self.repo.upsert_daily_market_env(rows)
        return {
            "written": written,
            "start_date": start_dt.isoformat(),
            "end_date": end_dt.isoformat(),
        }

    def _resolve_end_date(
        self, end_date: Optional[str], index_code: str
    ) -> dt.date | None:
        if end_date:
            try:
                return dt.date.fromisoformat(str(end_date))
            except Exception:
                return None
        stmt = text(
            """
            SELECT MAX(`date`) AS latest_date
            FROM history_index_daily_kline
            WHERE `code` = :code
            """
        )
        try:
            with self.repo.engine.begin() as conn:
                row = conn.execute(stmt, {"code": index_code}).mappings().first()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("读取最新交易日失败：%s", exc)
            return None
        if not row:
            return None
        latest = row.get("latest_date")
        if isinstance(latest, dt.datetime):
            return latest.date()
        if isinstance(latest, dt.date):
            return latest
        if latest:
            try:
                return pd.to_datetime(latest).date()
            except Exception:
                return None
        return None

    def _resolve_start_date(
        self,
        start_date: Optional[str],
        end_dt: dt.date,
        *,
        mode: str,
        latest_date: Optional[dt.date],
    ) -> dt.date:
        if start_date:
            return dt.date.fromisoformat(str(start_date))
        mode_norm = str(mode or "").strip().lower() or "incremental"
        if mode_norm == "incremental" and latest_date:
            return latest_date + dt.timedelta(days=1)
        default_days = 365
        return end_dt - dt.timedelta(days=default_days)

================================================================================
FILE: ashare/market_regime.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List

import pandas as pd

from .indicator_utils import consecutive_true


@dataclass
class MarketRegimeResult:
    score: float | None
    detail: Dict[str, Dict[str, Any]]
    regime: str | None
    position_hint: float | None
    context: Dict[str, Any]

    def to_payload(self) -> Dict[str, Any]:
        payload = {
            "score": self.score,
            "detail": self.detail,
            "regime": self.regime,
            "position_hint": self.position_hint,
        }
        payload.update(self.context)
        return payload


class MarketRegimeClassifier:
    """基于指数日线的市场环境分类器。"""

    def __init__(
        self,
        breakdown_window: int = 60,
        effective_breakdown_days: int = 3,
        effective_reclaim_days: int = 2,
    ) -> None:
        self.breakdown_window = max(int(breakdown_window), 1)
        self.effective_breakdown_days = max(int(effective_breakdown_days), 1)
        self.effective_reclaim_days = max(int(effective_reclaim_days), 1)

    def classify(
        self,
        df: pd.DataFrame,
        *,
        short: int = 20,
        mid: int = 60,
        long: int = 250,
    ) -> MarketRegimeResult:
        if df is None or df.empty:
            return MarketRegimeResult(
                score=None,
                detail={},
                regime=None,
                position_hint=None,
                context={},
            )

        work = df.copy()
        work["date"] = pd.to_datetime(work["date"], errors="coerce")
        for col in ["close", "ma20", "ma60", "ma250"]:
            if col in work.columns:
                work[col] = pd.to_numeric(work[col], errors="coerce")

        detail: Dict[str, Dict[str, Any]] = {}
        statuses: List[str] = []
        below_streaks: List[int] = []
        break_confirmed_flags: List[bool] = []
        reclaim_confirmed_flags: List[bool] = []

        for code, group in work.groupby("code"):
            grp = group.sort_values("date")
            close = pd.to_numeric(grp.get("close"), errors="coerce")
            grp["ma20"] = close.rolling(short, min_periods=short).mean()
            grp["ma60"] = close.rolling(mid, min_periods=mid).mean()
            grp["ma250"] = close.rolling(long, min_periods=long).mean()
            grp["rolling_low"] = close.rolling(self.breakdown_window, min_periods=1).min().shift(1)

            ma250_valid = grp["ma250"].notna()
            below_ma250 = (close < grp["ma250"]) & ma250_valid
            prev_below = (below_ma250.shift(1).astype("boolean").fillna(False).astype(bool))
            below_ma250_streak_series = consecutive_true(below_ma250)
            below_ma250_streak = (
                int(below_ma250_streak_series.iloc[-1])
                if not below_ma250_streak_series.empty
                else 0
            )

            above_ma250 = (close >= grp["ma250"]) & ma250_valid
            reclaim_streak_series = consecutive_true(above_ma250)
            reclaim_streak = (
                int(reclaim_streak_series.iloc[-1])
                if not reclaim_streak_series.empty
                else 0
            )

            break_confirmed = below_ma250_streak >= self.effective_breakdown_days
            reclaim_confirmed = (
                reclaim_streak >= self.effective_reclaim_days
                and bool(prev_below.iloc[-1])
            )

            yearline_state = "ABOVE"
            if break_confirmed:
                yearline_state = "BREAK_CONFIRMED"
            elif below_ma250_streak > 0:
                yearline_state = "BELOW_1_2D"
            elif reclaim_confirmed:
                yearline_state = "RECLAIM_CONFIRMED"

            latest = grp.iloc[-1]
            close_val = latest.get("close")
            ma20 = latest.get("ma20")
            ma60 = latest.get("ma60")
            ma250 = latest.get("ma250")
            rolling_low = latest.get("rolling_low")

            score = 0
            for ma in [ma20, ma60, ma250]:
                if ma is not None and close_val is not None and close_val > ma:
                    score += 1

            status = "RISK_ON"
            pullback_mode = None
            if close_val is None or pd.isna(close_val):
                status = "UNKNOWN"
            elif break_confirmed:
                status = "BEAR_CONFIRMED"
            elif rolling_low is not None and not pd.isna(rolling_low) and close_val < rolling_low:
                status = "BREAKDOWN"
            elif ma250 is not None and not pd.isna(ma250) and close_val < ma250:
                status = "RISK_OFF"
            elif ma60 is not None and not pd.isna(ma60) and close_val < ma60:
                status = "PULLBACK"
                daily_ret = close.pct_change().iloc[-1]
                pullback_mode = "FAST_DROP" if daily_ret <= -0.03 else "SIDEWAYS"
            elif ma20 is not None and not pd.isna(ma20) and close_val < ma20:
                status = "PULLBACK"
                pullback_mode = "SIDEWAYS"

            statuses.append(status)
            below_streaks.append(below_ma250_streak)
            break_confirmed_flags.append(break_confirmed)
            reclaim_confirmed_flags.append(reclaim_confirmed)
            detail[str(code)] = {
                "close": close_val,
                "ma20": ma20,
                "ma60": ma60,
                "ma250": ma250,
                "score": score,
                "status": status,
                "pullback_mode": pullback_mode,
                "rolling_low": rolling_low,
                "below_ma250_streak": below_ma250_streak,
                "break_confirmed": break_confirmed,
                "reclaim_confirmed": reclaim_confirmed,
                "reclaim_streak": reclaim_streak,
                "yearline_state": yearline_state,
            }

        if not detail:
            return MarketRegimeResult(
                score=None,
                detail={},
                regime=None,
                position_hint=None,
                context={},
            )

        avg_score = sum(v.get("score", 0) for v in detail.values()) / max(len(detail), 1)

        regime = "RISK_ON"
        if "BEAR_CONFIRMED" in statuses:
            regime = "BEAR_CONFIRMED"
        elif "BREAKDOWN" in statuses:
            regime = "BREAKDOWN"
        elif "RISK_OFF" in statuses:
            regime = "RISK_OFF"
        elif "PULLBACK" in statuses:
            regime = "PULLBACK"

        position_hint_map = {
            "RISK_ON": 0.8,
            "PULLBACK": 0.4,
            "RISK_OFF": 0.2,
            "BREAKDOWN": 0.0,
            "BEAR_CONFIRMED": 0.0,
        }
        position_hint = position_hint_map.get(regime)

        if regime == "PULLBACK":
            fast_drop = any(
                (v.get("pullback_mode") == "FAST_DROP") for v in detail.values()
            )
            if fast_drop and position_hint is not None:
                position_hint = min(position_hint, 0.3)

        below_ma250_max = max(below_streaks) if below_streaks else 0
        below_any = any(bs > 0 for bs in below_streaks)
        break_confirmed_any = any(break_confirmed_flags)
        reclaim_confirmed_any = any(reclaim_confirmed_flags)
        yearline_state = "ABOVE"
        if break_confirmed_any:
            yearline_state = "BREAK_CONFIRMED"
        elif below_any:
            yearline_state = "BELOW_1_2D"
        elif reclaim_confirmed_any:
            yearline_state = "RECLAIM_CONFIRMED"
        context: Dict[str, Any] = {
            "below_ma250_streak": below_ma250_max,
            "break_confirmed": break_confirmed_any,
            "reclaim_confirmed": reclaim_confirmed_any,
            "effective_breakdown_days": self.effective_breakdown_days,
            "effective_reclaim_days": self.effective_reclaim_days,
            "yearline_state": yearline_state,
        }
        if regime == "BEAR_CONFIRMED":
            context["regime_note"] = "年线有效跌破：连续3日收盘低于MA250，趋势破位"

        return MarketRegimeResult(
            score=avg_score,
            detail=detail,
            regime=regime,
            position_hint=position_hint,
            context=context,
        )

================================================================================
FILE: ashare/monitor_rules.py
================================================================================

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Any, List, Mapping

from .utils.convert import to_float as _to_float


@dataclass(frozen=True)
class MonitorRuleConfig:
    """开盘监测规则参数集中管理。

    说明：
    - 所有阈值集中到此处，open_monitor 只负责拼装上下文并执行规则。
    - 运行时动态值（例如 threshold_gap_up / ma20_thresh）由 DecisionContext 计算并携带。
    """

    # --- thresholds ---
    max_gap_up_pct: float = 0.05
    max_gap_up_atr_mult: float = 1.5
    max_gap_down_pct: float = -0.03
    min_open_vs_ma20_pct: float = 0.0
    pullback_min_open_vs_ma20_pct: float = -0.01
    below_ma20_tol_pct: float = 0.002
    limit_up_trigger_pct: float = 9.7
    max_entry_vs_ma5_pct: float = 0.08
    runup_atr_max: float = 1.2
    runup_atr_vol_mult: float = 1.2
    runup_atr_max_cap: float = 2.0
    runup_atr_tol: float = 0.02
    pullback_runup_atr_max: float = 1.5
    pullback_runup_dev_ma20_atr_min: float = 1.0
    ma20_atr_tol_mult: float = 0.5
    ma20_dyn_min_pct: float = -0.03
    ma20_prewarn_buffer_pct: float = 0.005
    dev_ma5_atr_max: float = 2.0
    dev_ma20_atr_max: float = 2.5
    stop_atr_mult: float = 2.0
    signal_day_limit_up_pct: float = 0.095
    env_index_score_threshold: float = 2.0
    weekly_soft_gate_strength_threshold: float = 3.5

    # --- switches ---
    enable_env_gate: bool = True
    enable_chip_score: bool = True
    enable_quote_missing: bool = True
    enable_gap_up: bool = True
    enable_gap_down: bool = True
    enable_below_ma20: bool = True
    enable_limit_up: bool = True
    enable_runup_breach: bool = True
    enable_ma20_prewarn: bool = True

    enable_signal_expired: bool = True

    # --- thresholds ---
    chip_score_wait_threshold: float = -0.5

    chip_score_allow_small_cap: float = 0.2

    # --- actions & reasons ---
    env_stop_action: str = "SKIP"
    env_stop_reason: str = "环境阻断"
    env_wait_action: str = "WAIT"
    env_wait_reason: str = "环境等待"
    chip_score_reason: str = "筹码评分<=-0.5"
    quote_missing_action: str = "SKIP"
    quote_missing_reason: str = "行情数据不可用"
    gap_up_action: str = "SKIP"
    gap_up_reason: str = "高开过阈值"
    gap_down_action: str = "SKIP"
    gap_down_reason: str = "低开破位"
    below_ma20_action: str = "SKIP"
    below_ma20_reason: str = "未站上MA20要求"
    ma20_prewarn_reason: str = "接近MA20阈值"
    limit_up_action: str = "SKIP"
    limit_up_reason: str = "涨停不可成交"
    runup_breach_action: str = "WAIT"
    runup_breach_fallback_reason: str = "拉升过快"


    signal_expired_action: str = "SKIP"
    signal_expired_reason: str = "信号已过期"

    # --- severity ---
    sev_env_stop: int = 100
    sev_env_wait: int = 90
    sev_signal_expired: int = 85
    sev_chip_score: int = 80
    sev_quote_missing: int = 75
    sev_gap_up: int = 70
    sev_gap_down: int = 70
    sev_below_ma20: int = 60
    sev_ma20_prewarn: int = 10
    sev_limit_up: int = 60
    sev_runup_breach: int = 55

    @classmethod
    def from_config(
        cls,
        cfg: Mapping[str, Any] | None,
        *,
        logger: logging.Logger | None = None,
    ) -> "MonitorRuleConfig":
        cfg = cfg or {}
        defaults = cls()

        def _get_bool(key: str, default: bool) -> bool:
            raw = cfg.get(key, default)
            if isinstance(raw, bool):
                return raw
            if isinstance(raw, str):
                return raw.strip().lower() in {"1", "true", "yes", "y", "on"}
            return bool(raw)

        def _get_float(key: str, default: float) -> float:
            parsed = _to_float(cfg.get(key, default))
            return default if parsed is None else float(parsed)

        def _normalize_ratio_pct(value: float, key: str) -> float:
            normalized = value / 100.0 if abs(value) > 1.5 else value
            if logger and abs(value) > 1.5:
                logger.info(
                    "配置 %s 以百分数填写（%s），已按比例 %.4f 处理。",
                    key,
                    value,
                    normalized,
                )
            return normalized

        def _normalize_percent_value(value: float, key: str) -> float:
            normalized = value * 100.0 if abs(value) <= 1.5 else value
            if logger and abs(value) <= 1.5:
                logger.info(
                    "配置 %s 以小数比例填写（%s），已按百分数 %.2f%% 处理。",
                    key,
                    value,
                    normalized,
                )
            return normalized

        return cls(
            max_gap_up_pct=_normalize_ratio_pct(
                _get_float("max_gap_up_pct", defaults.max_gap_up_pct),
                "max_gap_up_pct",
            ),
            max_gap_up_atr_mult=_get_float("max_gap_up_atr_mult", defaults.max_gap_up_atr_mult),
            max_gap_down_pct=_normalize_ratio_pct(
                _get_float("max_gap_down_pct", defaults.max_gap_down_pct),
                "max_gap_down_pct",
            ),
            min_open_vs_ma20_pct=_normalize_ratio_pct(
                _get_float("min_open_vs_ma20_pct", defaults.min_open_vs_ma20_pct),
                "min_open_vs_ma20_pct",
            ),
            pullback_min_open_vs_ma20_pct=_normalize_ratio_pct(
                _get_float("pullback_min_open_vs_ma20_pct", defaults.pullback_min_open_vs_ma20_pct),
                "pullback_min_open_vs_ma20_pct",
            ),
            below_ma20_tol_pct=_normalize_ratio_pct(
                _get_float("below_ma20_tol_pct", defaults.below_ma20_tol_pct),
                "below_ma20_tol_pct",
            ),
            limit_up_trigger_pct=_normalize_percent_value(
                _get_float("limit_up_trigger_pct", defaults.limit_up_trigger_pct),
                "limit_up_trigger_pct",
            ),
            max_entry_vs_ma5_pct=_normalize_ratio_pct(
                _get_float("max_entry_vs_ma5_pct", defaults.max_entry_vs_ma5_pct),
                "max_entry_vs_ma5_pct",
            ),
            runup_atr_max=_get_float("runup_atr_max", defaults.runup_atr_max),
            runup_atr_vol_mult=_get_float("runup_atr_vol_mult", defaults.runup_atr_vol_mult),
            runup_atr_max_cap=_get_float("runup_atr_max_cap", defaults.runup_atr_max_cap),
            runup_atr_tol=_get_float("runup_atr_tol", defaults.runup_atr_tol),
            pullback_runup_atr_max=_get_float(
                "pullback_runup_atr_max", defaults.pullback_runup_atr_max
            ),
            pullback_runup_dev_ma20_atr_min=_get_float(
                "pullback_runup_dev_ma20_atr_min", defaults.pullback_runup_dev_ma20_atr_min
            ),
            ma20_atr_tol_mult=_get_float("ma20_atr_tol_mult", defaults.ma20_atr_tol_mult),
            ma20_dyn_min_pct=_normalize_ratio_pct(
                _get_float("ma20_dyn_min_pct", defaults.ma20_dyn_min_pct),
                "ma20_dyn_min_pct",
            ),
            ma20_prewarn_buffer_pct=_normalize_ratio_pct(
                _get_float("ma20_prewarn_buffer_pct", defaults.ma20_prewarn_buffer_pct),
                "ma20_prewarn_buffer_pct",
            ),
            dev_ma5_atr_max=_get_float("dev_ma5_atr_max", defaults.dev_ma5_atr_max),
            dev_ma20_atr_max=_get_float("dev_ma20_atr_max", defaults.dev_ma20_atr_max),
            stop_atr_mult=_get_float("stop_atr_mult", defaults.stop_atr_mult),
            signal_day_limit_up_pct=_normalize_ratio_pct(
                _get_float("signal_day_limit_up_pct", defaults.signal_day_limit_up_pct),
                "signal_day_limit_up_pct",
            ),
            env_index_score_threshold=_get_float(
                "env_index_score_threshold",
                defaults.env_index_score_threshold,
            ),
            weekly_soft_gate_strength_threshold=_get_float(
                "weekly_soft_gate_strength_threshold",
                defaults.weekly_soft_gate_strength_threshold,
            ),
            enable_env_gate=_get_bool("enable_env_gate", defaults.enable_env_gate),
            enable_chip_score=_get_bool("enable_chip_score", defaults.enable_chip_score),
            enable_quote_missing=_get_bool("enable_quote_missing", defaults.enable_quote_missing),
            enable_gap_up=_get_bool("enable_gap_up", defaults.enable_gap_up),
            enable_gap_down=_get_bool("enable_gap_down", defaults.enable_gap_down),
            enable_below_ma20=_get_bool("enable_below_ma20", defaults.enable_below_ma20),
            enable_limit_up=_get_bool("enable_limit_up", defaults.enable_limit_up),
            enable_runup_breach=_get_bool("enable_runup_breach", defaults.enable_runup_breach),
            enable_ma20_prewarn=_get_bool(
                "enable_ma20_prewarn", defaults.enable_ma20_prewarn
            ),
            enable_signal_expired=_get_bool(
                "enable_signal_expired", defaults.enable_signal_expired
            ),
            chip_score_wait_threshold=_get_float(
                "chip_score_wait_threshold", defaults.chip_score_wait_threshold
            ),
            chip_score_allow_small_cap=_get_float(
                "chip_score_allow_small_cap", defaults.chip_score_allow_small_cap
            ),
            env_stop_action=str(cfg.get("env_stop_action", defaults.env_stop_action)).strip()
            or defaults.env_stop_action,
            env_stop_reason=str(cfg.get("env_stop_reason", defaults.env_stop_reason)).strip()
            or defaults.env_stop_reason,
            env_wait_action=str(cfg.get("env_wait_action", defaults.env_wait_action)).strip()
            or defaults.env_wait_action,
            env_wait_reason=str(cfg.get("env_wait_reason", defaults.env_wait_reason)).strip()
            or defaults.env_wait_reason,
            chip_score_reason=str(cfg.get("chip_score_reason", defaults.chip_score_reason)).strip()
            or defaults.chip_score_reason,
            quote_missing_action=str(
                cfg.get("quote_missing_action", defaults.quote_missing_action)
            ).strip()
            or defaults.quote_missing_action,
            quote_missing_reason=str(
                cfg.get("quote_missing_reason", defaults.quote_missing_reason)
            ).strip()
            or defaults.quote_missing_reason,
            gap_up_action=str(cfg.get("gap_up_action", defaults.gap_up_action)).strip()
            or defaults.gap_up_action,
            gap_up_reason=str(cfg.get("gap_up_reason", defaults.gap_up_reason)).strip()
            or defaults.gap_up_reason,
            gap_down_action=str(cfg.get("gap_down_action", defaults.gap_down_action)).strip()
            or defaults.gap_down_action,
            gap_down_reason=str(cfg.get("gap_down_reason", defaults.gap_down_reason)).strip()
            or defaults.gap_down_reason,
            below_ma20_action=str(
                cfg.get("below_ma20_action", defaults.below_ma20_action)
            ).strip()
            or defaults.below_ma20_action,
            below_ma20_reason=str(
                cfg.get("below_ma20_reason", defaults.below_ma20_reason)
            ).strip()
            or defaults.below_ma20_reason,
            ma20_prewarn_reason=str(
                cfg.get("ma20_prewarn_reason", defaults.ma20_prewarn_reason)
            ).strip()
            or defaults.ma20_prewarn_reason,
            limit_up_action=str(cfg.get("limit_up_action", defaults.limit_up_action)).strip()
            or defaults.limit_up_action,
            limit_up_reason=str(cfg.get("limit_up_reason", defaults.limit_up_reason)).strip()
            or defaults.limit_up_reason,
            runup_breach_action=str(
                cfg.get("runup_breach_action", defaults.runup_breach_action)
            ).strip()
            or defaults.runup_breach_action,
            runup_breach_fallback_reason=str(
                cfg.get("runup_breach_fallback_reason", defaults.runup_breach_fallback_reason)
            ).strip()
            or defaults.runup_breach_fallback_reason,
            signal_expired_action=str(
                cfg.get("signal_expired_action", defaults.signal_expired_action)
            ).strip()
            or defaults.signal_expired_action,
            signal_expired_reason=str(
                cfg.get("signal_expired_reason", defaults.signal_expired_reason)
            ).strip()
            or defaults.signal_expired_reason,
            sev_env_stop=int(cfg.get("sev_env_stop", defaults.sev_env_stop)),
            sev_env_wait=int(cfg.get("sev_env_wait", defaults.sev_env_wait)),
            sev_signal_expired=int(cfg.get("sev_signal_expired", defaults.sev_signal_expired)),
            sev_chip_score=int(cfg.get("sev_chip_score", defaults.sev_chip_score)),
            sev_quote_missing=int(cfg.get("sev_quote_missing", defaults.sev_quote_missing)),
            sev_gap_up=int(cfg.get("sev_gap_up", defaults.sev_gap_up)),
            sev_gap_down=int(cfg.get("sev_gap_down", defaults.sev_gap_down)),
            sev_below_ma20=int(cfg.get("sev_below_ma20", defaults.sev_below_ma20)),
            sev_ma20_prewarn=int(cfg.get("sev_ma20_prewarn", defaults.sev_ma20_prewarn)),
            sev_limit_up=int(cfg.get("sev_limit_up", defaults.sev_limit_up)),
            sev_runup_breach=int(cfg.get("sev_runup_breach", defaults.sev_runup_breach)),
        )


def build_default_monitor_rules(
    config: MonitorRuleConfig,
    *,
    Rule: Any,
    RuleResult: Any,
) -> List[Any]:
    """构建默认的开盘监测“硬门控规则”。

    说明：
    - 通过注入 Rule / RuleResult 类型，避免 monitor_rules 直接 import open_monitor 引发循环依赖。
    - predicate 依赖 DecisionContext 上的字段（由 open_monitor 在 per-row 构造 ctx 时填充）。
    """

    def _chip_effective(ctx: Any) -> bool:
        chip_score = getattr(ctx, "chip_score", None)
        if chip_score is None:
            return False
        chip_age = getattr(ctx, "chip_age_days", None)
        if chip_age is None:
            return False
        if chip_age > 45:
            return False
        if getattr(ctx, "chip_stale_hit", None):
            return False
        chip_reason = str(getattr(ctx, "chip_reason", "") or "").strip()
        if chip_reason.startswith("DATA_"):
            return False
        if "OUTLIER" in chip_reason.upper():
            return False
        return True

    return [
        Rule(
            id="ENV_STOP",
            category="ACTION",
            severity=config.sev_env_stop,
            predicate=lambda ctx: bool(
                config.enable_env_gate
                and getattr(ctx, "env", None)
                and getattr(getattr(ctx, "env"), "gate_action", None) == "STOP"
            ),
            effect=lambda ctx: RuleResult(
                reason=config.env_stop_reason,
                action_override=config.env_stop_action,
            ),
        ),
        Rule(
            id="ENV_WAIT",
            category="ACTION",
            severity=config.sev_env_wait,
            predicate=lambda ctx: bool(
                config.enable_env_gate
                and getattr(ctx, "env", None)
                and getattr(getattr(ctx, "env"), "gate_action", None) == "WAIT"
            ),
            effect=lambda ctx: RuleResult(
                reason=config.env_wait_reason,
                action_override=config.env_wait_action,
            ),
        ),
        Rule(
            id="CHIP_SCORE_NEG",
            category="ACTION",
            severity=config.sev_chip_score,
            predicate=lambda ctx: bool(
                config.enable_chip_score
                and getattr(ctx, "chip_score", None) is not None
                and _chip_effective(ctx)
                and getattr(ctx, "chip_score") < config.chip_score_wait_threshold
            ),
            effect=lambda ctx: (
                RuleResult(
                    reason=config.chip_score_reason,
                    cap_override=config.chip_score_allow_small_cap,
                )
            ),
        ),
        Rule(
            id="QUOTE_MISSING",
            category="ACTION",
            severity=config.sev_quote_missing,
            predicate=lambda ctx: bool(config.enable_quote_missing and getattr(ctx, "price_now", None) is None),
            effect=lambda ctx: RuleResult(
                reason=config.quote_missing_reason,
                action_override=config.quote_missing_action,
            ),
        ),
        Rule(
            id="GAP_UP_TOO_MUCH",
            category="ACTION",
            severity=config.sev_gap_up,
            predicate=lambda ctx: bool(
                config.enable_gap_up
                and getattr(ctx, "live_gap", None) is not None
                and getattr(ctx, "threshold_gap_up", None) is not None
                and getattr(ctx, "live_gap") > getattr(ctx, "threshold_gap_up")
            ),
            effect=lambda ctx: RuleResult(
                reason=config.gap_up_reason,
                action_override=config.gap_up_action,
            ),
        ),
        Rule(
            id="GAP_DOWN_BREAK",
            category="ACTION",
            severity=config.sev_gap_down,
            predicate=lambda ctx: bool(
                config.enable_gap_down
                and getattr(ctx, "live_gap", None) is not None
                and getattr(ctx, "max_gap_down", None) is not None
                and getattr(ctx, "live_gap") < getattr(ctx, "max_gap_down")
            ),
            effect=lambda ctx: RuleResult(
                reason=config.gap_down_reason,
                action_override=config.gap_down_action,
            ),
        ),
        Rule(
            id="MA20_PREWARN",
            category="ACTION",
            severity=config.sev_ma20_prewarn,
            predicate=lambda ctx: bool(
                config.enable_ma20_prewarn
                and getattr(ctx, "ma20_prewarn", False)
            ),
            effect=lambda ctx: RuleResult(
                reason=getattr(ctx, "ma20_prewarn_reason", None)
                or config.ma20_prewarn_reason,
            ),
        ),
        Rule(
            id="BELOW_MA20_REQ",
            category="ACTION",
            severity=config.sev_below_ma20,
            predicate=lambda ctx: bool(
                config.enable_below_ma20
                and getattr(ctx, "price_now", None) is not None
                and getattr(ctx, "sig_ma20", None) is not None
                and getattr(ctx, "ma20_thresh", None) is not None
                and getattr(ctx, "price_now") < getattr(ctx, "sig_ma20") * (1 + getattr(ctx, "ma20_thresh") - config.below_ma20_tol_pct)
            ),
            effect=lambda ctx: RuleResult(
                reason=config.below_ma20_reason,
                action_override=config.below_ma20_action,
            ),
        ),
        Rule(
            id="LIMIT_UP",
            category="ACTION",
            severity=config.sev_limit_up,
            predicate=lambda ctx: bool(
                config.enable_limit_up
                and getattr(ctx, "live_pct", None) is not None
                and getattr(ctx, "limit_up_trigger", None) is not None
                and getattr(ctx, "live_pct") >= getattr(ctx, "limit_up_trigger")
            ),
            effect=lambda ctx: RuleResult(
                reason=config.limit_up_reason,
                action_override=config.limit_up_action,
            ),
        ),
        Rule(
            id="RUNUP_BREACH",
            category="ACTION",
            severity=config.sev_runup_breach,
            predicate=lambda ctx: bool(config.enable_runup_breach and bool(getattr(ctx, "runup_breach", False))),
            effect=lambda ctx: RuleResult(
                reason=getattr(ctx, "runup_breach_reason", None) or config.runup_breach_fallback_reason,
                action_override=config.runup_breach_action,
            ),
        ),
    ]

================================================================================
FILE: ashare/open_monitor.py
================================================================================

"""开盘监测：检查“前一交易日收盘信号”在今日开盘是否仍可执行。

目标：
- 读取 strategy_signal_events 中“最新交易日”的 BUY 信号（通常是昨天收盘跑出来的）。
- 在开盘/集合竞价阶段拉取实时行情（今开/最新价），做二次过滤：
  - 高开过多（追高风险/买不到合理价）
  - 低开破位（跌破 MA20 / 大幅低开）
  - 涨停（大概率买不到）

输出：
- 可选写入 MySQL：strategy_open_monitor_eval、strategy_open_monitor_quote（默认 append）
- 可选导出 CSV 到 output/open_monitor

注意：
- 该脚本“只做监测与清单输出”，不下单。
- 实时行情默认使用 Eastmoney push2 接口；如需测试 AkShare，可在 config.yaml 将 open_monitor.quote_source=akshare。
"""

from __future__ import annotations

import datetime as dt
import json
import logging
import math
from dataclasses import dataclass, replace
from typing import Any, Callable, List

import pandas as pd

from .config import get_section
from .db import DatabaseConfig, MySQLWriter
from .monitor_rules import MonitorRuleConfig, build_default_monitor_rules
from .open_monitor_env import OpenMonitorEnvService
from .open_monitor_eval import (
    OpenMonitorEvaluator,
    merge_gate_actions,
)
from .open_monitor_market_data import OpenMonitorMarketData
from .open_monitor_repo import OpenMonitorRepository, calc_run_id, make_snapshot_hash
from .open_monitor_rules import Rule, RuleEngine, RuleResult
from .market_indicator_builder import MarketIndicatorBuilder
from .schema_manager import (
    STRATEGY_CODE_MA5_MA20_TREND,
    TABLE_STRATEGY_DAILY_MARKET_ENV,
    TABLE_STRATEGY_OPEN_MONITOR_ENV,
    TABLE_STRATEGY_OPEN_MONITOR_EVAL,
    TABLE_STRATEGY_OPEN_MONITOR_QUOTE,
    TABLE_STRATEGY_OPEN_MONITOR_RUN,
    TABLE_STRATEGY_WEEKLY_MARKET_ENV,
    WEEKLY_MARKET_BENCHMARK_CODE,
    VIEW_STRATEGY_OPEN_MONITOR,
    VIEW_STRATEGY_OPEN_MONITOR_ENV,
    VIEW_STRATEGY_OPEN_MONITOR_WIDE,
    TABLE_STRATEGY_READY_SIGNALS,
)
from .utils.convert import to_float as _to_float
from .utils.logger import setup_logger
from .weekly_env_builder import WeeklyEnvironmentBuilder


@dataclass(frozen=True)
class OpenMonitorParams:
    """开盘监测参数（支持从 config.yaml 的 open_monitor 覆盖）。"""

    enabled: bool = True
    # 运行时上下文（由 Runner 填充；用于记录行情抓取时间）
    checked_at: dt.datetime | None = None
    monitor_date: str | None = None

    # 信号输入：只接受 ready_signals_view（由 SchemaManager 负责生成/维护）
    ready_signals_view: str = TABLE_STRATEGY_READY_SIGNALS
    # 契约与 fail-fast（默认开启）
    strict_ready_signals_required: bool = True
    strict_quotes: bool = True
    quote_table: str = TABLE_STRATEGY_OPEN_MONITOR_QUOTE
    strategy_code: str = STRATEGY_CODE_MA5_MA20_TREND

    # 输出表：开盘检查结果
    output_table: str = TABLE_STRATEGY_OPEN_MONITOR_EVAL
    run_table: str = TABLE_STRATEGY_OPEN_MONITOR_RUN
    open_monitor_view: str = VIEW_STRATEGY_OPEN_MONITOR
    open_monitor_wide_view: str = VIEW_STRATEGY_OPEN_MONITOR_WIDE
    open_monitor_env_view: str = VIEW_STRATEGY_OPEN_MONITOR_ENV

    # 回看近 N 个交易日的 BUY 信号
    signal_lookback_days: int = 3

    # 行情来源：eastmoney / akshare（兼容：auto 将按 eastmoney 处理）
    quote_source: str = "eastmoney"

    # 输出控制
    write_to_db: bool = True

    # 指数快照配置
    index_code: str = "sh.000001"
    index_hist_lookback_days: int = 250

    # 增量写入：
    # - True：每次运行都 append（保留历史快照，便于对比）
    # - False：按 monitor_date+code 先删后写（只保留当天最新一份）
    incremental_write: bool = True

    # 增量导出：文件名带 checked_at 时间戳，避免同一天多次导出互相覆盖
    incremental_export_timestamp: bool = True

    export_csv: bool = True
    export_top_n: int = 100
    output_subdir: str = "open_monitor"
    interval_minutes: int = 5
    run_id_minutes: int = 5

    # 环境表：存储周线计划等“批次级别”信息，避免在每条标的记录里重复。
    open_monitor_env_table: str = TABLE_STRATEGY_OPEN_MONITOR_ENV

    weekly_indicator_table: str = TABLE_STRATEGY_WEEKLY_MARKET_ENV
    daily_indicator_table: str = TABLE_STRATEGY_DAILY_MARKET_ENV
    weekly_benchmark_code: str = WEEKLY_MARKET_BENCHMARK_CODE

    # 同一批次内同一 code 只保留“最新 date（信号日）”那条 BUY 信号。
    # 目的：避免同一批次出现重复 code（例如同一只股票在 12-09 与 12-11 都触发 BUY）。
    unique_code_latest_date_only: bool = True

    # 输出模式：FULL 保留全部字段，COMPACT 只保留核心字段
    output_mode: str = "FULL"

    # 环境快照持久化：与行情/评估表保持一致，默认写入
    persist_env_snapshot: bool = True
    # 盘中关键位突破：触达/站稳/回踩阈值
    live_breakout_high_eps: float = 0.002
    live_breakout_latest_eps: float = 0.001
    live_retest_pct: float = 0.003
    live_retest_atr_mult: float = 0.5

    @classmethod
    def from_config(cls) -> "OpenMonitorParams":
        sec = get_section("open_monitor") or {}
        if not isinstance(sec, dict):
            sec = {}
        default_strategy_code = STRATEGY_CODE_MA5_MA20_TREND

        strat = get_section("strategy_ma5_ma20_trend") or {}
        if isinstance(strat, dict):
            default_strategy_code = strat.get("strategy_code", STRATEGY_CODE_MA5_MA20_TREND)
        else:
            default_strategy_code = STRATEGY_CODE_MA5_MA20_TREND

        logger = logging.getLogger(__name__)

        def _get_bool(key: str, default: bool) -> bool:
            val = sec.get(key, default)
            if isinstance(val, bool):
                return val
            if isinstance(val, str):
                return val.strip().lower() in {"1", "true", "yes", "y", "on"}
            return bool(val)

        def _get_int(key: str, default: int) -> int:
            raw = sec.get(key, default)
            try:
                return int(raw)
            except Exception:
                return default

        def _get_float(key: str, default: float) -> float:
            raw = sec.get(key, default)
            try:
                return float(raw)
            except Exception:
                return default

        quote_source = str(sec.get("quote_source", cls.quote_source)).strip().lower() or "auto"
        if quote_source == "auto":
            quote_source = "eastmoney"

        interval_minutes = _get_int("interval_minutes", cls.interval_minutes)

        run_id_configured = sec.get("run_id_minutes")
        run_id_minutes = (
            _get_int("run_id_minutes", interval_minutes)
            if run_id_configured is not None
            else interval_minutes
        )

        return cls(
            enabled=_get_bool("enabled", cls.enabled),
            ready_signals_view=str(
                sec.get("ready_signals_view", cls.ready_signals_view)
            ).strip()
                               or cls.ready_signals_view,
            strict_ready_signals_required=_get_bool(
                "strict_ready_signals_required", cls.strict_ready_signals_required
            ),
            strict_quotes=_get_bool("strict_quotes", cls.strict_quotes),
            quote_table=str(sec.get("quote_table", cls.quote_table)).strip() or cls.quote_table,
            strategy_code=str(sec.get("strategy_code", default_strategy_code)).strip()
                          or default_strategy_code,
            output_table=str(sec.get("output_table", cls.output_table)).strip() or cls.output_table,
            run_table=str(sec.get("run_table", cls.run_table)).strip() or cls.run_table,
            open_monitor_view=str(
                sec.get("open_monitor_view", cls.open_monitor_view)
            ).strip()
                              or cls.open_monitor_view,
            open_monitor_env_view=str(
                sec.get("open_monitor_env_view", cls.open_monitor_env_view)
            ).strip()
                                  or cls.open_monitor_env_view,
            open_monitor_wide_view=str(
                sec.get("open_monitor_wide_view", cls.open_monitor_wide_view)
            ).strip()
                                   or cls.open_monitor_wide_view,
            signal_lookback_days=_get_int("signal_lookback_days", cls.signal_lookback_days),
            quote_source=quote_source,
            index_code=str(sec.get("index_code", cls.index_code)).strip() or cls.index_code,
            index_hist_lookback_days=_get_int(
                "index_hist_lookback_days", cls.index_hist_lookback_days
            ),
            write_to_db=_get_bool("write_to_db", cls.write_to_db),
            incremental_write=_get_bool("incremental_write", cls.incremental_write),
            incremental_export_timestamp=_get_bool(
                "incremental_export_timestamp", cls.incremental_export_timestamp
            ),
            export_csv=_get_bool("export_csv", cls.export_csv),
            export_top_n=_get_int("export_top_n", cls.export_top_n),
            output_subdir=str(sec.get("output_subdir", cls.output_subdir)).strip()
                          or cls.output_subdir,
            interval_minutes=interval_minutes,
            run_id_minutes=run_id_minutes,
            unique_code_latest_date_only=_get_bool(
                "unique_code_latest_date_only", cls.unique_code_latest_date_only
            ),
            open_monitor_env_table=str(
                sec.get("open_monitor_env_table", cls.open_monitor_env_table)
            ).strip()
            or cls.open_monitor_env_table,
            weekly_indicator_table=cls.weekly_indicator_table,
            daily_indicator_table=str(
                sec.get("daily_indicator_table", cls.daily_indicator_table)
            ).strip()
            or cls.daily_indicator_table,
            weekly_benchmark_code=cls.weekly_benchmark_code,
            output_mode=str(sec.get("output_mode", cls.output_mode)).strip().upper()
                        or cls.output_mode,
            persist_env_snapshot=_get_bool("persist_env_snapshot", cls.persist_env_snapshot),
            live_breakout_high_eps=_get_float(
                "live_breakout_high_eps", cls.live_breakout_high_eps
            ),
            live_breakout_latest_eps=_get_float(
                "live_breakout_latest_eps", cls.live_breakout_latest_eps
            ),
            live_retest_pct=_get_float("live_retest_pct", cls.live_retest_pct),
            live_retest_atr_mult=_get_float(
                "live_retest_atr_mult", cls.live_retest_atr_mult
            ),
        )


class MA5MA20OpenMonitorRunner:
    """开盘监测 Runner：读取前一交易日 BUY 信号 → 拉实时行情 → 输出可执行清单。"""

    def __init__(self) -> None:
        self.logger = setup_logger()
        self.params = OpenMonitorParams.from_config()
        self.rule_config = MonitorRuleConfig.from_config(
            get_section("open_monitor"),
            logger=self.logger,
        )
        self.rules = build_default_monitor_rules(
            self.rule_config,
            Rule=Rule,
            RuleResult=RuleResult,
        )
        self.rule_engine = RuleEngine(merge_gate_actions)
        self.db_writer = MySQLWriter(DatabaseConfig.from_env())

        app_cfg = get_section("app") or {}
        self.index_codes = []
        if isinstance(app_cfg, dict):
            codes = app_cfg.get("index_codes", [])
            if isinstance(codes, (list, tuple)):
                self.index_codes = [str(c).strip() for c in codes if str(c).strip()]
        ak_cfg = get_section("akshare") or {}
        board_cfg = ak_cfg.get("board_industry", {}) if isinstance(ak_cfg, dict) else {}
        if not isinstance(board_cfg, dict):
            board_cfg = {}
        self.board_env_enabled = bool(board_cfg.get("enabled", False))
        self.board_spot_enabled = bool(board_cfg.get("spot_enabled", True))
        self.env_builder = WeeklyEnvironmentBuilder(
            db_writer=self.db_writer,
            logger=self.logger,
            index_codes=self.index_codes,
            board_env_enabled=self.board_env_enabled,
            board_spot_enabled=self.board_spot_enabled,
            env_index_score_threshold=self.rule_config.env_index_score_threshold,
            weekly_soft_gate_strength_threshold=self.rule_config.weekly_soft_gate_strength_threshold,
        )

        self.repo = OpenMonitorRepository(self.db_writer.engine, self.logger, self.params)
        self.market_data = OpenMonitorMarketData(self.logger, self.params)
        self.env_service = OpenMonitorEnvService(
            self.repo,
            self.logger,
            self.params,
            self.env_builder,
            MarketIndicatorBuilder(
                env_builder=self.env_builder,
                logger=self.logger,
            ),
        )
        self.evaluator = OpenMonitorEvaluator(
            self.logger, self.params, self.rule_engine, self.rule_config, self.rules
        )

    def _calc_run_id(self, ts: dt.datetime) -> str:
        return calc_run_id(ts, self.params.run_id_minutes)

    def _build_run_params_json(self) -> str:
        payload = {
            "signal_lookback_days": self.params.signal_lookback_days,
            "index_code": self.params.index_code,
            "index_hist_lookback_days": self.params.index_hist_lookback_days,
            "quote_source": self.params.quote_source,
            "interval_minutes": self.params.interval_minutes,
            "run_id_minutes": self.params.run_id_minutes,
            "strategy_code": self.params.strategy_code,
            "ready_signals_view": self.params.ready_signals_view,
        }
        return json.dumps(payload, ensure_ascii=False, separators=(",", ":"))

    def build_and_persist_open_monitor_env(
        self,
        latest_trade_date: str,
        *,
        monitor_date: str | None = None,
        run_id: str | None = None,
        run_pk: int | None = None,
        checked_at: dt.datetime | None = None,
        allow_auto_compute: bool = False,
        fetch_index_live_quote: Callable[[], dict[str, Any]] | None = None,
    ) -> dict[str, Any] | None:
        return self.env_service.build_and_persist_open_monitor_env(
            latest_trade_date,
            monitor_date=monitor_date,
            run_id=run_id,
            run_pk=run_pk,
            checked_at=checked_at,
            allow_auto_compute=allow_auto_compute,
            fetch_index_live_quote=(
                self.market_data.fetch_index_live_quote
                if fetch_index_live_quote is None
                else fetch_index_live_quote
            ),
        )

    def load_open_monitor_env_context(
            self,
            monitor_date: str,
            run_pk: int | None = None,
    ) -> dict[str, Any] | None:
        return self.env_service.load_open_monitor_env_context(monitor_date, run_pk)

    def run(self, *, force: bool = False, checked_at: dt.datetime | None = None) -> None:
        """执行开盘监测。

        - 默认遵循 config.yaml: open_monitor.enabled。
        - 当 force=True 时，即便 enabled=false 也会执行（用于单独运行脚本）。
        """

        if (not force) and (not self.params.enabled):
            self.logger.info("open_monitor.enabled=false，跳过开盘监测。")
            return

        if force and (not self.params.enabled):
            self.logger.info("open_monitor.enabled=false，但 force=True，仍将执行开盘监测。")

        checked_at = checked_at or dt.datetime.now()
        monitor_date = self.repo.resolve_monitor_trade_date(checked_at)
        biz_ts = dt.datetime.combine(dt.date.fromisoformat(monitor_date), checked_at.time())

        # 将本次运行时上下文透传给行情层（用于补齐 live_trade_date）
        self.params = replace(self.params, checked_at=checked_at, monitor_date=monitor_date)
        self.repo.params = self.params
        self.market_data.params = self.params
        run_id = self._calc_run_id(biz_ts)

        latest_trade_date, signal_dates, signals = self.repo.load_recent_buy_signals()
        if latest_trade_date and (signals is not None) and (not signals.empty):
            codes = signals["code"].dropna().astype(str).unique().tolist()
            asof_df = self.repo.load_latest_indicators(latest_trade_date, codes)
            if asof_df is not None and not asof_df.empty:
                asof_df = asof_df.copy()
                asof_df["code"] = asof_df["code"].astype(str)
                asof_df = asof_df.rename(
                    columns={
                        "trade_date": "asof_trade_date",
                        "close": "asof_close",
                        "ma5": "asof_ma5",
                        "ma20": "asof_ma20",
                        "ma60": "asof_ma60",
                        "ma250": "asof_ma250",
                        "vol_ratio": "asof_vol_ratio",
                        "macd_hist": "asof_macd_hist",
                        "atr14": "asof_atr14",
                        "avg_volume_20": "asof_avg_volume_20",
                    }
                )
                signals = signals.merge(asof_df, on="code", how="left")
                if "avg_volume_20" in signals.columns and "asof_avg_volume_20" in signals.columns:
                    signals["avg_volume_20"] = signals["avg_volume_20"].fillna(
                        signals["asof_avg_volume_20"]
                    )
                    signals = signals.drop(columns=["asof_avg_volume_20"])
            if "asof_trade_date" not in signals.columns:
                signals["asof_trade_date"] = latest_trade_date
            else:
                signals["asof_trade_date"] = signals["asof_trade_date"].fillna(
                    latest_trade_date
                )

        run_id_norm = str(run_id or "").strip()
        run_stage = run_id_norm.split(" ", 1)[0] if " " in run_id_norm else ""
        if run_stage not in {"PREOPEN", "BREAK", "POSTCLOSE"}:
            run_stage = "INTRADAY"

        fetch_index_quote = None
        dedup_sig = None
        signals_sig = None
        index_sig = None
        if latest_trade_date and (signals is not None) and (not signals.empty):
            codes = signals["code"].dropna().astype(str).unique().tolist()
            signal_dates_sorted = sorted(
                [str(d) for d in (signal_dates or []) if str(d).strip()]
            )
            signals_payload = {
                "latest_trade_date": latest_trade_date,
                "signal_dates": signal_dates_sorted,
                "codes": sorted(codes),
            }
            signals_sig = make_snapshot_hash(signals_payload)

            index_live_quote = self.market_data.fetch_index_live_quote()
            if isinstance(index_live_quote, dict) and index_live_quote:
                fetch_index_quote = lambda: dict(index_live_quote)
                index_payload = {
                    "index_code": str(self.params.index_code or "").strip() or None,
                    "live_trade_date": index_live_quote.get("live_trade_date"),
                    "live_open": _to_float(index_live_quote.get("live_open") or index_live_quote.get("open")),
                    "live_high": _to_float(index_live_quote.get("live_high") or index_live_quote.get("high")),
                    "live_low": _to_float(index_live_quote.get("live_low") or index_live_quote.get("low")),
                    "live_latest": _to_float(index_live_quote.get("live_latest") or index_live_quote.get("latest")),
                    "live_pct_change": _to_float(
                        index_live_quote.get("live_pct_change")
                        or index_live_quote.get("pct_change")
                    ),
                    "live_volume": _to_float(
                        index_live_quote.get("live_volume") or index_live_quote.get("volume")
                    ),
                    "live_amount": _to_float(
                        index_live_quote.get("live_amount") or index_live_quote.get("amount")
                    ),
                }
                index_sig = make_snapshot_hash(index_payload)

            if index_sig:
                dedup_sig = make_snapshot_hash(
                    {
                        "dedup_stage": run_stage,
                        "signals_sig": signals_sig,
                        "index_sig": index_sig,
                    }
                )
                prev = self.repo.load_latest_run_context_by_stage(
                    monitor_date, stage=run_stage
                )
                if prev and prev.get("dedup_sig") == dedup_sig and prev.get("run_id"):
                    run_id = str(prev["run_id"])

        run_params_json = self._build_run_params_json()
        try:
            run_params = json.loads(run_params_json)
        except Exception:
            run_params = {"raw_params_json": run_params_json}
        if isinstance(run_params, dict):
            run_params.setdefault("dedup_stage", run_stage)
            if dedup_sig:
                run_params["dedup_sig"] = dedup_sig
            if signals_sig:
                run_params["signals_sig"] = signals_sig
            if index_sig:
                run_params["index_sig"] = index_sig

        run_pk = self.repo.ensure_run_context(
            monitor_date,
            run_id,
            checked_at=checked_at,
            triggered_at=checked_at,
            run_stage=run_stage,
            params_json=run_params,
        )
        if run_pk is None:
            self.logger.error("未获取 run_pk，无法继续执行开盘监测。")
            return

        if not latest_trade_date or signals.empty:
            return


        codes = signals["code"].dropna().astype(str).unique().tolist()
        self.logger.info("待监测标的数量：%s（信号日：%s）", len(codes), signal_dates)

        env_context = self.build_and_persist_open_monitor_env(
            latest_trade_date,
            monitor_date=monitor_date,
            run_id=run_id,
            run_pk=run_pk,
            checked_at=checked_at,
            allow_auto_compute=True,
            fetch_index_live_quote=fetch_index_quote,
        )
        if not env_context:
            self.logger.error(
                "未构建环境快照（monitor_date=%s, run_id=%s），本次开盘监测终止。",
                monitor_date,
                run_id,
            )
            return

        quotes = self.market_data.fetch_quotes(codes)
        if quotes is None or quotes.empty:
            self.logger.warning("未获取到任何实时行情，将输出 UNKNOWN 结果。")
        else:
            self.logger.info("实时行情已获取：%s 条", len(quotes))

        env_instruction = {
            "gate_status": env_context.get("env_final_gate_action"),
            "position_cap": env_context.get("env_final_cap_pct"),
            "reason": env_context.get("env_final_reason_json"),
        }
        reason_matrix = None
        if env_instruction.get("reason"):
            try:
                reason_payload = json.loads(env_instruction["reason"])
                if isinstance(reason_payload, dict):
                    reason_matrix = reason_payload.get("risk_emotion_matrix")
            except Exception:
                reason_matrix = None
        env_payload = {
            "env_final_gate_action": env_instruction.get("gate_status"),
            "env_final_cap_pct": env_instruction.get("position_cap"),
            "env_final_reason_json": env_instruction.get("reason"),
            "env_weekly_gate_action": env_context.get("weekly_gate_action")
            or env_context.get("weekly_gate_policy"),
            "weekly_risk_level": env_context.get("weekly_risk_level"),
            "weekly_scene_code": env_context.get("weekly_scene_code"),
            "index_score": env_context.get("index_score"),
            "regime": env_context.get("regime"),
            "regime_raw": env_context.get("regime_raw"),
            "position_hint": env_context.get("position_hint"),
        }
        env_final_gate_action = env_instruction.get("gate_status")
        self.logger.info(
            "已构建环境快照（monitor_date=%s, run_id=%s, gate=%s, cap=%s, regime=%s(raw=%s), weekly_risk=%s, matrix=%s）。",
            monitor_date,
            run_id,
            env_final_gate_action,
            env_instruction.get("position_cap"),
            env_context.get("regime"),
            env_context.get("regime_raw"),
            env_context.get("weekly_risk_level"),
            reason_matrix,
        )
        result = self.evaluator.evaluate(
            signals,
            quotes,
            env_payload,
            checked_at=checked_at,
            run_id=run_id,
            run_pk=run_pk,
            ready_signals_used=self.repo.ready_signals_used,
        )

        if result.empty:
            return
        ranked_df, rank_meta = self.evaluator.build_rank_frame(result, env_context)
        if rank_meta:
            self.logger.info(
                "展示排序权重(不影响 gate/action/入库)：market_weight=%.2f（%s） board_weight=%s stock_quality=%s",
                float(rank_meta.get("market_weight") or 0.0),
                str(rank_meta.get("market_note") or "-")[:120],
                str(rank_meta.get("board_weight_map") or "-")[:120],
                str(rank_meta.get("stock_quality_source") or "-")[:120],
            )

        summary = result["action"].value_counts(dropna=False).to_dict()
        self.logger.info("开盘监测结果统计：%s", summary)

        exec_df = ranked_df[ranked_df["action"] == "EXECUTE"].copy()
        gap_col = "live_gap_pct" if "live_gap_pct" in exec_df.columns else "gap_pct"
        exec_df[gap_col] = exec_df[gap_col].apply(_to_float)
        if "final_rank_score" in exec_df.columns:
            exec_df["final_rank_score"] = pd.to_numeric(exec_df["final_rank_score"], errors="coerce")
            exec_df = exec_df.sort_values(
                by=["final_rank_score", gap_col],
                ascending=[False, True],
            )
        else:
            exec_df = exec_df.sort_values(by=gap_col, ascending=True)
        top_n = min(30, len(exec_df))
        if top_n > 0:
            preview_cols = [
                "code",
                "name",
                "sig_close",
                "live_open",
                "live_latest",
                gap_col,
                "board_status",
                "signal_strength",
                "final_rank_score",
                "action_reason",
            ]
            preview_cols = [c for c in preview_cols if c in exec_df.columns]
            preview = exec_df[preview_cols].head(top_n)
            preview_disp = preview.copy()
            if gap_col.endswith("_pct") and gap_col in preview_disp.columns:

                def _fmt_pct(v):
                    try:
                        fv = float(v)
                    except Exception:
                        return ""
                    if math.isnan(fv):
                        return ""
                    return f"{fv * 100:.3f}%"

                preview_disp[gap_col] = preview_disp[gap_col].apply(_fmt_pct)
            self.logger.info(
                "可执行清单 Top%s（按 final_rank_score 优先，其次 gap）：\n%s",
                top_n,
                preview_disp.to_string(index=False),
            )

        wait_df = ranked_df[ranked_df["action"] == "WAIT"].copy()
        wait_df[gap_col] = wait_df[gap_col].apply(_to_float)
        if "final_rank_score" in wait_df.columns:
            wait_df["final_rank_score"] = pd.to_numeric(wait_df["final_rank_score"], errors="coerce")
            wait_df = wait_df.sort_values(
                by=["final_rank_score", gap_col],
                ascending=[False, True],
            )
        else:
            wait_df = wait_df.sort_values(by=gap_col, ascending=True)
        wait_top = min(10, len(wait_df))
        if wait_top > 0:
            wait_cols = [
                "code",
                "name",
                "sig_close",
                "live_open",
                "live_latest",
                gap_col,
                "board_status",
                "signal_strength",
                "final_rank_score",
                "status_reason",
            ]
            wait_cols = [c for c in wait_cols if c in wait_df.columns]
            wait_preview = wait_df[wait_cols].head(wait_top)
            wait_preview_disp = wait_preview.copy()
            if gap_col.endswith("_pct") and gap_col in wait_preview_disp.columns:

                def _fmt_pct(v):
                    try:
                        fv = float(v)
                    except Exception:
                        return ""
                    if math.isnan(fv):
                        return ""
                    return f"{fv * 100:.3f}%"

                wait_preview_disp[gap_col] = wait_preview_disp[gap_col].apply(_fmt_pct)
            self.logger.info(
                "WAIT 观察清单 Top%s（按 final_rank_score 优先，其次 gap）：\n%s",
                wait_top,
                wait_preview_disp.to_string(index=False),
            )

        self.repo.persist_results(result)
        export_df = self.repo.load_open_monitor_view_data(monitor_date, run_pk)
        if export_df.empty:
            export_df = result
        self.evaluator.export_csv(export_df)

================================================================================
FILE: ashare/open_monitor_env.py
================================================================================

"""open_monitor 环境与快照服务。"""

from __future__ import annotations

import datetime as dt
from typing import Any, Callable

import pandas as pd

from .open_monitor_repo import calc_run_id, make_snapshot_hash
from .utils.convert import to_float as _to_float


def derive_index_gate_action(regime: str | None, position_hint: float | None) -> str | None:
    regime_norm = str(regime or "").strip().upper() or None
    pos_hint_val = _to_float(position_hint)

    if regime_norm in {"BREAKDOWN", "BEAR_CONFIRMED"}:
        return "STOP"

    if regime_norm == "RISK_OFF":
        return "WAIT"

    if regime_norm == "PULLBACK":
        if pos_hint_val is not None and pos_hint_val <= 0.3:
            return "WAIT"
        return "ALLOW"

    if regime_norm == "RISK_ON":
        return "ALLOW"

    if pos_hint_val is not None:
        if pos_hint_val <= 0:
            return "STOP"
        if pos_hint_val < 0.3:
            return "WAIT"
        return "ALLOW"

    return None


def _parse_weekly_key_levels(levels_str: str | None) -> dict[str, float]:
    if not levels_str:
        return {}
    levels: dict[str, float] = {}
    for token in str(levels_str).split(";"):
        token = token.strip()
        if not token or "=" not in token:
            continue
        key, value = token.split("=", 1)
        key = key.strip().lower()
        level = _to_float(value)
        if key and level is not None:
            levels[key] = level
    return levels


class OpenMonitorEnvService:
    """负责环境快照构建与加载的服务层。"""

    def __init__(self, repo, logger, params, env_builder, indicator_builder) -> None:
        self.repo = repo
        self.logger = logger
        self.params = params
        self.env_builder = env_builder
        self.indicator_builder = indicator_builder

    def load_open_monitor_env_context(
        self,
        monitor_date: str,
        run_pk: int | None = None,
    ) -> dict[str, Any] | None:
        return self._load_open_monitor_env_context(monitor_date, run_pk)

    def _load_open_monitor_env_context(
        self, monitor_date: str, run_pk: int | None = None
    ) -> dict[str, Any] | None:
        df = self.repo.load_open_monitor_env_row(monitor_date, run_pk)
        if df is None or df.empty:
            return None

        row = df.iloc[0]
        env_view_row = self.repo.load_open_monitor_env_view_row(monitor_date, run_pk)
        weekly_asof = row.get("env_weekly_asof_trade_date")
        daily_asof = row.get("env_daily_asof_trade_date")
        weekly_indicator = (
            self.repo.load_weekly_indicator(str(weekly_asof)) if weekly_asof else {}
        )
        benchmark_code = str(self.params.index_code or "sh.000001").strip() or "sh.000001"
        daily_env = (
            self.repo.load_daily_market_env(
                asof_trade_date=str(daily_asof),
                benchmark_code=benchmark_code,
            )
            if daily_asof
            else None
        )
        weekly_scenario = {
            "weekly_asof_trade_date": weekly_indicator.get("weekly_asof_trade_date")
            or weekly_asof,
            "weekly_risk_level": weekly_indicator.get("weekly_risk_level"),
            "weekly_scene_code": weekly_indicator.get("weekly_scene_code"),
            "weekly_phase": weekly_indicator.get("weekly_phase"),
            "weekly_gate_policy": weekly_indicator.get("weekly_gate_policy"),
            "weekly_gate_action": weekly_indicator.get("weekly_gate_policy"),
            "weekly_structure_status": weekly_indicator.get("weekly_structure_status"),
            "weekly_pattern_status": weekly_indicator.get("weekly_pattern_status"),
            "weekly_plan_a_exposure_cap": weekly_indicator.get("weekly_plan_a_exposure_cap"),
            "weekly_key_levels_str": weekly_indicator.get("weekly_key_levels_str"),
            "weekly_zone_id": weekly_indicator.get("weekly_zone_id"),
            "weekly_zone_score": weekly_indicator.get("weekly_zone_score"),
            "weekly_exp_return_bucket": weekly_indicator.get("weekly_exp_return_bucket"),
            "weekly_zone_reason": weekly_indicator.get("weekly_zone_reason"),
            "weekly_money_proxy": weekly_indicator.get("weekly_money_proxy"),
            "weekly_tags": weekly_indicator.get("weekly_tags"),
            "weekly_note": weekly_indicator.get("weekly_note"),
            "weekly_plan_json": weekly_indicator.get("weekly_plan_json"),
        }

        index_snapshot_hash = row.get("env_index_snapshot_hash")
        index_snapshot = {"env_index_snapshot_hash": index_snapshot_hash}
        if isinstance(env_view_row, dict):
            index_fields = [
                "env_index_code",
                "env_index_asof_trade_date",
                "env_index_live_trade_date",
                "env_index_asof_close",
                "env_index_asof_ma20",
                "env_index_asof_ma60",
                "env_index_asof_macd_hist",
                "env_index_asof_atr14",
                "env_index_live_open",
                "env_index_live_high",
                "env_index_live_low",
                "env_index_live_latest",
                "env_index_live_pct_change",
                "env_index_live_volume",
                "env_index_live_amount",
                "env_index_dev_ma20_atr",
                "env_index_gate_action",
                "env_index_gate_reason",
                "env_index_position_cap",
            ]
            index_snapshot.update(
                {key: env_view_row.get(key) for key in index_fields}
            )

        env_context: dict[str, Any] = {
            "weekly_scenario": weekly_scenario,
            "weekly_asof_trade_date": weekly_scenario.get("weekly_asof_trade_date"),
            "weekly_risk_level": weekly_scenario.get("weekly_risk_level"),
            "weekly_scene_code": weekly_scenario.get("weekly_scene_code"),
            "weekly_phase": weekly_scenario.get("weekly_phase"),
            "weekly_gate_policy": weekly_scenario.get("weekly_gate_policy"),
            "weekly_gate_action": weekly_scenario.get("weekly_gate_action"),
            "weekly_zone_id": weekly_scenario.get("weekly_zone_id"),
            "weekly_zone_score": weekly_scenario.get("weekly_zone_score"),
            "weekly_exp_return_bucket": weekly_scenario.get("weekly_exp_return_bucket"),
            "weekly_zone_reason": weekly_scenario.get("weekly_zone_reason"),
            "regime": (daily_env or {}).get("regime")
            or (env_view_row or {}).get("env_regime"),
            "regime_raw": (daily_env or {}).get("regime_raw")
            or (env_view_row or {}).get("env_regime_raw")
            or (env_view_row or {}).get("env_regime"),
            "index_score": (daily_env or {}).get("score")
            or (env_view_row or {}).get("env_index_score"),
            "position_hint": (daily_env or {}).get("position_hint")
            or (env_view_row or {}).get("env_position_hint"),
            "effective_position_hint": (daily_env or {}).get("position_hint")
            or (env_view_row or {}).get("env_position_hint"),
            "run_pk": row.get("run_pk"),
            "daily_asof_trade_date": (daily_env or {}).get("asof_trade_date")
            or daily_asof,
            "cycle_phase": (daily_env or {}).get("cycle_phase"),
            "daily_ma20": (daily_env or {}).get("ma20"),
            "daily_ma60": (daily_env or {}).get("ma60"),
            "daily_ma250": (daily_env or {}).get("ma250"),
            "daily_macd_hist": (daily_env or {}).get("macd_hist"),
            "daily_atr14": (daily_env or {}).get("atr14"),
            "daily_dev_ma20_atr": (daily_env or {}).get("dev_ma20_atr"),
            "daily_bb_mid": (daily_env or {}).get("bb_mid"),
            "daily_bb_upper": (daily_env or {}).get("bb_upper"),
            "daily_bb_lower": (daily_env or {}).get("bb_lower"),
            "daily_bb_width": (daily_env or {}).get("bb_width"),
            "daily_bb_pos": (daily_env or {}).get("bb_pos"),
            "daily_zone_id": (daily_env or {}).get("daily_zone_id"),
            "daily_zone_score": (daily_env or {}).get("daily_zone_score"),
            "daily_cap_multiplier": (daily_env or {}).get("daily_cap_multiplier"),
            "daily_zone_reason": (daily_env or {}).get("daily_zone_reason"),
            "breadth_pct_above_ma20": (daily_env or {}).get("breadth_pct_above_ma20"),
            "breadth_pct_above_ma60": (daily_env or {}).get("breadth_pct_above_ma60"),
            "breadth_risk_off_ratio": (daily_env or {}).get("breadth_risk_off_ratio"),
            "dispersion_score": (daily_env or {}).get("dispersion_score"),
            "env_index_snapshot_hash": index_snapshot_hash,
            "env_final_gate_action": row.get("env_final_gate_action"),
            "env_final_cap_pct": row.get("env_final_cap_pct"),
            "env_final_reason_json": row.get("env_final_reason_json"),
            "env_live_override_action": row.get("env_live_override_action"),
            "env_live_cap_multiplier": row.get("env_live_cap_multiplier"),
            "env_live_event_tags": row.get("env_live_event_tags"),
            "env_live_reason": row.get("env_live_reason"),
        }
        env_context["index_intraday"] = index_snapshot
        if not env_context.get("regime_raw"):
            env_context["regime_raw"] = env_context.get("regime")

        return env_context

    def resolve_env_weekly_gate_policy(
        self, env_context: dict[str, Any] | None
    ) -> str | None:
        return self.env_builder.resolve_env_weekly_gate_policy(env_context)

    def load_index_trend(self, latest_trade_date: str) -> dict[str, Any]:
        return self.env_builder.load_index_trend(latest_trade_date)

    def load_index_weekly_channel(self, latest_trade_date: str) -> dict[str, Any]:
        return self.env_builder.load_index_weekly_channel(latest_trade_date)

    def build_weekly_scenario(
        self, weekly_payload: dict[str, Any], index_trend: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        return self.env_builder.build_weekly_scenario(weekly_payload, index_trend)

    def build_environment_context(
        self,
        latest_trade_date: str,
        *,
        checked_at: dt.datetime | None = None,
        allow_auto_compute: bool = False,
    ) -> dict[str, Any]:
        expected_weekly_asof, _ = self.env_builder.resolve_latest_closed_week_end(
            latest_trade_date
        )
        weekly_indicator = self.repo.load_weekly_indicator(expected_weekly_asof) or {}
        if not weekly_indicator:
            if not allow_auto_compute:
                raise RuntimeError(
                    f"周线环境缺失（weekly_asof={expected_weekly_asof}），已禁止自动补算。"
                )
            weekly_rows = self.indicator_builder.compute_weekly_indicator(
                latest_trade_date, checked_at=checked_at
            )
            self.repo.upsert_weekly_indicator(weekly_rows)
            weekly_indicator = self.repo.load_weekly_indicator(expected_weekly_asof) or {}

        benchmark_code = str(self.params.index_code or "sh.000001").strip() or "sh.000001"
        daily_env = self.repo.load_daily_market_env(
            asof_trade_date=latest_trade_date, benchmark_code=benchmark_code
        )
        if not daily_env:
            if not allow_auto_compute:
                raise RuntimeError(
                    f"日线环境缺失（asof_trade_date={latest_trade_date}），已禁止自动补算。"
                )
            start_date = dt.date.fromisoformat(latest_trade_date)
            daily_rows = self.indicator_builder.compute_daily_indicators(
                start_date, start_date
            )
            daily_rows = self.repo.attach_cycle_phase_from_weekly(daily_rows)
            self.repo.upsert_daily_market_env(daily_rows)
            daily_env = self.repo.load_daily_market_env(
                asof_trade_date=latest_trade_date, benchmark_code=benchmark_code
            )

        weekly_scenario = {
            "weekly_asof_trade_date": weekly_indicator.get("weekly_asof_trade_date")
            or expected_weekly_asof,
            "weekly_scene_code": weekly_indicator.get("weekly_scene_code"),
            "weekly_phase": weekly_indicator.get("weekly_phase"),
            "weekly_structure_status": weekly_indicator.get("weekly_structure_status"),
            "weekly_pattern_status": weekly_indicator.get("weekly_pattern_status"),
            "weekly_risk_score": weekly_indicator.get("weekly_risk_score"),
            "weekly_risk_level": weekly_indicator.get("weekly_risk_level"),
            "weekly_gate_policy": weekly_indicator.get("weekly_gate_policy"),
            "weekly_gate_action": weekly_indicator.get("weekly_gate_policy"),
            "weekly_plan_a_exposure_cap": weekly_indicator.get("weekly_plan_a_exposure_cap"),
            "weekly_key_levels_str": weekly_indicator.get("weekly_key_levels_str"),
            "weekly_zone_id": weekly_indicator.get("weekly_zone_id"),
            "weekly_zone_score": weekly_indicator.get("weekly_zone_score"),
            "weekly_exp_return_bucket": weekly_indicator.get("weekly_exp_return_bucket"),
            "weekly_zone_reason": weekly_indicator.get("weekly_zone_reason"),
            "weekly_money_proxy": weekly_indicator.get("weekly_money_proxy"),
            "weekly_tags": weekly_indicator.get("weekly_tags"),
            "weekly_note": weekly_indicator.get("weekly_note"),
            "weekly_plan_json": weekly_indicator.get("weekly_plan_json"),
        }
        weekly_gate_policy = weekly_indicator.get("weekly_gate_policy")
        env_context = {
            "weekly_scenario": weekly_scenario,
            "weekly_asof_trade_date": weekly_scenario.get("weekly_asof_trade_date"),
            "weekly_risk_score": weekly_scenario.get("weekly_risk_score"),
            "weekly_risk_level": weekly_scenario.get("weekly_risk_level"),
            "weekly_scene_code": weekly_scenario.get("weekly_scene_code"),
            "weekly_phase": weekly_scenario.get("weekly_phase"),
            "weekly_structure_status": weekly_scenario.get("weekly_structure_status"),
            "weekly_pattern_status": weekly_scenario.get("weekly_pattern_status"),
            "weekly_plan_a_exposure_cap": weekly_scenario.get("weekly_plan_a_exposure_cap"),
            "weekly_key_levels_str": weekly_scenario.get("weekly_key_levels_str"),
            "weekly_money_proxy": weekly_scenario.get("weekly_money_proxy"),
            "weekly_tags": weekly_scenario.get("weekly_tags"),
            "weekly_note": weekly_scenario.get("weekly_note"),
            "weekly_gate_policy": weekly_gate_policy,
            "weekly_gate_action": weekly_gate_policy,
            "weekly_zone_id": weekly_scenario.get("weekly_zone_id"),
            "weekly_zone_score": weekly_scenario.get("weekly_zone_score"),
            "weekly_exp_return_bucket": weekly_scenario.get("weekly_exp_return_bucket"),
            "weekly_zone_reason": weekly_scenario.get("weekly_zone_reason"),
            "daily_asof_trade_date": daily_env.get("asof_trade_date")
            if daily_env
            else latest_trade_date,
            "index_score": _to_float(daily_env.get("score"))
            if daily_env
            else None,
            "regime": daily_env.get("regime") if daily_env else None,
            "regime_raw": daily_env.get("regime_raw") if daily_env else None,
            "position_hint": _to_float(daily_env.get("position_hint"))
            if daily_env
            else None,
            "effective_position_hint": _to_float(daily_env.get("position_hint"))
            if daily_env
            else None,
            "cycle_phase": daily_env.get("cycle_phase") if daily_env else None,
            "daily_ma20": _to_float(daily_env.get("ma20")) if daily_env else None,
            "daily_ma60": _to_float(daily_env.get("ma60")) if daily_env else None,
            "daily_ma250": _to_float(daily_env.get("ma250")) if daily_env else None,
            "daily_macd_hist": _to_float(daily_env.get("macd_hist")) if daily_env else None,
            "daily_atr14": _to_float(daily_env.get("atr14")) if daily_env else None,
            "daily_dev_ma20_atr": _to_float(daily_env.get("dev_ma20_atr"))
            if daily_env
            else None,
            "daily_bb_mid": _to_float(daily_env.get("bb_mid")) if daily_env else None,
            "daily_bb_upper": _to_float(daily_env.get("bb_upper")) if daily_env else None,
            "daily_bb_lower": _to_float(daily_env.get("bb_lower")) if daily_env else None,
            "daily_bb_width": _to_float(daily_env.get("bb_width")) if daily_env else None,
            "daily_bb_pos": _to_float(daily_env.get("bb_pos")) if daily_env else None,
            "daily_zone_id": daily_env.get("daily_zone_id") if daily_env else None,
            "daily_zone_score": daily_env.get("daily_zone_score") if daily_env else None,
            "daily_cap_multiplier": _to_float(daily_env.get("daily_cap_multiplier"))
            if daily_env
            else None,
            "daily_zone_reason": daily_env.get("daily_zone_reason") if daily_env else None,
            "breadth_pct_above_ma20": _to_float(daily_env.get("breadth_pct_above_ma20"))
            if daily_env
            else None,
            "breadth_pct_above_ma60": _to_float(daily_env.get("breadth_pct_above_ma60"))
            if daily_env
            else None,
            "breadth_risk_off_ratio": _to_float(daily_env.get("breadth_risk_off_ratio"))
            if daily_env
            else None,
            "dispersion_score": _to_float(daily_env.get("dispersion_score"))
            if daily_env
            else None,
        }
        if not env_context.get("regime_raw"):
            env_context["regime_raw"] = env_context.get("regime")
        if not weekly_gate_policy:
            weekly_gate_policy = self.env_builder.resolve_env_weekly_gate_policy(env_context)
            env_context["weekly_gate_policy"] = weekly_gate_policy
            env_context["weekly_gate_action"] = weekly_gate_policy
        self.env_builder._finalize_env_directives(
            env_context, weekly_gate_policy=weekly_gate_policy
        )

        # 补充板块强弱映射：用于展示/排序的 board_status（不影响 gate/action/入库）
        board_map: dict[str, dict[str, Any]] = {}
        try:
            from .open_monitor_eval import OpenMonitorEvaluator

            board_strength = self.env_builder.load_board_spot_strength(
                latest_trade_date, checked_at
            )
            board_map = OpenMonitorEvaluator.build_board_map_from_strength(board_strength)
        except Exception:  # noqa: BLE001
            board_map = {}

        env_context["boards"] = board_map
        return env_context

    def build_and_persist_open_monitor_env(
        self,
        latest_trade_date: str,
        *,
        monitor_date: str | None = None,
        run_id: str | None = None,
        run_pk: int | None = None,
        checked_at: dt.datetime | None = None,
        allow_auto_compute: bool = False,
        fetch_index_live_quote: Callable[[], dict[str, Any]] | None = None,
    ) -> dict[str, Any] | None:
        if checked_at is None:
            checked_at = dt.datetime.now()
        if monitor_date is None:
            monitor_date = checked_at.date().isoformat()
        if run_id is None:
            run_id = calc_run_id(checked_at, self.params.run_id_minutes)
        run_id_norm = str(run_id or "").strip()
        run_stage = run_id_norm.split(" ", 1)[0] if " " in run_id_norm else ""
        if run_stage not in {"PREOPEN", "BREAK", "POSTCLOSE"}:
            run_stage = "INTRADAY"
        if run_pk is None:
            run_pk = self.repo.ensure_run_context(
                monitor_date,
                run_id,
                checked_at=checked_at,
                triggered_at=checked_at,
                run_stage=run_stage,
                params_json=None,
            )

        env_context = self.build_environment_context(
            latest_trade_date,
            checked_at=checked_at,
            allow_auto_compute=allow_auto_compute,
        )

        env_context, _, _ = self.attach_index_snapshot(
            latest_trade_date,
            monitor_date,
            run_id,
            checked_at,
            env_context,
            run_pk=run_pk,
            fetch_index_live_quote=fetch_index_live_quote,
        )

        weekly_gate_policy = env_context.get("weekly_gate_policy")
        self.env_builder._finalize_env_directives(
            env_context, weekly_gate_policy=weekly_gate_policy
        )

        if run_pk is None:
            self.logger.error("环境快照缺少 run_pk，已跳过写入。")
            return env_context

        self.repo.persist_open_monitor_env(env_context, monitor_date, run_pk)

        return env_context

    def _build_index_env_snapshot(
        self,
        asof_indicators: dict[str, Any],
        live_quote: dict[str, Any],
        env_context: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        snapshot: dict[str, Any] = {}
        if isinstance(asof_indicators, dict):
            snapshot.update(
                {
                    "env_index_code": asof_indicators.get("index_code"),
                    "env_index_asof_trade_date": asof_indicators.get("asof_trade_date"),
                    "env_index_asof_close": _to_float(asof_indicators.get("asof_close")),
                    "env_index_asof_ma20": _to_float(asof_indicators.get("asof_ma20")),
                    "env_index_asof_ma60": _to_float(asof_indicators.get("asof_ma60")),
                    "env_index_asof_macd_hist": _to_float(
                        asof_indicators.get("asof_macd_hist")
                    ),
                    "env_index_asof_atr14": _to_float(asof_indicators.get("asof_atr14")),
                }
            )
        if isinstance(live_quote, dict):
            snapshot.update(
                {
                    "env_index_live_trade_date": live_quote.get("live_trade_date"),
                    "env_index_live_open": _to_float(live_quote.get("live_open") or live_quote.get("open")),
                    "env_index_live_high": _to_float(live_quote.get("live_high") or live_quote.get("high")),
                    "env_index_live_low": _to_float(live_quote.get("live_low") or live_quote.get("low")),
                    "env_index_live_latest": _to_float(
                        live_quote.get("live_latest") or live_quote.get("latest")
                    ),
                    "env_index_live_pct_change": _to_float(
                        live_quote.get("live_pct_change") or live_quote.get("pct_change")
                    ),
                    "env_index_live_volume": _to_float(
                        live_quote.get("live_volume") or live_quote.get("volume")
                    ),
                    "env_index_live_amount": _to_float(
                        live_quote.get("live_amount") or live_quote.get("amount")
                    ),
                }
            )

        asof_close = _to_float(snapshot.get("env_index_asof_close"))
        asof_ma20 = _to_float(snapshot.get("env_index_asof_ma20"))
        asof_atr14 = _to_float(snapshot.get("env_index_asof_atr14"))
        live_latest = _to_float(snapshot.get("env_index_live_latest"))

        dev_ma20_atr = None
        if live_latest is not None and asof_ma20 is not None and asof_atr14:
            dev_ma20_atr = (live_latest - asof_ma20) / asof_atr14

        snapshot["env_index_dev_ma20_atr"] = dev_ma20_atr

        index_score = None
        regime = None
        position_hint = None
        if env_context and isinstance(env_context, dict):
            index_score = _to_float(env_context.get("index_score"))
            regime = env_context.get("regime")
            position_hint = env_context.get("position_hint")
        snapshot["env_index_score"] = index_score
        gate_action = derive_index_gate_action(regime, position_hint)
        gate_reason = f"regime={regime} pos_hint={position_hint}"
        snapshot["env_index_gate_action"] = gate_action
        snapshot["env_index_gate_reason"] = gate_reason
        snapshot["env_index_position_cap"] = _to_float(position_hint)

        return snapshot

    @staticmethod
    def _merge_live_actions(actions: list[str]) -> str:
        severity = {"EXIT": 3, "PAUSE": 2, "REDUCE": 1, "NONE": 0}
        if not actions:
            return "NONE"
        normalized = [str(action).strip().upper() for action in actions if action]
        if not normalized:
            return "NONE"
        normalized.sort(key=lambda x: severity.get(x, 0), reverse=True)
        return normalized[0]

    def evaluate_live_override(
        self, env_context: dict[str, Any], index_snapshot: dict[str, Any]
    ) -> dict[str, Any]:
        live_pct = _to_float(index_snapshot.get("env_index_live_pct_change"))
        live_latest = _to_float(index_snapshot.get("env_index_live_latest"))
        live_high = _to_float(index_snapshot.get("env_index_live_high"))
        live_low = _to_float(index_snapshot.get("env_index_live_low"))
        live_dev_ma20_atr = _to_float(index_snapshot.get("env_index_dev_ma20_atr"))
        daily_ma20 = _to_float(env_context.get("daily_ma20"))
        daily_atr14 = _to_float(env_context.get("daily_atr14"))
        daily_bb_lower = _to_float(env_context.get("daily_bb_lower"))
        daily_bb_upper = _to_float(env_context.get("daily_bb_upper"))
        daily_bb_width = _to_float(env_context.get("daily_bb_width"))
        weekly_key_levels_str = env_context.get("weekly_key_levels_str")
        if not weekly_key_levels_str:
            weekly_scenario = env_context.get("weekly_scenario") or {}
            if isinstance(weekly_scenario, dict):
                weekly_key_levels_str = weekly_scenario.get("weekly_key_levels_str")
        key_levels = _parse_weekly_key_levels(weekly_key_levels_str)

        actions: list[str] = []
        cap_candidates: list[float] = []
        events: list[str] = []
        unlock_gate = None
        upper_reason = None

        if live_pct is not None:
            if live_pct <= -3.0:
                actions.append("PAUSE")
                cap_candidates.append(0.0)
                events.append("INTRADAY_CRASH")
            elif live_pct <= -2.0:
                actions.append("REDUCE")
                cap_candidates.append(0.25)
                events.append("INTRADAY_DROP")

        if live_latest is not None and daily_bb_lower is not None:
            if live_latest < daily_bb_lower:
                actions.append("PAUSE")
                cap_candidates.append(0.0)
                events.append("BB_BREAK_LOWER")

        if (
            live_latest is not None
            and daily_ma20 is not None
            and live_dev_ma20_atr is not None
            and live_latest < daily_ma20
            and live_dev_ma20_atr <= -1.2
        ):
            actions.append("REDUCE")
            cap_candidates.append(0.5)
            events.append("BREAK_MA20")

        if (
            live_latest is not None
            and daily_bb_upper is not None
            and daily_bb_width is not None
            and live_latest > daily_bb_upper
            and daily_bb_width >= 0.12
        ):
            actions.append("REDUCE")
            cap_candidates.append(0.5)
            events.append("BB_OVERHEAT")

        upper_level = key_levels.get("upper")
        if (
            upper_level is not None
            and live_high is not None
            and live_latest is not None
            and live_low is not None
        ):
            breakout_high_eps = self.params.live_breakout_high_eps
            breakout_latest_eps = self.params.live_breakout_latest_eps
            retest_pct = self.params.live_retest_pct
            retest_atr_mult = self.params.live_retest_atr_mult
            retest_atr_allowance = (
                daily_atr14 * retest_atr_mult if daily_atr14 is not None else 0.0
            )
            retest_pct_allowance = upper_level * retest_pct
            retest_allowance = max(retest_pct_allowance, retest_atr_allowance)
            retest_floor = upper_level - retest_allowance

            if live_high >= upper_level * (1 + breakout_high_eps):
                events.append("LIVE_TOUCH_UPPER")
            if live_latest >= upper_level * (1 + breakout_latest_eps):
                events.append("LIVE_BREAKOUT_HOLDING")
            if (
                live_high >= upper_level * (1 + breakout_high_eps)
                and live_low < retest_floor
            ):
                events.append("LIVE_RETEST_FAILED")

            if (
                live_high >= upper_level * (1 + breakout_high_eps)
                and live_latest >= upper_level * (1 + breakout_latest_eps)
                and live_low >= retest_floor
            ):
                unlock_gate = "ALLOW_SMALL"
                events.append("LIVE_BREAKOUT_UPPER")
                events.append("LIVE_BREAKOUT_RETEST_HELD")

            upper_reason = (
                "upper={:.2f} high={:.2f} low={:.2f} latest={:.2f} "
                "hold_low>={:.2f}"
            ).format(upper_level, live_high, live_low, live_latest, retest_floor)

        action = self._merge_live_actions(actions)
        cap_multiplier = min(cap_candidates) if cap_candidates else 1.0
        if upper_reason:
            reason = upper_reason[:255]
        else:
            reason = ";".join(events)[:255] if events else None
        return {
            "env_live_override_action": action,
            "env_live_cap_multiplier": cap_multiplier,
            "env_live_event_tags": ";".join(events)[:255] if events else None,
            "env_live_reason": reason,
            "env_live_unlock_gate": unlock_gate,
        }

    def attach_index_snapshot(
        self,
        latest_trade_date: str,
        monitor_date: str,
        run_id: str | None,
        checked_at: dt.datetime,
        env_context: dict[str, Any] | None,
        *,
        run_pk: int | None = None,
        fetch_index_live_quote: Callable[[], dict[str, Any]] | None,
    ) -> tuple[dict[str, Any] | None, dict[str, Any], str | None]:
        ctx = env_context or {}

        # run_id 用作 join key：保持非空（为空时用 checked_at 的 HH:MM 兜底）
        rid = str(run_id or "").strip()
        if not rid:
            rid = checked_at.strftime("%H:%M")
            self.logger.warning("env index snapshot: empty run_id -> fallback to %s", rid)
        run_id = rid
        index_env_snapshot: dict[str, Any] = {}
        env_index_snapshot_hash: str | None = None

        if latest_trade_date:
            idx = ctx.get("index") if isinstance(ctx, dict) else {}
            if not isinstance(idx, dict):
                idx = {}
            index_score = _to_float(ctx.get("index_score"))
            if index_score is None:
                index_score = _to_float(idx.get("score"))

            regime = ctx.get("regime") or idx.get("regime")

            position_hint = _to_float(ctx.get("position_hint"))
            if position_hint is None:
                position_hint = _to_float(idx.get("position_hint"))
            if regime is not None:
                regime = str(regime).strip() or None

            ctx["index_score"] = index_score
            ctx["regime"] = regime
            ctx["position_hint"] = position_hint

            if regime is None or position_hint is None:
                self.logger.warning(
                    "指数环境缺失（regime/position_hint），已跳过指数快照构建。 index_score=%s regime=%s position_hint=%s",
                    index_score,
                    regime,
                    position_hint,
                )
                return ctx or env_context, {}, None

            asof_indicators = self.repo.load_index_history(latest_trade_date)
            live_quote = fetch_index_live_quote() if fetch_index_live_quote else {}
            if isinstance(live_quote, dict):
                live_trade_date = live_quote.get("live_trade_date")
                if pd.isna(live_trade_date) or (
                    isinstance(live_trade_date, str) and not live_trade_date.strip()
                ):
                    live_quote["live_trade_date"] = monitor_date
            index_env_snapshot = self._build_index_env_snapshot(
                asof_indicators,
                live_quote,
                env_context=ctx,
            )

        if index_env_snapshot:
            index_snapshot_payload = {
                "monitor_date": monitor_date,
                "checked_at": checked_at,
                "run_id": run_id,
                "index_code": index_env_snapshot.get("env_index_code"),
                "asof_trade_date": index_env_snapshot.get("env_index_asof_trade_date"),
                "live_trade_date": index_env_snapshot.get("env_index_live_trade_date"),
                "asof_close": _to_float(index_env_snapshot.get("env_index_asof_close")),
                "asof_ma20": _to_float(index_env_snapshot.get("env_index_asof_ma20")),
                "asof_ma60": _to_float(index_env_snapshot.get("env_index_asof_ma60")),
                "asof_macd_hist": _to_float(index_env_snapshot.get("env_index_asof_macd_hist")),
                "asof_atr14": _to_float(index_env_snapshot.get("env_index_asof_atr14")),
                "live_open": _to_float(index_env_snapshot.get("env_index_live_open")),
                "live_high": _to_float(index_env_snapshot.get("env_index_live_high")),
                "live_low": _to_float(index_env_snapshot.get("env_index_live_low")),
                "live_latest": _to_float(index_env_snapshot.get("env_index_live_latest")),
                "live_pct_change": _to_float(index_env_snapshot.get("env_index_live_pct_change")),
                "live_volume": _to_float(index_env_snapshot.get("env_index_live_volume")),
                "live_amount": _to_float(index_env_snapshot.get("env_index_live_amount")),
                "dev_ma20_atr": _to_float(index_env_snapshot.get("env_index_dev_ma20_atr")),
                "gate_action": index_env_snapshot.get("env_index_gate_action"),
                "gate_reason": index_env_snapshot.get("env_index_gate_reason"),
                "position_cap": _to_float(index_env_snapshot.get("env_index_position_cap")),
            }
            index_snapshot_payload["snapshot_hash"] = make_snapshot_hash(index_snapshot_payload)
            env_index_snapshot_hash = index_snapshot_payload["snapshot_hash"]
            index_env_snapshot["env_index_snapshot_hash"] = env_index_snapshot_hash
            ctx["index_intraday"] = index_env_snapshot
            live_override = self.evaluate_live_override(ctx, index_env_snapshot)
            ctx.update(live_override)
            if index_env_snapshot.get("env_index_code"):
                if run_pk is None:
                    self.logger.warning("指数行情快照缺少 run_pk，已跳过写入。")
                else:
                    quote_payload = {
                        "monitor_date": monitor_date,
                        "run_pk": run_pk,
                        "run_id": run_id,
                        "code": index_env_snapshot.get("env_index_code"),
                        "live_trade_date": index_env_snapshot.get("env_index_live_trade_date"),
                        "live_open": _to_float(index_env_snapshot.get("env_index_live_open")),
                        "live_high": _to_float(index_env_snapshot.get("env_index_live_high")),
                        "live_low": _to_float(index_env_snapshot.get("env_index_live_low")),
                        "live_latest": _to_float(index_env_snapshot.get("env_index_live_latest")),
                        "live_volume": _to_float(index_env_snapshot.get("env_index_live_volume")),
                        "live_amount": _to_float(index_env_snapshot.get("env_index_live_amount")),
                    }
                    quote_df = pd.DataFrame([quote_payload])
                    self.repo.persist_quote_snapshots(quote_df)

        if index_env_snapshot:
            gate_action = index_env_snapshot.get("env_index_gate_action")
            gate_reason = index_env_snapshot.get("env_index_gate_reason") or "-"
            live_pct = _to_float(index_env_snapshot.get("env_index_live_pct_change"))
            dev_ma20_atr = _to_float(index_env_snapshot.get("env_index_dev_ma20_atr"))
            regime_eff = None
            regime_raw = None
            if isinstance(ctx, dict):
                regime_eff = ctx.get("regime")
                regime_raw = ctx.get("regime_raw") or regime_eff
            self.logger.info(
                "指数环境快照：%s asof=%s live=%s pct=%.2f%% dev_ma20_atr=%.2f index_gate=%s regime=%s(raw=%s) reason=%s",
                index_env_snapshot.get("env_index_code"),
                index_env_snapshot.get("env_index_asof_trade_date"),
                index_env_snapshot.get("env_index_live_trade_date"),
                live_pct if live_pct is not None else 0.0,
                dev_ma20_atr if dev_ma20_atr is not None else 0.0,
                gate_action,
                regime_eff,
                regime_raw,
                gate_reason,
            )

        return ctx or env_context, index_env_snapshot, env_index_snapshot_hash

    def log_weekly_scenario(self, env_context: dict[str, Any] | None) -> None:
        weekly_scenario = (
            env_context.get("weekly_scenario", {}) if isinstance(env_context, dict) else {}
        )
        if not isinstance(weekly_scenario, dict):
            return

        confirm_tags = ",".join(weekly_scenario.get("weekly_confirm_tags", []) or [])

        asof_trade_date = weekly_scenario.get("weekly_asof_trade_date")
        current_week_closed = weekly_scenario.get("weekly_current_week_closed")

        risk_level = weekly_scenario.get("weekly_risk_level")
        risk_score = _to_float(weekly_scenario.get("weekly_risk_score"))
        risk_score = risk_score or 0.0

        scene_code = weekly_scenario.get("weekly_scene_code")

        bias = weekly_scenario.get("weekly_bias")

        status = weekly_scenario.get("weekly_status")
        if status is None:
            status = (
                weekly_scenario.get("weekly_pattern_status")
                or weekly_scenario.get("weekly_structure_status")
            )

        key_levels_str = weekly_scenario.get("weekly_key_levels_str")
        key_levels_short = str(key_levels_str or "")[:120]

        if not confirm_tags:
            tags_str = weekly_scenario.get("weekly_tags")
            if isinstance(tags_str, str) and tags_str.strip():
                confirm_tags = ",".join(
                    [t.strip() for t in tags_str.replace(";", ",").split(",") if t.strip()]
                )

        self.logger.info(
            "周线情景：asof=%s current_week_closed=%s risk=%s(%.1f) scene=%s bias=%s status=%s levels=%s 确认标签=%s",
            asof_trade_date,
            current_week_closed,
            risk_level,
            risk_score,
            scene_code,
            bias,
            status,
            key_levels_short,
            confirm_tags,
        )
        weekly_note = str(weekly_scenario.get("weekly_note") or "").strip()
        if weekly_note:
            self.logger.info("周线备注：%s", weekly_note[:200])
        plan_a = str(weekly_scenario.get("weekly_plan_a") or "").strip()[:200]
        plan_b = str(weekly_scenario.get("weekly_plan_b") or "").strip()[:200]
        if plan_a:
            self.logger.info("周线 PlanA：%s", plan_a)
        if plan_b:
            self.logger.info("周线 PlanB：%s", plan_b)
        plan_a_if = weekly_scenario.get("weekly_plan_a_if")
        plan_b_if = weekly_scenario.get("weekly_plan_b_if")
        plan_a_confirm = weekly_scenario.get("weekly_plan_a_confirm")
        plan_b_recover = weekly_scenario.get("weekly_plan_b_recover_if")
        plan_tokens = [
            plan_a_if,
            weekly_scenario.get("weekly_plan_a_then"),
            plan_a_confirm,
            plan_b_if,
            plan_b_recover,
        ]
        if any(str(token or "").strip() for token in plan_tokens):
            self.logger.info(
                "周线 Plan tokens: A_if=%s A_then=%s A_confirm=%s B_if=%s B_recover=%s",
                str(plan_a_if or "")[:120],
                str(weekly_scenario.get("weekly_plan_a_then") or "")[:64],
                str(plan_a_confirm or "")[:64],
                str(plan_b_if or "")[:120],
                str(plan_b_recover or "")[:120],
            )

================================================================================
FILE: ashare/open_monitor_eval.py
================================================================================

"""open_monitor 评估与排名层。"""

from __future__ import annotations

import datetime as dt
import json
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any, List

import pandas as pd

from .config import get_section
from .open_monitor_repo import calc_run_id, make_snapshot_hash
from .open_monitor_rules import DecisionContext, MarketEnvironment, RuleEngine
from .utils.convert import to_float as _to_float


def merge_gate_actions(*actions: str | None) -> str | None:
    severity = {
        "STOP": 3,
        "WAIT": 2,
        "ALLOW_SMALL": 1,
        "ALLOW": 0,
        "NOT_APPLIED": -1,
        None: -1,
    }
    normalized = []
    for action in actions:
        if action is None:
            normalized.append((severity[None], None))
            continue
        action_norm = str(action).strip().upper()
        if action_norm == "GO":
            action_norm = "ALLOW"
        score = severity.get(action_norm, 0)
        normalized.append((score, action_norm))
    if not normalized:
        return None
    normalized.sort(key=lambda x: x[0], reverse=True)
    top_action = normalized[0][1]
    if top_action == "GO":
        return "ALLOW"
    return top_action


@dataclass(frozen=True)
class RunupMetrics:
    runup_ref_price: float | None
    runup_ref_source: str | None
    runup_from_sigclose: float | None
    runup_from_sigclose_atr: float | None


def compute_runup_metrics(
    sig_close: float | None,
    *,
    asof_close: float | None,
    live_high: float | None,
    sig_atr14: float | None,
    eps: float = 1e-6,
) -> RunupMetrics:
    live_high_val = _to_float(live_high)
    asof_close_val = _to_float(asof_close)
    sig_close_val = _to_float(sig_close)
    sig_atr_val = _to_float(sig_atr14)

    ref_price = None
    ref_source = None
    if live_high_val is not None and asof_close_val is not None:
        ref_price = max(live_high_val, asof_close_val)
        ref_source = "max(asof_close, live_high)"
    elif live_high_val is not None:
        ref_price = live_high_val
        ref_source = "live_high"
    elif asof_close_val is not None:
        ref_price = asof_close_val
        ref_source = "asof_close"

    runup_from_sigclose = None
    runup_from_sigclose_atr = None
    if ref_price is not None and sig_close_val is not None:
        runup_from_sigclose = ref_price - sig_close_val
        if sig_atr_val is not None:
            runup_from_sigclose_atr = runup_from_sigclose / max(sig_atr_val, eps)

    return RunupMetrics(
        runup_ref_price=ref_price,
        runup_ref_source=ref_source,
        runup_from_sigclose=runup_from_sigclose,
        runup_from_sigclose_atr=runup_from_sigclose_atr,
    )


def evaluate_runup_breach(
    metrics: RunupMetrics,
    *,
    runup_atr_max: float | None,
    runup_atr_tol: float,
    dev_ma20_atr: float | None = None,
    dev_ma20_atr_min: float | None = None,
) -> tuple[bool, str | None]:
    if runup_atr_max is None or metrics.runup_from_sigclose_atr is None:
        return False, None

    if dev_ma20_atr_min is not None:
        if dev_ma20_atr is None or dev_ma20_atr < dev_ma20_atr_min:
            return False, None

    threshold = runup_atr_max + runup_atr_tol
    if metrics.runup_from_sigclose_atr > threshold:
        ref_price_display = (
            f"{metrics.runup_ref_price:.2f}" if metrics.runup_ref_price is not None else "N/A"
        )
        ref_source = metrics.runup_ref_source or "unknown"
        reason = (
            f"runup_ref_source={ref_source}, runup_ref_price={ref_price_display}, "
            f"runup_from_sigclose_atr={metrics.runup_from_sigclose_atr:.3f} "
            f"> runup_atr_max={runup_atr_max:.3f}+tol={runup_atr_tol:.3f}"
        )
        return True, reason
    return False, None


class OpenMonitorEvaluator:
    """负责监测评估与排名组装。"""

    def __init__(self, logger, params, rule_engine: RuleEngine, rule_config, rules) -> None:
        self.logger = logger
        self.params = params
        self.rule_engine = rule_engine
        self.rule_config = rule_config
        self.rules = rules

    @staticmethod
    def _is_pullback_signal(signal_reason: str) -> bool:
        reason_text = str(signal_reason or "")
        lower = reason_text.lower()
        return ("回踩" in reason_text) and (("ma20" in lower) or ("ma 20" in lower))

    def _parse_market_environment(self, env_context: dict[str, Any] | None) -> MarketEnvironment:
        return MarketEnvironment.from_snapshot(env_context)

    def prepare_monitor_frame(
        self,
        signals: pd.DataFrame,
        quotes: pd.DataFrame,
    ) -> pd.DataFrame:
        strict_quotes = bool(getattr(self.params, "strict_quotes", True))
        if quotes is None or getattr(quotes, "empty", True):
            q = pd.DataFrame(
                columns=[
                    "code",
                    "live_open",
                    "live_high",
                    "live_low",
                    "live_latest",
                    "live_volume",
                    "live_amount",
                    "live_pct_change",
                    "live_gap_pct",
                    "live_intraday_vol_ratio",
                    "prev_close",
                ]
            )
        else:
            q = quotes.copy()
            if "code" not in q.columns:
                msg = "quotes 缺少 code 列"
                if strict_quotes:
                    raise RuntimeError(msg)
                self.logger.error("%s（strict_quotes=false，将跳过行情合并）", msg)
                q["code"] = ""
            q["code"] = q["code"].astype(str)
            required = [
                "live_open",
                "live_high",
                "live_low",
                "live_latest",
                "live_volume",
                "live_amount",
            ]
            missing = [c for c in required if c not in q.columns]
            if missing:
                msg = f"quotes 缺少统一行情列：{missing}"
                if strict_quotes:
                    raise RuntimeError(msg)
                self.logger.error("%s（strict_quotes=false，将补空列）", msg)
                for c in missing:
                    q[c] = None

        merged = signals.copy()
        merged["code"] = merged["code"].astype(str)
        merged = merged.merge(q, on="code", how="left", suffixes=("", "_q"))

        for col in [
            "asof_trade_date",
            "asof_close",
            "asof_ma5",
            "asof_ma20",
            "asof_ma60",
            "asof_ma250",
            "asof_vol_ratio",
            "asof_macd_hist",
            "asof_atr14",
            "asof_stop_ref",
        ]:
            if col not in merged.columns:
                merged[col] = None
        if "avg_volume_20" not in merged.columns:
            merged["avg_volume_20"] = None
        if "sig_date" in merged.columns:
            merged["asof_trade_date"] = merged["asof_trade_date"].where(
                pd.notna(merged["asof_trade_date"]), merged["sig_date"]
            )

        float_cols = [
            "sig_close",
            "sig_ma5",
            "sig_ma20",
            "sig_ma60",
            "sig_ma250",
            "sig_vol_ratio",
            "sig_macd_hist",
            "sig_kdj_k",
            "sig_kdj_d",
            "sig_atr14",
            "sig_stop_ref",
            "live_open",
            "live_latest",
            "live_high",
            "live_low",
            "live_pct_change",
            "live_volume",
            "live_amount",
            "prev_close",
        ]
        for col in float_cols:
            if col in merged.columns:
                merged[col] = merged.get(col).apply(_to_float)

        def _coalesce_numeric(target: str, fallback: str) -> None:
            if target not in merged.columns:
                merged[target] = None
            if fallback not in merged.columns:
                return
            left = pd.to_numeric(merged[target], errors="coerce")
            right = pd.to_numeric(merged[fallback], errors="coerce")
            merged[target] = left.fillna(right)

        _coalesce_numeric("asof_close", "sig_close")
        _coalesce_numeric("asof_ma5", "sig_ma5")
        _coalesce_numeric("asof_ma20", "sig_ma20")
        _coalesce_numeric("asof_ma60", "sig_ma60")
        _coalesce_numeric("asof_ma250", "sig_ma250")
        _coalesce_numeric("asof_vol_ratio", "sig_vol_ratio")
        _coalesce_numeric("asof_macd_hist", "sig_macd_hist")
        _coalesce_numeric("asof_atr14", "sig_atr14")
        _coalesce_numeric("asof_stop_ref", "sig_stop_ref")

        return merged

    def evaluate(
        self,
        signals: pd.DataFrame,
        quotes: pd.DataFrame,
        env_instruction: dict[str, Any] | None = None,
        *,
        checked_at: dt.datetime | None = None,
        run_id: str | None = None,
        run_pk: int | None = None,
        ready_signals_used: bool = False,
    ) -> pd.DataFrame:
        if signals.empty:
            return pd.DataFrame()

        checked_at_ts = checked_at or dt.datetime.now()
        run_id_val = run_id or calc_run_id(checked_at_ts, self.params.run_id_minutes)
        monitor_date = checked_at_ts.date().isoformat()
        run_id_norm = str(run_id_val or "").strip().upper()
        run_stage = run_id_norm.split(" ", 1)[0] if run_id_norm else ""
        if not ready_signals_used:
            self.logger.error(
                "未启用 ready_signals_view（严格模式），已终止评估（monitor_date=%s, run_id=%s）。",
                monitor_date,
                run_id_val,
            )
            return pd.DataFrame()

        env = self._parse_market_environment(env_instruction)

        if env.gate_action is None:
            self.logger.error(
                "环境快照缺少 env_final_gate_action，已终止评估（monitor_date=%s, run_id=%s）。",
                monitor_date,
                run_id_val,
            )
            return pd.DataFrame()
        if env.position_cap_pct is None:
            self.logger.error(
                "环境快照缺少 env_final_cap_pct，已终止评估（monitor_date=%s, run_id=%s）。",
                monitor_date,
                run_id_val,
            )
            return pd.DataFrame()

        merged = self.prepare_monitor_frame(signals, quotes)
        strategy_code = None
        if "strategy_code" in merged.columns:
            raw_strategy = merged["strategy_code"].dropna().astype(str)
            if not raw_strategy.empty:
                strategy_code = raw_strategy.iloc[0]
        if not strategy_code:
            strategy_code = str(getattr(self.params, "strategy_code", "") or "").strip() or None
        if "strategy_code" not in merged.columns:
            merged["strategy_code"] = strategy_code
        else:
            merged["strategy_code"] = merged["strategy_code"].fillna(strategy_code)

        def _resolve_ref_close(row: pd.Series) -> float | None:
            for key in ("prev_close", "sig_close"):
                val = _to_float(row.get(key))
                if val is not None and val > 0:
                    return val
            return None

        live_gap_list: List[float | None] = []
        live_pct_change_list: List[float | None] = []
        live_intraday_vol_list: List[float | None] = []
        dev_ma5_list: List[float | None] = []
        dev_ma20_list: List[float | None] = []
        dev_ma5_atr_list: List[float | None] = []
        dev_ma20_atr_list: List[float | None] = []
        runup_from_sigclose_list: List[float | None] = []
        runup_from_sigclose_atr_list: List[float | None] = []
        runup_ref_price_list: List[float | None] = []
        runup_ref_source_list: List[str | None] = []
        actions: List[str] = []
        action_reasons: List[str] = []
        states: List[str] = []
        status_reasons: List[str] = []
        signal_kinds: List[str] = []
        entry_exposure_caps: List[float | None] = []
        trade_stop_refs: List[float | None] = []
        sig_stop_refs: List[float | None] = []
        effective_stop_refs: List[float | None] = []
        rule_hits_json_list: List[str | None] = []
        summary_lines: List[str | None] = []
        signal_strength_list: List[float | None] = []
        strength_delta_list: List[float | None] = []
        strength_trend_list: List[str | None] = []
        strength_note_list: List[str | None] = []

        max_up = self.rule_config.max_gap_up_pct
        max_up_atr_mult = self.rule_config.max_gap_up_atr_mult
        max_down = self.rule_config.max_gap_down_pct
        min_vs_ma20 = self.rule_config.min_open_vs_ma20_pct
        pullback_min_vs_ma20 = self.rule_config.pullback_min_open_vs_ma20_pct
        limit_up_trigger = self.rule_config.limit_up_trigger_pct
        stop_atr_mult = self.rule_config.stop_atr_mult
        runup_atr_max = self.rule_config.runup_atr_max
        runup_atr_vol_mult = self.rule_config.runup_atr_vol_mult
        runup_atr_max_cap = self.rule_config.runup_atr_max_cap
        pullback_runup_atr_max = self.rule_config.pullback_runup_atr_max
        pullback_runup_dev_ma20_atr_min = self.rule_config.pullback_runup_dev_ma20_atr_min
        runup_atr_tol = self.rule_config.runup_atr_tol
        ma20_atr_tol_mult = self.rule_config.ma20_atr_tol_mult
        ma20_dyn_min_pct = self.rule_config.ma20_dyn_min_pct
        ma20_prewarn_buffer_pct = self.rule_config.ma20_prewarn_buffer_pct
        below_ma20_tol_pct = self.rule_config.below_ma20_tol_pct
        for _, row in merged.iterrows():
            sig_reason_text = str(row.get("sig_reason") or row.get("reason") or "")
            is_pullback = self._is_pullback_signal(sig_reason_text)
            sig_stop_ref = _to_float(row.get("sig_stop_ref"))
            sig_stop_refs.append(sig_stop_ref)

            price_open = _to_float(row.get("live_open"))
            price_latest = _to_float(row.get("live_latest"))
            price_now = price_open if price_open is not None else price_latest
            ref_close = _resolve_ref_close(row)
            if price_now is None:
                if run_stage == "PREOPEN" and ref_close is not None:
                    price_now = ref_close
                elif run_stage == "POSTCLOSE":
                    price_now = _to_float(row.get("live_latest")) or ref_close or _to_float(
                        row.get("sig_close")
                    )

            live_gap = None
            live_pct = None
            if ref_close is not None and ref_close > 0:
                gap_price = price_open if price_open is not None else price_latest
                if gap_price is not None:
                    live_gap = (gap_price - ref_close) / ref_close
                if price_latest is not None:
                    live_pct = (price_latest / ref_close - 1.0) * 100.0
            live_gap_list.append(live_gap)
            live_pct_change_list.append(live_pct)

            avg_vol_20 = _to_float(row.get("avg_volume_20"))
            live_vol = _to_float(row.get("live_volume"))
            live_intraday = None
            if avg_vol_20 is not None and avg_vol_20 > 0 and live_vol is not None:
                live_intraday = live_vol / avg_vol_20
            live_intraday_vol_list.append(live_intraday)

            sig_ma5 = _to_float(row.get("sig_ma5"))
            sig_ma20 = _to_float(row.get("sig_ma20"))
            sig_atr14 = _to_float(row.get("sig_atr14"))
            atr_pct = None
            if sig_atr14 is not None and sig_ma20 is not None and sig_ma20 != 0:
                atr_pct = sig_atr14 / sig_ma20

            dev_ma5 = None
            dev_ma20 = None
            dev_ma5_atr = None
            dev_ma20_atr = None
            if price_now is not None:
                if sig_ma5:
                    dev_ma5 = price_now - sig_ma5
                if sig_ma20:
                    dev_ma20 = price_now - sig_ma20
                if sig_atr14 and sig_atr14 != 0:
                    if sig_ma5 is not None:
                        dev_ma5_atr = (price_now - sig_ma5) / sig_atr14
                    if sig_ma20 is not None:
                        dev_ma20_atr = (price_now - sig_ma20) / sig_atr14
            dev_ma5_list.append(dev_ma5)
            dev_ma20_list.append(dev_ma20)
            dev_ma5_atr_list.append(dev_ma5_atr)
            dev_ma20_atr_list.append(dev_ma20_atr)

            ref_asof_close = (
                _to_float(row.get("asof_close"))
                or ref_close
                or _to_float(row.get("sig_close"))
            )
            runup_metrics = compute_runup_metrics(
                _to_float(row.get("sig_close")),
                asof_close=ref_asof_close,
                live_high=_to_float(row.get("live_high")) or price_now,
                sig_atr14=sig_atr14,
            )
            runup_from_sigclose_list.append(runup_metrics.runup_from_sigclose)
            runup_from_sigclose_atr_list.append(runup_metrics.runup_from_sigclose_atr)
            runup_ref_price_list.append(runup_metrics.runup_ref_price)
            runup_ref_source_list.append(runup_metrics.runup_ref_source)

            threshold_gap_up = max_up
            if sig_atr14 and sig_atr14 > 0 and ref_close:
                atr_gap = max_up_atr_mult * sig_atr14 / ref_close
                threshold_gap_up = min(max_up, atr_gap)

            ma20_thresh = pullback_min_vs_ma20 if is_pullback else min_vs_ma20
            if atr_pct is not None and ma20_atr_tol_mult is not None:
                dyn_thresh = -ma20_atr_tol_mult * atr_pct
                ma20_thresh = min(ma20_thresh, dyn_thresh)
                if ma20_dyn_min_pct is not None:
                    ma20_thresh = max(ma20_thresh, ma20_dyn_min_pct)

            ma20_prewarn = False
            ma20_prewarn_reason = None
            if (
                price_now is not None
                and sig_ma20 is not None
                and ma20_thresh is not None
                and below_ma20_tol_pct is not None
            ):
                invalid_cut = sig_ma20 * (1 + ma20_thresh - below_ma20_tol_pct)
                prewarn_cut = sig_ma20 * (
                    1 + ma20_thresh - below_ma20_tol_pct + ma20_prewarn_buffer_pct
                )
                if price_now < prewarn_cut and price_now >= invalid_cut:
                    ma20_prewarn = True
                    ma20_prewarn_reason = (
                        f"near_ma20_cut price={price_now:.2f} prewarn={prewarn_cut:.2f}"
                    )

            chip_score = _to_float(row.get("sig_chip_score"))
            chip_reason = row.get("sig_chip_reason")
            chip_age_days = _to_float(row.get("sig_chip_age_days"))
            chip_stale_raw = row.get("sig_chip_stale")
            chip_stale_hit = None
            if chip_stale_raw is not None and not pd.isna(chip_stale_raw):
                if isinstance(chip_stale_raw, bool):
                    chip_stale_hit = chip_stale_raw
                elif isinstance(chip_stale_raw, (int, float)):
                    chip_stale_hit = bool(int(chip_stale_raw))
                else:
                    flag = str(chip_stale_raw).strip().lower()
                    if flag in {"1", "true", "yes", "y", "on"}:
                        chip_stale_hit = True
                    elif flag in {"0", "false", "no", "n", "off"}:
                        chip_stale_hit = False

            breach = False
            breach_reason = None
            if price_now is not None:
                dev_ma20_atr_val = dev_ma20_atr
                runup_limit = pullback_runup_atr_max if is_pullback else runup_atr_max
                if atr_pct is not None and runup_limit is not None and runup_atr_vol_mult is not None:
                    runup_limit = runup_limit + runup_atr_vol_mult * atr_pct
                    if runup_atr_max_cap is not None:
                        runup_limit = min(runup_limit, runup_atr_max_cap)
                breach, breach_reason = evaluate_runup_breach(
                    runup_metrics,
                    runup_atr_max=runup_limit,
                    runup_atr_tol=runup_atr_tol,
                    dev_ma20_atr=dev_ma20_atr_val,
                    dev_ma20_atr_min=pullback_runup_dev_ma20_atr_min if is_pullback else None,
                )

            signal_age = None
            raw_signal_age = row.get("signal_age")
            if raw_signal_age is not None:
                try:
                    if not pd.isna(raw_signal_age):
                        signal_age = int(float(raw_signal_age))
                except Exception:
                    signal_age = None

            chip_reason_val = None
            if chip_reason is not None and not pd.isna(chip_reason):
                chip_reason_val = str(chip_reason).strip() or None

            ctx = DecisionContext(
                entry_exposure_cap=env.position_cap_pct,
                env=env,
                chip_score=chip_score,
                chip_reason=chip_reason_val,
                chip_age_days=chip_age_days,
                chip_stale_hit=chip_stale_hit,
                price_now=price_now,
                live_gap=live_gap,
                live_pct=live_pct,
                threshold_gap_up=threshold_gap_up,
                max_gap_down=max_down,
                sig_ma20=sig_ma20,
                ma20_thresh=ma20_thresh,
                ma20_prewarn=ma20_prewarn,
                ma20_prewarn_reason=ma20_prewarn_reason,
                signal_age=signal_age,
                limit_up_trigger=limit_up_trigger,
                runup_breach=breach,
                runup_breach_reason=breach_reason,
            )
            self.rule_engine.apply(ctx, self.rules)

            result = ctx.export_result()

            action = result.action
            action_reason = result.action_reason

            trade_stop_ref = None
            if price_now is not None and sig_atr14 is not None:
                trade_stop_ref = price_now - stop_atr_mult * sig_atr14

            entry_cap = result.entry_exposure_cap
            if entry_cap is None:
                entry_cap = env.position_cap_pct
            sig_final_cap = _to_float(row.get("final_cap"))
            if sig_final_cap is not None:
                entry_cap = sig_final_cap if entry_cap is None else min(entry_cap, sig_final_cap)
            entry_exposure_caps.append(entry_cap)
            trade_stop_refs.append(trade_stop_ref)
            effective_stop_refs.append(trade_stop_ref)

            missing_live = (
                pd.isna(row.get("live_latest"))
                and pd.isna(row.get("live_open"))
                and pd.isna(row.get("live_high"))
                and pd.isna(row.get("live_low"))
            )
            missing_asof = pd.isna(row.get("asof_trade_date")) or pd.isna(row.get("asof_close"))

            state = result.state
            status_reason = result.status_reason
            rule_hits_json = result.rule_hits_json
            summary_line = result.summary_line

            if missing_live or missing_asof:
                reason = "MISSING_LIVE_QUOTE" if missing_live else "MISSING_LATEST_SNAPSHOT"
                state = "INVALID"
                status_reason = reason
                action = "SKIP"
                action_reason = reason
                summary_line = f"{summary_line} | DATA:{reason}"

                try:
                    hits = json.loads(rule_hits_json) if rule_hits_json else []
                    if not isinstance(hits, list):
                        hits = []
                except Exception:
                    hits = []
                hits.append({"rule": reason, "severity": 100, "reason": reason})
                rule_hits_json = json.dumps(hits, ensure_ascii=False)

            row2 = row.copy()
            row2["dev_ma5_atr"] = dev_ma5_atr
            row2["dev_ma20_atr"] = dev_ma20_atr
            row2["runup_from_sigclose_atr"] = runup_metrics.runup_from_sigclose_atr
            row2["live_intraday_vol_ratio"] = live_intraday

            strength, strength_note = self._calc_signal_strength(
                row2,
                rule_hits_json=rule_hits_json,
                state=state,
                status_reason=status_reason,
            )
            strength_delta = None
            strength_trend = None

            states.append(state)
            status_reasons.append(status_reason)
            actions.append(action)
            action_reasons.append(action_reason)
            signal_kinds.append("PULLBACK" if is_pullback else "CROSS")
            rule_hits_json_list.append(rule_hits_json)
            summary_lines.append(summary_line)
            signal_strength_list.append(strength)
            strength_delta_list.append(strength_delta)
            strength_trend_list.append(strength_trend)
            strength_note_list.append(strength_note)

        merged["monitor_date"] = monitor_date
        if "live_trade_date" not in merged.columns:
            merged["live_trade_date"] = pd.NA
        merged["live_gap_pct"] = live_gap_list
        merged["live_pct_change"] = live_pct_change_list
        merged["live_intraday_vol_ratio"] = live_intraday_vol_list
        merged["action"] = actions
        merged["action_reason"] = action_reasons
        merged["state"] = states
        merged["status_reason"] = status_reasons
        merged["signal_kind"] = signal_kinds
        merged["signal_age"] = merged.get("signal_age")
        merged["trade_stop_ref"] = trade_stop_refs
        merged["effective_stop_ref"] = effective_stop_refs
        merged["entry_exposure_cap"] = entry_exposure_caps
        merged["run_id"] = run_id_val
        if "run_pk" not in merged.columns:
            merged["run_pk"] = run_pk
        else:
            merged["run_pk"] = merged["run_pk"].fillna(run_pk)
        merged["runup_from_sigclose"] = runup_from_sigclose_list
        merged["runup_from_sigclose_atr"] = runup_from_sigclose_atr_list
        merged["runup_ref_price"] = runup_ref_price_list
        merged["runup_ref_source"] = runup_ref_source_list
        merged["dev_ma5"] = dev_ma5_list
        merged["dev_ma20"] = dev_ma20_list
        merged["dev_ma5_atr"] = dev_ma5_atr_list
        merged["dev_ma20_atr"] = dev_ma20_atr_list
        merged["rule_hits_json"] = rule_hits_json_list
        merged["summary_line"] = summary_lines
        merged["signal_strength"] = signal_strength_list
        merged["strength_delta"] = strength_delta_list
        merged["strength_trend"] = strength_trend_list
        merged["strength_note"] = strength_note_list

        def _coalesce_numeric_into(target: str, fallback: str) -> None:
            if target not in merged.columns:
                merged[target] = None
            if fallback not in merged.columns:
                return
            left = pd.to_numeric(merged[target], errors="coerce")
            right = pd.to_numeric(merged[fallback], errors="coerce")
            merged[target] = left.fillna(right)

        if "asof_trade_date" in merged.columns and "sig_date" in merged.columns:
            mask = merged["asof_trade_date"].notna()
            merged["asof_trade_date"] = merged["asof_trade_date"].where(mask, merged["sig_date"])

        _coalesce_numeric_into("asof_close", "sig_close")
        _coalesce_numeric_into("asof_ma5", "sig_ma5")
        _coalesce_numeric_into("asof_ma20", "sig_ma20")
        _coalesce_numeric_into("asof_ma60", "sig_ma60")
        _coalesce_numeric_into("asof_ma250", "sig_ma250")
        _coalesce_numeric_into("asof_vol_ratio", "sig_vol_ratio")
        _coalesce_numeric_into("asof_macd_hist", "sig_macd_hist")
        _coalesce_numeric_into("asof_atr14", "sig_atr14")
        _coalesce_numeric_into("asof_stop_ref", "sig_stop_ref")

        full_keep_cols = [
            "monitor_date",
            "sig_date",
            "signal_age",
            "strategy_code",
            "code",
            "name",
            "asof_trade_date",
            "live_trade_date",
            "avg_volume_20",
            "live_open",
            "live_high",
            "live_low",
            "live_latest",
            "live_volume",
            "live_amount",
            "live_gap_pct",
            "live_pct_change",
            "live_intraday_vol_ratio",
            "sig_close",
            "sig_ma5",
            "sig_ma20",
            "sig_ma60",
            "sig_ma250",
            "sig_vol_ratio",
            "sig_macd_hist",
            "sig_atr14",
            "sig_stop_ref",
            "effective_stop_ref",
            "asof_close",
            "asof_ma5",
            "asof_ma20",
            "asof_ma60",
            "asof_ma250",
            "asof_vol_ratio",
            "asof_macd_hist",
            "asof_atr14",
            "asof_stop_ref",
            "sig_kdj_k",
            "sig_kdj_d",
            "trade_stop_ref",
            "dev_ma5",
            "dev_ma20",
            "dev_ma5_atr",
            "dev_ma20_atr",
            "runup_from_sigclose",
            "runup_from_sigclose_atr",
            "runup_ref_price",
            "runup_ref_source",
            "entry_exposure_cap",
            "industry",
            "board_name",
            "board_code",
            "board_status",
            "board_rank",
            "board_chg_pct",
            "signal_strength",
            "strength_delta",
            "strength_trend",
            "strength_note",
            "risk_tag",
            "risk_note",
            "rule_hits_json",
            "summary_line",
            "signal_kind",
            "sig_signal",
            "sig_reason",
            "state",
            "status_reason",
            "action",
            "action_reason",
            "run_id",
            "run_pk",
            "snapshot_hash",
        ]

        compact_keep_cols = [
            "monitor_date",
            "sig_date",
            "asof_trade_date",
            "live_trade_date",
            "signal_age",
            "strategy_code",
            "code",
            "name",
            "live_open",
            "live_latest",
            "live_high",
            "live_low",
            "live_pct_change",
            "live_gap_pct",
            "live_intraday_vol_ratio",
            "sig_close",
            "sig_ma5",
            "sig_ma20",
            "sig_ma60",
            "sig_atr14",
            "sig_vol_ratio",
            "sig_macd_hist",
            "sig_stop_ref",
            "trade_stop_ref",
            "dev_ma5",
            "dev_ma20",
            "dev_ma5_atr",
            "dev_ma20_atr",
            "runup_from_sigclose",
            "runup_from_sigclose_atr",
            "runup_ref_price",
            "runup_ref_source",
            "effective_stop_ref",
            "board_name",
            "board_code",
            "board_status",
            "board_rank",
            "board_chg_pct",
            "signal_strength",
            "strength_delta",
            "strength_trend",
            "strength_note",
            "risk_tag",
            "risk_note",
            "rule_hits_json",
            "summary_line",
            "signal_kind",
            "sig_signal",
            "sig_reason",
            "state",
            "status_reason",
            "action",
            "action_reason",
            "run_id",
            "run_pk",
            "snapshot_hash",
        ]

        output_mode = (self.params.output_mode or "COMPACT").upper()
        keep_cols = full_keep_cols if output_mode == "FULL" else compact_keep_cols

        for col in keep_cols:
            if col not in merged.columns:
                merged[col] = None

        merged["snapshot_hash"] = merged.apply(
            lambda row: make_snapshot_hash(row.to_dict()), axis=1
        )
        return merged[keep_cols].copy()

    @staticmethod
    def _clamp01(val: float | None) -> float:
        if val is None:
            return 0.0
        try:
            fv = float(val)
        except Exception:  # noqa: BLE001
            return 0.0
        if fv != fv:
            return 0.0
        return 0.0 if fv < 0.0 else 1.0 if fv > 1.0 else fv

    @staticmethod
    def _scale_range(val: float | None, low: float, high: float) -> float | None:
        if val is None:
            return None
        if high <= low:
            return None
        if val <= low:
            return 0.0
        if val >= high:
            return 1.0
        return (val - low) / (high - low)

    @staticmethod
    def _extract_rule_hit_names(rule_hits_json: str | None) -> set[str]:
        if not rule_hits_json:
            return set()
        try:
            hits = json.loads(rule_hits_json)
        except Exception:  # noqa: BLE001
            return set()
        if not isinstance(hits, list):
            return set()
        names: set[str] = set()
        for hit in hits:
            if not isinstance(hit, dict):
                continue
            name = hit.get("name") or hit.get("rule") or hit.get("id")
            name = str(name or "").strip().upper()
            if name:
                names.add(name)
        return names

    def _calc_signal_strength(
        self,
        row: pd.Series,
        *,
        rule_hits_json: str | None,
        state: str | None,
        status_reason: str | None,
    ) -> tuple[float | None, str | None]:
        note_parts: List[str] = []
        weights: List[tuple[float, float]] = []

        def _add_component(
            label: str,
            raw_val: Any,
            low: float,
            high: float,
            weight: float,
        ) -> None:
            val = _to_float(raw_val)
            if val is None:
                return
            scaled = self._scale_range(val, low, high)
            if scaled is None:
                return
            weights.append((weight, scaled))
            note_parts.append(f"{label}={val:.2f}")

        _add_component("dev_ma5_atr", row.get("dev_ma5_atr"), -1.0, 2.0, 0.25)
        _add_component("dev_ma20_atr", row.get("dev_ma20_atr"), -1.0, 2.0, 0.35)
        _add_component("runup_atr", row.get("runup_from_sigclose_atr"), -0.5, 3.0, 0.25)
        _add_component("vol_ratio", row.get("live_intraday_vol_ratio"), 0.5, 2.5, 0.15)

        rule_names = self._extract_rule_hit_names(rule_hits_json)
        state_norm = str(state or "").strip().upper()

        if state_norm == "INVALID":
            reason = str(status_reason or "").strip() or "INVALID"
            return 0.0, f"INVALID:{reason}"

        if "LIMIT_UP" in rule_names:
            return 0.0, "INVALID:LIMIT_UP"

        if not weights:
            return None, None

        total_weight = sum(weight for weight, _ in weights)
        strength = (
            100.0 * sum(weight * value for weight, value in weights) / total_weight
            if total_weight > 0
            else None
        )
        if strength is None:
            return None, None

        if "RUNUP_BREACH" in rule_names:
            strength = min(strength, 20.0)
            note_parts.append("CAP=RUNUP_BREACH")
        if "BELOW_MA20_REQ" in rule_names:
            strength = min(strength, 35.0)
            note_parts.append("CAP=BELOW_MA20_REQ")

        note = " ".join(note_parts) if note_parts else None
        return strength, note

    def _calc_market_weight(self, env_context: dict[str, Any] | None) -> tuple[float, str]:
        env = MarketEnvironment.from_snapshot(env_context)
        gate = str(env.gate_action or "").strip().upper() or "-"
        pos_hint = _to_float(env.position_hint)
        weekly_risk = str(env.weekly_risk_level or "").strip().upper() or "-"

        base_map = {"ALLOW": 1.0, "ALLOW_SMALL": 0.75, "WAIT": 0.45, "STOP": 0.0}
        base = base_map.get(gate, 0.85)
        pos_factor = 1.0 if pos_hint is None else self._clamp01(pos_hint)
        risk_factor = 1.0
        if weekly_risk == "HIGH":
            risk_factor = 0.65
        elif weekly_risk == "MEDIUM":
            risk_factor = 0.85
        elif weekly_risk == "LOW":
            risk_factor = 1.05

        market_weight = self._clamp01(base * pos_factor * risk_factor)
        note = f"gate={gate} pos_hint={pos_hint if pos_hint is not None else '-'} weekly_risk={weekly_risk}"
        return market_weight, note

    @staticmethod
    def _resolve_board_payload(
        boards_map: dict[str, Any] | None, board_code: Any, board_name: Any
    ) -> dict[str, Any] | None:
        if not isinstance(boards_map, dict) or not boards_map:
            return None
        for key in (board_code, board_name):
            key_norm = str(key or "").strip()
            if key_norm and key_norm in boards_map:
                payload = boards_map.get(key_norm)
                return payload if isinstance(payload, dict) else None
        return None

    def build_rank_frame(
        self, df: pd.DataFrame, env_context: dict[str, Any] | None
    ) -> tuple[pd.DataFrame, dict[str, Any]]:
        if df.empty:
            return df.copy(), {}

        ranked = df.copy()
        meta: dict[str, Any] = {}

        market_weight, market_note = self._calc_market_weight(env_context)
        ranked["market_weight"] = market_weight
        meta["market_weight"] = market_weight
        meta["market_note"] = market_note

        boards_map = env_context.get("boards") if isinstance(env_context, dict) else None
        if "board_status" not in ranked.columns:
            ranked["board_status"] = None
        if "board_rank" not in ranked.columns:
            ranked["board_rank"] = None
        if "board_chg_pct" not in ranked.columns:
            ranked["board_chg_pct"] = None

        def _fill_board(row: pd.Series) -> pd.Series:
            payload = self._resolve_board_payload(
                boards_map, row.get("board_code"), row.get("board_name")
            )
            if not payload:
                return row
            if not str(row.get("board_status") or "").strip():
                row["board_status"] = payload.get("status")
            if row.get("board_rank") in (None, "", 0) and payload.get("rank") is not None:
                row["board_rank"] = payload.get("rank")
            if row.get("board_chg_pct") in (None, "") and payload.get("chg_pct") is not None:
                row["board_chg_pct"] = payload.get("chg_pct")
            return row

        if boards_map:
            ranked = ranked.apply(_fill_board, axis=1)

        status_weight_map = {"strong": 1.0, "neutral": 0.7, "weak": 0.4}
        board_status = ranked.get("board_status")
        if board_status is None:
            ranked["board_weight"] = 0.6
        else:
            ranked["board_weight"] = (
                board_status.astype(str)
                .str.strip()
                .str.lower()
                .map(status_weight_map)
                .fillna(0.6)
            )

        meta["board_weight_map"] = status_weight_map

        quality_col = None
        quality_series = None
        for cand in ["signal_strength", "sig_vol_ratio", "sig_chip_score"]:
            if cand in ranked.columns:
                s = pd.to_numeric(ranked[cand], errors="coerce")
                if s.notna().sum() >= 3:
                    quality_col = cand
                    quality_series = s
                    break

        if quality_series is None:
            ranked["stock_quality_weight"] = 0.5
            meta["stock_quality_source"] = None
        else:
            pct = quality_series.rank(pct=True)
            median = float(pct.dropna().median()) if pct.notna().any() else 0.5
            pct_filled = pct.fillna(median)
            if quality_col == "sig_chip_score":
                ranked["stock_quality_weight"] = 0.5 + (pct_filled - 0.5) * 0.2
            else:
                ranked["stock_quality_weight"] = pct_filled
            meta["stock_quality_source"] = quality_col

        ranked["final_rank_score"] = (
            100.0
            * (
                0.45 * ranked["market_weight"]
                + 0.35 * ranked["board_weight"]
                + 0.20 * ranked["stock_quality_weight"]
            )
        )

        return ranked, meta

    @staticmethod
    def build_board_map_from_strength(board_strength: pd.DataFrame) -> dict[str, dict[str, Any]]:
        board_map: dict[str, dict[str, Any]] = {}
        if board_strength is None or getattr(board_strength, "empty", True):
            return board_map

        total = len(board_strength)
        for _, row in board_strength.iterrows():
            name = str(row.get("board_name") or "").strip()
            code = str(row.get("board_code") or "").strip()
            rank = row.get("rank")
            pct = row.get("chg_pct")
            status = "neutral"
            if total > 0 and rank not in (None, ""):
                try:
                    rank_i = int(rank)
                except Exception:  # noqa: BLE001
                    rank_i = None
                if rank_i is not None:
                    if rank_i <= max(1, int(total * 0.2)):
                        status = "strong"
                    elif rank_i >= max(1, int(total * 0.8)):
                        status = "weak"
            payload = {"rank": rank, "chg_pct": pct, "status": status}
            for key in [name, code]:
                key_norm = str(key).strip()
                if key_norm:
                    board_map[key_norm] = payload

        return board_map

    def export_csv(self, df: pd.DataFrame) -> None:
        if df.empty or (not self.params.export_csv):
            return

        app_sec = get_section("app") or {}
        base_dir = "output"
        if isinstance(app_sec, dict):
            base_dir = str(app_sec.get("output_dir", base_dir))

        outdir = Path(base_dir) / self.params.output_subdir
        outdir.mkdir(parents=True, exist_ok=True)

        monitor_date = str(df.iloc[0].get("monitor_date") or dt.date.today().isoformat())

        suffix = ""
        if self.params.incremental_export_timestamp:
            checked_at = str(df.iloc[0].get("checked_at") or "").strip()
            if checked_at and " " in checked_at:
                time_part = checked_at.split(" ", 1)[1].replace(":", "").replace(".", "")
                if time_part:
                    suffix = f"_{time_part}"
            if not suffix:
                run_id = str(df.iloc[0].get("run_id") or "").strip()
                if run_id and " " in run_id:
                    time_part = run_id.split(" ", 1)[1].replace(":", "").replace(".", "")
                    if time_part:
                        suffix = f"_{time_part}"

        path = outdir / f"open_monitor_{monitor_date}{suffix}.csv"

        export_df = df.copy()
        export_df = export_df.drop(columns=["snapshot_hash"], errors="ignore")
        gap_col = "live_gap_pct" if "live_gap_pct" in export_df.columns else "gap_pct"
        export_df[gap_col] = export_df[gap_col].apply(_to_float)
        action_rank = {"EXECUTE": 0, "WAIT": 1, "SKIP": 2, "UNKNOWN": 3}
        export_df["_action_rank"] = export_df["action"].map(action_rank).fillna(99)

        state_rank = {
            "OK": 0,
            "OVEREXTENDED": 1,
            "RUNUP_TOO_LARGE": 2,
            "INVALID": 3,
            "STALE": 4,
            "UNKNOWN": 9,
        }
        if "state" in export_df.columns:
            export_df["_state_rank"] = export_df["state"].map(state_rank).fillna(99)
        else:
            export_df["_state_rank"] = 99
        export_df = export_df.sort_values(
            by=["_action_rank", "_state_rank", gap_col],
            ascending=[True, True, True],
        )
        export_df = export_df.drop(
            columns=["_action_rank", "_state_rank"],
            errors="ignore",
        )
        if self.params.export_top_n > 0:
            export_df = export_df.head(self.params.export_top_n)

        export_df.to_csv(path, index=False, encoding="utf-8-sig")
        self.logger.info("开盘监测 CSV 已导出：%s", path)

================================================================================
FILE: ashare/open_monitor_market_data.py
================================================================================

"""open_monitor 行情抓取与路由层。"""

from __future__ import annotations

from typing import Any, List

import pandas as pd

from .open_monitor_quotes import fetch_quotes_akshare, fetch_quotes_eastmoney


class OpenMonitorMarketData:
    """开盘监测行情抓取与数据源路由。"""

    def __init__(self, logger, params) -> None:
        self.logger = logger
        self.params = params

    def fetch_quotes(self, codes: List[str]) -> pd.DataFrame:
        """获取实时行情。

        A 修复点：
        - live_trade_date 优先使用行情源字段（trade_date/date/live_trade_date）。
        - 若行情源未提供交易日字段，使用本次运行的 monitor_date（或 checked_at.date）兜底，便于对账。
        """
        if not codes:
            return pd.DataFrame(columns=["code"])

        source = (self.params.quote_source or "eastmoney").strip().lower()
        if source == "akshare":
            df = self._fetch_quotes_akshare(codes)
        else:
            df = self._fetch_quotes_eastmoney(codes)

        # ---- A: 补齐 live_trade_date（优先使用行情源字段；缺失时兜底 monitor_date）----
        if "live_trade_date" not in df.columns:
            df["live_trade_date"] = pd.NA
        for cand in ("trade_date", "date"):
            if cand in df.columns:
                df["live_trade_date"] = df["live_trade_date"].fillna(df[cand])

        checked_at = getattr(self.params, "checked_at", None)
        monitor_date = getattr(self.params, "monitor_date", None)
        if monitor_date:
            df["live_trade_date"] = df["live_trade_date"].fillna(monitor_date)
        elif checked_at is not None:
            df["live_trade_date"] = df["live_trade_date"].fillna(
                checked_at.date().isoformat()
            )

        if checked_at is not None:
            df["quote_fetched_at"] = checked_at
            df["quote_fetched_date"] = checked_at.date().isoformat()

        return df

    def fetch_index_live_quote(self) -> dict[str, Any]:
        code = str(self.params.index_code or "").strip()
        if not code:
            return {}
        df = self.fetch_quotes([code])
        if df.empty:
            return {"index_code": code}
        row = df.iloc[0].to_dict()
        row["index_code"] = code

        live_trade_date = row.get("live_trade_date")
        if pd.isna(live_trade_date):
            live_trade_date = None
        if not live_trade_date:
            live_trade_date = row.get("trade_date") or row.get("date")
        if pd.isna(live_trade_date):
            live_trade_date = None
        if not live_trade_date:
            monitor_date = getattr(self.params, "monitor_date", None)
            checked_at = getattr(self.params, "checked_at", None)
            if monitor_date:
                live_trade_date = monitor_date
            elif checked_at is not None:
                live_trade_date = checked_at.date().isoformat()
        row["live_trade_date"] = live_trade_date
        return row

    def _fetch_quotes_akshare(self, codes: List[str]) -> pd.DataFrame:
        strict_quotes = bool(getattr(self.params, "strict_quotes", True))
        return fetch_quotes_akshare(codes, strict_quotes=strict_quotes, logger=self.logger)

    def _fetch_quotes_eastmoney(self, codes: List[str]) -> pd.DataFrame:
        strict_quotes = bool(getattr(self.params, "strict_quotes", True))
        return fetch_quotes_eastmoney(codes, strict_quotes=strict_quotes, logger=self.logger)

================================================================================
FILE: ashare/open_monitor_quotes.py
================================================================================

from __future__ import annotations

"""open_monitor 的行情抓取与字段标准化。

目标：
- 将 eastmoney / akshare 行情抓取逻辑从 open_monitor.py 拆分出来；
- 统一输出列名（live_open/live_high/live_low/live_latest/live_volume/live_amount...）。
"""

import json
import time
import urllib.parse
import urllib.request
from typing import Any, Dict, List

import pandas as pd

from .utils.convert import to_float as _to_float


def normalize_quotes_columns(df: pd.DataFrame) -> pd.DataFrame:
    """将不同来源行情列统一成 open_monitor 契约列。"""

    if not isinstance(df, pd.DataFrame):
        return pd.DataFrame(
            columns=[
                "code",
                "live_open",
                "live_high",
                "live_low",
                "live_latest",
                "live_volume",
                "live_amount",
                "live_pct_change",
                "prev_close",
            ]
        )

    out = df.copy()

    mapping: Dict[str, str] = {
        # 统一英文列
        "open": "live_open",
        "high": "live_high",
        "low": "live_low",
        "latest": "live_latest",
        "volume": "live_volume",
        "amount": "live_amount",
        "pct_change": "live_pct_change",
        "gap_pct": "live_gap_pct",
        "intraday_vol_ratio": "live_intraday_vol_ratio",
        # 统一中文列（以 akshare spot 为主）
        "今开": "live_open",
        "最高": "live_high",
        "最低": "live_low",
        "最新价": "live_latest",
        "成交量": "live_volume",
        "成交额": "live_amount",
        "涨跌幅": "live_pct_change",
        "昨收": "prev_close",
    }

    rename_cols = {k: v for k, v in mapping.items() if k in out.columns and v not in out.columns}
    if rename_cols:
        out = out.rename(columns=rename_cols)

    # 确保统一列存在（即便为空行情，也保持契约列结构稳定）
    for col in (
        "live_open",
        "live_high",
        "live_low",
        "live_latest",
        "live_volume",
        "live_amount",
    ):
        if col not in out.columns:
            out[col] = None

    return out


def strip_baostock_prefix(code: str) -> str:
    code = str(code or "").strip()
    if code.startswith("sh.") or code.startswith("sz."):
        return code[3:]
    return code


def to_baostock_code(exchange: str, symbol: str) -> str:
    ex = str(exchange or "").lower().strip()
    sym = str(symbol or "").strip()
    if ex in {"sh", "1"}:
        return f"sh.{sym}"
    if ex in {"sz", "0"}:
        return f"sz.{sym}"
    # fallback：猜测 6/9 为沪，0/3 为深
    if sym.startswith(("6", "9")):
        return f"sh.{sym}"
    return f"sz.{sym}"


def to_eastmoney_secid(code: str) -> str:
    code = str(code or "").strip()
    if code.startswith("sh."):
        digits = strip_baostock_prefix(code)
        return f"1.{digits}"
    if code.startswith("sz."):
        digits = strip_baostock_prefix(code)
        return f"0.{digits}"
    # fallback：按 6/9 -> 沪
    digits = strip_baostock_prefix(code)
    if digits.startswith(("6", "9")):
        return f"1.{digits}"
    return f"0.{digits}"


def urlopen_json_no_proxy(url: str, *, timeout: int = 10, retries: int = 2) -> Dict[str, Any]:
    """访问东财接口并返回 JSON（默认不使用环境代理）。"""

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0 Safari/537.36"
        ),
        "Accept": "application/json, text/plain, */*",
        "Referer": "https://quote.eastmoney.com/",
        "Connection": "close",
    }
    req = urllib.request.Request(url, headers=headers)
    opener = urllib.request.build_opener(urllib.request.ProxyHandler({}))

    last_exc: Exception | None = None
    for i in range(retries + 1):
        try:
            with opener.open(req, timeout=timeout) as resp:
                raw = resp.read().decode("utf-8", errors="ignore")
            return json.loads(raw) if raw else {}
        except Exception as exc:  # noqa: BLE001
            last_exc = exc
            if i < retries:
                time.sleep(0.5 * (2**i))
                continue
            raise last_exc


def fetch_quotes_akshare(
    codes: List[str],
    *,
    strict_quotes: bool = True,
    logger: Any = None,  # noqa: ANN401
) -> pd.DataFrame:
    try:
        import akshare as ak  # type: ignore
    except Exception as exc:  # noqa: BLE001
        if logger is not None:
            logger.info("AkShare 不可用（将回退）：%s", exc)
        return normalize_quotes_columns(pd.DataFrame())

    digits = {strip_baostock_prefix(c) for c in codes}
    try:
        spot = ak.stock_zh_a_spot_em()
    except Exception as exc:  # noqa: BLE001
        if logger is not None:
            logger.warning("AkShare 行情拉取失败（将回退）：%s", exc)
        return normalize_quotes_columns(pd.DataFrame())

    if spot is None or getattr(spot, "empty", True):
        return normalize_quotes_columns(pd.DataFrame())

    rename_map = {
        "代码": "symbol",
        "名称": "name",
        "最新价": "latest",
        "涨跌幅": "pct_change",
        "今开": "open",
        "昨收": "prev_close",
        "最高": "high",
        "最低": "low",
        "成交量": "volume",
        "成交额": "amount",
    }
    for k in list(rename_map.keys()):
        if k not in spot.columns:
            rename_map.pop(k, None)

    spot = spot.rename(columns=rename_map)
    if "symbol" not in spot.columns:
        return normalize_quotes_columns(pd.DataFrame())

    spot["symbol"] = spot["symbol"].astype(str)
    spot = spot[spot["symbol"].isin(digits)].copy()
    if spot.empty:
        return normalize_quotes_columns(pd.DataFrame())

    out = pd.DataFrame()
    out["code"] = spot["symbol"].apply(lambda x: to_baostock_code("auto", str(x)))
    out["symbol"] = spot["symbol"].astype(str)
    out["name"] = spot.get("name", pd.Series([""] * len(spot))).astype(str)
    out["open"] = spot.get("open", pd.Series([None] * len(spot))).apply(_to_float)
    out["latest"] = spot.get("latest", pd.Series([None] * len(spot))).apply(_to_float)
    out["prev_close"] = spot.get("prev_close", pd.Series([None] * len(spot))).apply(_to_float)
    out["high"] = spot.get("high", pd.Series([None] * len(spot))).apply(_to_float)
    out["low"] = spot.get("low", pd.Series([None] * len(spot))).apply(_to_float)
    out["volume"] = spot.get("volume", pd.Series([None] * len(spot))).apply(_to_float)
    out["amount"] = spot.get("amount", pd.Series([None] * len(spot))).apply(_to_float)
    out["pct_change"] = spot.get("pct_change", pd.Series([None] * len(spot))).apply(_to_float)

    mapping = {strip_baostock_prefix(c): c for c in codes}
    out["code"] = out["symbol"].map(mapping).fillna(out["code"])
    out = normalize_quotes_columns(out)
    required = [
        "code",
        "live_open",
        "live_high",
        "live_low",
        "live_latest",
        "live_volume",
        "live_amount",
    ]
    missing = [c for c in required if c not in out.columns]
    if missing:
        msg = f"akshare 行情缺少统一列：{missing}"
        if strict_quotes:
            raise RuntimeError(msg)
        if logger is not None:
            logger.error("%s（strict_quotes=false，将补空列）", msg)
        for c in missing:
            out[c] = None
    return out.reset_index(drop=True)


def fetch_quotes_eastmoney(
    codes: List[str],
    *,
    strict_quotes: bool = True,
    logger: Any = None,  # noqa: ANN401
) -> pd.DataFrame:
    if not codes:
        return normalize_quotes_columns(pd.DataFrame())

    base_url = "https://push2.eastmoney.com/api/qt/ulist.np/get"
    fields = "f2,f3,f4,f5,f6,f12,f14,f15,f16,f17,f18"
    secids = [to_eastmoney_secid(c) for c in codes]

    batch_size = 80
    rows: List[Dict[str, Any]] = []
    for i in range(0, len(secids), batch_size):
        part = secids[i : i + batch_size]
        query = {
            "fltt": "2",
            "invt": "2",
            "fields": fields,
            "secids": ",".join(part),
        }
        url = f"{base_url}?{urllib.parse.urlencode(query)}"
        try:
            payload = urlopen_json_no_proxy(url, timeout=10, retries=2)
        except Exception as exc:  # noqa: BLE001
            if logger is not None:
                logger.error("Eastmoney 行情请求失败：%s", exc)
            continue

        data = (payload or {}).get("data") or {}
        diff = data.get("diff") or []
        if isinstance(diff, list):
            rows.extend([r for r in diff if isinstance(r, dict)])

    if not rows:
        return normalize_quotes_columns(pd.DataFrame())

    out_rows: List[Dict[str, Any]] = []
    mapping = {strip_baostock_prefix(c): c for c in codes}
    for r in rows:
        symbol = str(r.get("f12") or "").strip()
        name = str(r.get("f14") or "").strip()
        latest = _to_float(r.get("f2"))
        pct = _to_float(r.get("f3"))
        high = _to_float(r.get("f15"))
        low = _to_float(r.get("f16"))
        open_px = _to_float(r.get("f17"))
        prev_close = _to_float(r.get("f18"))
        # Eastmoney 成交量单位为“手”，统一转换为“股”口径
        volume = _to_float(r.get("f5"))
        volume = volume * 100 if volume is not None else None
        amount = _to_float(r.get("f6"))

        code_guess = to_baostock_code("auto", symbol)
        code = mapping.get(symbol, code_guess)

        out_rows.append(
            {
                "code": code,
                "symbol": symbol,
                "name": name,
                "open": open_px,
                "latest": latest,
                "prev_close": prev_close,
                "high": high,
                "low": low,
                "volume": volume,
                "amount": amount,
                "pct_change": pct,
            }
        )

    out = pd.DataFrame(out_rows)
    out = normalize_quotes_columns(out)
    required = [
        "code",
        "live_open",
        "live_high",
        "live_low",
        "live_latest",
        "live_volume",
        "live_amount",
    ]
    missing = [c for c in required if c not in out.columns]
    if missing:
        msg = f"Eastmoney 行情缺少统一列：{missing}"
        if strict_quotes:
            raise RuntimeError(msg)
        if logger is not None:
            logger.error("%s（strict_quotes=false，将补空列）", msg)
        for c in missing:
            out[c] = None
    return out.reset_index(drop=True)
================================================================================
FILE: ashare/open_monitor_repo.py
================================================================================

"""open_monitor 的数据访问层。"""

from __future__ import annotations

import datetime as dt
import hashlib
import json
import math
from typing import Any, Dict, List, Tuple

import pandas as pd
from sqlalchemy import bindparam, text

from .baostock_core import BaostockDataFetcher
from .baostock_session import BaostockSession
from .config import get_section
from .db import DatabaseConfig, MySQLWriter
from .env_snapshot_utils import load_trading_calendar
from .ma5_ma20_trend_strategy import _atr, _macd
from .utils.convert import to_float as _to_float


SNAPSHOT_HASH_EXCLUDE = {
    "checked_at",
    "run_id",
    "run_pk",
}

READY_SIGNALS_REQUIRED_COLS = (
    "sig_date",
    "code",
    "strategy_code",
    "expires_on",
    "close",
    "ma20",
    "atr14",
)


def _normalize_snapshot_value(value: Any) -> Any:  # noqa: ANN401
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, float):
        if math.isnan(value) or math.isinf(value):
            return None
        return round(value, 6)
    if isinstance(value, int):
        return value
    try:
        if hasattr(value, "item"):
            base = value.item()
            if isinstance(base, (float, int)):
                return _normalize_snapshot_value(base)
            return base
    except Exception:
        pass
    if isinstance(value, dt.datetime):
        return value.isoformat(sep=" ")
    if isinstance(value, dt.date):
        return value.isoformat()
    return value


def make_snapshot_hash(row: Dict[str, Any]) -> str:
    payload = {}
    for key, value in row.items():
        if key in SNAPSHOT_HASH_EXCLUDE:
            continue
        payload[key] = _normalize_snapshot_value(value)
    serialized = json.dumps(
        payload, sort_keys=True, ensure_ascii=False, separators=(",", ":")
    )
    return hashlib.md5(serialized.encode("utf-8")).hexdigest()


def calc_run_id(ts: dt.datetime, run_id_minutes: int | None) -> str:
    window_minutes = max(int(run_id_minutes or 5), 1)

    auction_start = dt.time(9, 15)
    lunch_break_start = dt.time(11, 30)
    lunch_break_end = dt.time(13, 0)
    market_close = dt.time(15, 0)

    minute_of_day = ts.hour * 60 + ts.minute
    slot_minute = (minute_of_day // window_minutes) * window_minutes
    slot_time = dt.datetime.combine(ts.date(), dt.time(slot_minute // 60, slot_minute % 60))
    slot_text = slot_time.strftime("%Y-%m-%d %H:%M")

    t = ts.time()
    if t < auction_start:
        return f"PREOPEN {slot_text}"
    if lunch_break_start <= t < lunch_break_end:
        return f"BREAK {slot_text}"
    if t >= market_close:
        return f"POSTCLOSE {slot_text}"

    return slot_text


class OpenMonitorRepository:
    """开盘监测数据访问层，集中管理 SQL 与表结构。"""

    def __init__(self, engine, logger, params) -> None:
        self.engine = engine
        self.logger = logger
        self.params = params
        self.db_writer = MySQLWriter(DatabaseConfig.from_env())
        self.db_name = self.db_writer.config.db_name

        self._ready_signals_used: bool = False
        self._recent_buy_signals_cache_key = None
        self._recent_buy_signals_cache = None

    @property
    def ready_signals_used(self) -> bool:
        return self._ready_signals_used

    def _resolve_ready_signals_view(self) -> str | None:
        view = str(self.params.ready_signals_view or "").strip()
        if not view:
            return None

        view_exists = self._table_exists(view)
        if view.startswith("v_"):
            candidate = view[2:]
            if candidate and self._table_exists(candidate):
                candidate_cols = set(self._get_table_columns(candidate))
                required_cols = set(READY_SIGNALS_REQUIRED_COLS)
                if required_cols.issubset(candidate_cols):
                    if not view_exists:
                        self.logger.warning(
                            "ready_signals_view=%s missing, falling back to table %s.",
                            view,
                            candidate,
                        )
                    else:
                        self.logger.info(
                            "ready_signals_view=%s resolved to materialized table %s.",
                            view,
                            candidate,
                        )
                    return candidate

        if not view_exists:
            return None

        return view

    def _table_exists(self, table: str) -> bool:
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(text("SHOW TABLES LIKE :t"), conn, params={"t": table})
            return not df.empty
        except Exception:
            return False

    def _column_exists(self, table: str, column: str) -> bool:
        try:
            stmt = text(
                """
                SELECT COUNT(*) AS cnt
                FROM information_schema.columns
                WHERE table_schema = :schema AND table_name = :table AND column_name = :column
                """
            )
            with self.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={
                        "schema": self.db_name,
                        "table": table,
                        "column": column,
                    },
                )
            return not df.empty and bool(df.iloc[0].get("cnt", 0))
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("检查列 %s.%s 是否存在失败：%s", table, column, exc)
            return False

    def _get_table_columns(self, table: str) -> List[str]:
        stmt = text(
            """
            SELECT COLUMN_NAME FROM information_schema.COLUMNS
            WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = :t
            """
        )
        try:
            with self.engine.begin() as conn:
                return pd.read_sql(stmt, conn, params={"t": table})["COLUMN_NAME"].tolist()
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("无法获取 %s 列信息：%s", table, exc)
            return []

    def _daily_table(self) -> str:
        """获取日线数据表名（用于补充计算“信号日涨幅”等信息）。"""

        strat = get_section("strategy_ma5_ma20_trend") or {}
        if isinstance(strat, dict):
            name = str(strat.get("daily_table") or "").strip()
            if name:
                return name
        return "history_daily_kline"

    def _indicator_table(self) -> str:
        open_cfg = get_section("open_monitor") or {}
        if isinstance(open_cfg, dict):
            name = str(open_cfg.get("indicator_table") or "").strip()
            if name:
                return name

        strat = get_section("strategy_ma5_ma20_trend") or {}
        if isinstance(strat, dict):
            name = str(
                strat.get("indicator_table") or strat.get("signals_indicator_table") or ""
            ).strip()
            if name:
                return name

        return "strategy_indicator_daily"

    def _is_trading_day(self, date_str: str, latest_trade_date: str | None = None) -> bool:
        """粗略判断是否为交易日（优先用日线表，其次用工作日）。"""

        try:
            d = dt.datetime.strptime(str(date_str)[:10], "%Y-%m-%d").date()
        except Exception:  # noqa: BLE001
            return False

        if d.weekday() >= 5:
            return False

        target_str = d.isoformat()
        start = d - dt.timedelta(days=400)
        calendar = load_trading_calendar(start, d)
        if calendar:
            return target_str in calendar

        daily = self._daily_table()
        if not self._table_exists(daily):
            return True

        stmt = text(f"SELECT 1 FROM `{daily}` WHERE `date` = :d LIMIT 1")
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(stmt, conn, params={"d": str(d)})
            if not df.empty:
                return True
            if latest_trade_date and target_str > str(latest_trade_date)[:10]:
                return True
            return False
        except Exception:  # noqa: BLE001
            if latest_trade_date and target_str > str(latest_trade_date)[:10]:
                return True
            return False

    def _load_trade_age_map(
        self, latest_trade_date: str, min_date: str, monitor_date: str | None
    ) -> Dict[str, int]:
        """返回 {date_str: trading_day_age}，0 表示监控基准日。"""

        base_date = latest_trade_date
        monitor_str = str(monitor_date or "").strip()
        if (
            monitor_str
            and monitor_str > latest_trade_date
            and self._is_trading_day(monitor_str, latest_trade_date)
        ):
            base_date = monitor_str

        daily = self._daily_table()
        if not self._table_exists(daily):
            return {monitor_str: 0} if monitor_str else {}

        stmt = text(
            f"""
            SELECT DISTINCT CAST(`date` AS CHAR) AS d
            FROM `{daily}`
            WHERE `date` <= :base_date AND `date` >= :min_date
            ORDER BY d DESC
            """
        )
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt, conn, params={"base_date": base_date, "min_date": min_date}
                )
        except Exception:
            df = None

        dates = df["d"].dropna().astype(str).str[:10].tolist() if df is not None else []
        if (
            monitor_str
            and monitor_str not in dates
            and monitor_str > latest_trade_date
            and self._is_trading_day(monitor_str, latest_trade_date)
        ):
            dates.insert(0, monitor_str)

        if not dates:
            return {}

        return {d: i for i, d in enumerate(dates)}

    def _resolve_latest_trade_date(
        self,
        *,
        signals: pd.DataFrame | None = None,
        ready_view: str | None = None,
    ) -> str | None:
        """统一推导 latest_trade_date，避免对基础数据表的隐性依赖。"""

        if isinstance(signals, pd.DataFrame) and not signals.empty:
            for col in ("asof_trade_date", "sig_date"):
                if col in signals.columns:
                    s = pd.to_datetime(signals[col], errors="coerce").dropna()
                    if not s.empty:
                        return s.max().date().isoformat()

        daily = self._daily_table()
        if daily and self._table_exists(daily):
            try:
                with self.engine.begin() as conn:
                    df = pd.read_sql_query(
                        text(f"SELECT MAX(`date`) AS latest_trade_date FROM `{daily}`"),
                        conn,
                    )
                if df is not None and not df.empty:
                    v = df.iloc[0].get("latest_trade_date")
                    ts = pd.to_datetime(v, errors="coerce")
                    if pd.notna(ts):
                        return ts.date().isoformat()
            except Exception as exc:  # noqa: BLE001
                self.logger.debug("推导 latest_trade_date 失败（daily_table）：%s", exc)

        view = ready_view or self._resolve_ready_signals_view()
        if view and self._table_exists(view):
            try:
                with self.engine.begin() as conn:
                    df = pd.read_sql_query(
                        text(f"SELECT MAX(`sig_date`) AS latest_sig_date FROM `{view}`"),
                        conn,
                    )
                if df is not None and not df.empty:
                    v = df.iloc[0].get("latest_sig_date")
                    ts = pd.to_datetime(v, errors="coerce")
                    if pd.notna(ts):
                        return ts.date().isoformat()
            except Exception as exc:  # noqa: BLE001
                self.logger.debug("推导 latest_trade_date 失败（ready_view）：%s", exc)

        return None

    def resolve_monitor_trade_date(self, checked_at: dt.datetime) -> str:
        """解析业务交易日：非交易日则回落到最近交易日。"""

        try:
            candidate_date = checked_at.date()

            try:
                calendar = load_trading_calendar(
                    start=candidate_date - dt.timedelta(days=370),
                    end=candidate_date,
                )
            except Exception:  # noqa: BLE001
                calendar = set()

            if calendar:
                calendar_dates = [
                    dt.datetime.strptime(d, "%Y-%m-%d").date()
                    for d in calendar
                    if isinstance(d, str)
                ]
                calendar_dates = [d for d in calendar_dates if d <= candidate_date]
                if calendar_dates:
                    return max(calendar_dates).isoformat()

            view = self._resolve_ready_signals_view()
            latest_trade_date = self._resolve_latest_trade_date(ready_view=view)
            if latest_trade_date and self._is_trading_day(candidate_date.isoformat(), latest_trade_date):
                return candidate_date.isoformat()
            if latest_trade_date:
                return latest_trade_date

            target = candidate_date
            for _ in range(14):
                if target.weekday() < 5:
                    return target.isoformat()
                target -= dt.timedelta(days=1)
            return candidate_date.isoformat()
        except Exception:  # noqa: BLE001
            return checked_at.date().isoformat()

    def _load_signal_day_pct_change(
        self, signal_date: str, codes: List[str]
    ) -> Dict[str, float]:
        if not signal_date or not codes:
            return {}

        daily = self._daily_table()
        if not self._table_exists(daily):
            return {}

        stmt = text(
            f"""
            SELECT `code`, `close`, `prev_close`
            FROM (
              SELECT
                `code`, `date`, `close`,
                LAG(`close`) OVER (PARTITION BY `code` ORDER BY `date`) AS `prev_close`
              FROM `{daily}`
              WHERE `code` IN :codes AND `date` <= :d
            ) t
            WHERE `date` = :d
            """
        ).bindparams(bindparam("codes", expanding=True))

        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(stmt, conn, params={"d": signal_date, "codes": codes})
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取 %s 信号日涨幅失败（将跳过）：%s", daily, exc)
            return {}

        if df is None or df.empty:
            return {}

        out: Dict[str, float] = {}
        for _, row in df.iterrows():
            code = str(row.get("code") or "").strip()
            close = _to_float(row.get("close"))
            prev_close = _to_float(row.get("prev_close"))
            if not code or close is None or prev_close is None or prev_close <= 0:
                continue
            out[code] = (close - prev_close) / prev_close

        return out

    def load_recent_buy_signals(self) -> Tuple[str | None, List[str], pd.DataFrame]:
        """从 ready_signals_view 读取最近 BUY 信号（严格模式，无旧逻辑回退）。"""

        view = self._resolve_ready_signals_view()
        strict_ready = bool(getattr(self.params, "strict_ready_signals_required", True))
        if not view:
            msg = "未配置 ready_signals_view"
            if strict_ready:
                raise RuntimeError(msg)
            self.logger.error("%s，已跳过开盘监测。", msg)
            return None, [], pd.DataFrame()
        if not self._table_exists(view):
            msg = f"ready_signals_view={view} 不存在"
            if strict_ready:
                raise RuntimeError(msg)
            self.logger.error(
                "%s，已跳过开盘监测；请先确保 SchemaManager.ensure_all 已执行或检查配置。",
                msg,
            )
            return None, [], pd.DataFrame()

        required_view_cols = set(READY_SIGNALS_REQUIRED_COLS)
        view_cols = set(self._get_table_columns(view))
        missing_view_cols = sorted(required_view_cols - view_cols)
        if missing_view_cols:
            msg = f"ready_signals_view `{view}` 缺少关键列：{missing_view_cols}"
            if strict_ready:
                raise RuntimeError(msg)
            self.logger.error("%s，已跳过开盘监测。", msg)
            return None, [], pd.DataFrame()

        self._ready_signals_used = True

        monitor_date = str(getattr(self.params, "monitor_date", "") or "").strip()
        if not monitor_date:
            monitor_date = dt.date.today().isoformat()
        lookback = max(int(self.params.signal_lookback_days or 0), 1)

        cache_key = (monitor_date, view, lookback, str(self.params.strategy_code))
        if cache_key == self._recent_buy_signals_cache_key and self._recent_buy_signals_cache:
            return self._recent_buy_signals_cache

        latest_trade_date = self._resolve_latest_trade_date(ready_view=view)
        if latest_trade_date is None:
            self.logger.error("无法推导 latest_trade_date（daily_table/view 均不可用），已跳过开盘监测。")
            return None, [], pd.DataFrame()

        base_table = view
        signal_dates: List[str] = []
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(
                    text(
                        f"""
                        SELECT DISTINCT `sig_date`
                        FROM `{base_table}`
                        WHERE `sig_date` <= :latest_trade_date AND `strategy_code` = :strategy_code
                          AND `expires_on` >= :monitor_date
                        ORDER BY `sig_date` DESC
                        LIMIT :lookback
                        """
                    ),
                    conn,
                    params={
                        "latest_trade_date": latest_trade_date,
                        "strategy_code": self.params.strategy_code,
                        "monitor_date": monitor_date,
                        "lookback": lookback,
                    },
                )
            if df is not None and not df.empty:
                signal_dates = (
                    pd.to_datetime(df["sig_date"], errors="coerce")
                    .dropna()
                    .dt.strftime("%Y-%m-%d")
                    .tolist()
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.error("读取 %s BUY 信号日期失败：%s", view, exc)
            return None, [], pd.DataFrame()

        if not signal_dates:
            self.logger.error("未读取到任何 BUY 信号日期（latest_trade_date=%s）。", latest_trade_date)
            return latest_trade_date, [], pd.DataFrame()

        stmt = text(
            f"""
            SELECT *
            FROM `{view}`
            WHERE `sig_date` IN :dates
              AND `strategy_code` = :strategy_code
              AND `expires_on` >= :monitor_date
            ORDER BY `sig_date` DESC, `code`
            """
        ).bindparams(bindparam("dates", expanding=True))

        try:
            with self.engine.begin() as conn:
                events_df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={
                        "dates": signal_dates,
                        "strategy_code": self.params.strategy_code,
                        "monitor_date": monitor_date,
                    },
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.error("读取 %s BUY 信号失败：%s", view, exc)
            return None, [], pd.DataFrame()

        if events_df is None or events_df.empty:
            return latest_trade_date, [], pd.DataFrame()

        events_df = events_df.copy()
        events_df["code"] = events_df["code"].astype(str)
        events_df["sig_date"] = pd.to_datetime(events_df["sig_date"], errors="coerce")

        base_cols = [
            "sig_date",
            "code",
            "strategy_code",
            "expires_on",
            "close",
            "ma5",
            "ma20",
            "ma60",
            "ma250",
            "vol_ratio",
            "macd_hist",
            "kdj_k",
            "kdj_d",
            "atr14",
            "stop_ref",
            "signal",
            "final_action",
            "final_reason",
            "final_cap",
            "macd_event",
            "chip_score",
            "gdhs_delta_pct",
            "gdhs_announce_date",
            "chip_reason",
            "chip_penalty",
            "chip_note",
            "age_days",
            "deadzone_hit",
            "stale_hit",
            "fear_score",
            "wave_type",
            "extra_json",
            "reason",
            "yearline_state",
            "risk_tag",
            "risk_note",
            "industry",
            "board_name",
            "board_code",
            "industry_classification",
        ]
        for col in base_cols:
            if col not in events_df.columns:
                events_df[col] = None

        events_df["sig_date"] = events_df["sig_date"].dt.strftime("%Y-%m-%d")

        if self.params.unique_code_latest_date_only:
            before = len(events_df)
            events_df["_date_dt"] = pd.to_datetime(events_df["sig_date"], errors="coerce")
            events_df = events_df.sort_values(by=["code", "_date_dt"], ascending=[True, False])
            events_df = events_df.drop_duplicates(subset=["code"], keep="first")
            events_df = events_df.drop(columns=["_date_dt"], errors="ignore")
            dropped = before - len(events_df)
            if dropped > 0:
                self.logger.info(
                    "同一 code 多次触发 BUY：已按最新信号日去重 %s 条（保留 %s 条）。",
                    dropped,
                    len(events_df),
                )
            signal_dates = sorted(
                events_df["sig_date"].dropna().unique().tolist(), reverse=True
            )

        min_date = events_df["sig_date"].min()
        trade_age_map = self._load_trade_age_map(latest_trade_date, str(min_date), monitor_date)
        events_df["signal_age"] = events_df["sig_date"].map(trade_age_map)

        # 过滤后重算信号日列表（用于日志、行情/指标补全等后续路径）
        signal_dates = sorted(events_df["sig_date"].dropna().unique().tolist())

        try:
            for d in signal_dates:
                codes = (
                    events_df.loc[events_df["sig_date"] == d, "code"]
                    .dropna()
                    .unique()
                    .tolist()
                )
                pct_map = self._load_signal_day_pct_change(d, codes)
                mask = events_df["sig_date"] == d
                events_df.loc[mask, "_signal_day_pct_change"] = events_df.loc[
                    mask, "code"
                ].map(pct_map)
        except Exception:
            events_df["_signal_day_pct_change"] = None

        signal_prefix_map = {
            "close": "sig_close",
            "ma5": "sig_ma5",
            "ma20": "sig_ma20",
            "ma60": "sig_ma60",
            "ma250": "sig_ma250",
            "vol_ratio": "sig_vol_ratio",
            "macd_hist": "sig_macd_hist",
            "kdj_k": "sig_kdj_k",
            "kdj_d": "sig_kdj_d",
            "atr14": "sig_atr14",
            "stop_ref": "sig_stop_ref",
            "signal": "sig_signal",
            "final_action": "sig_final_action",
            "final_reason": "sig_final_reason",
            "final_cap": "sig_final_cap",
            "macd_event": "sig_macd_event",
            "chip_score": "sig_chip_score",
            "gdhs_delta_pct": "sig_gdhs_delta_pct",
            "gdhs_announce_date": "sig_gdhs_announce_date",
            "chip_reason": "sig_chip_reason",
            "chip_penalty": "sig_chip_penalty",
            "chip_note": "sig_chip_note",
            "age_days": "sig_chip_age_days",
            "deadzone_hit": "sig_chip_deadzone",
            "stale_hit": "sig_chip_stale",
            "fear_score": "sig_fear_score",
            "wave_type": "sig_wave_type",
            "reason": "sig_reason",
        }
        df = events_df.rename(
            columns={k: v for k, v in signal_prefix_map.items() if k in events_df.columns}
        )
        for src, target in signal_prefix_map.items():
            if target not in df.columns:
                df[target] = None

        self._recent_buy_signals_cache_key = cache_key
        self._recent_buy_signals_cache = (latest_trade_date, list(signal_dates), df.copy())

        return latest_trade_date, signal_dates, df

    def load_latest_snapshots(self, latest_trade_date: str, codes: List[str]) -> pd.DataFrame:
        if not latest_trade_date or not codes:
            return pd.DataFrame()

        daily_table = self._daily_table()
        if not daily_table or not self._table_exists(daily_table):
            return pd.DataFrame()

        codes = [str(c) for c in codes if str(c).strip()]
        if not codes:
            return pd.DataFrame()

        stmt = (
            text(
                f"""
                SELECT CAST(`date` AS CHAR) AS trade_date, `code`, `close`
                FROM `{daily_table}`
                WHERE `date` = :d AND `code` IN :codes
                """
            ).bindparams(bindparam("codes", expanding=True))
        )
        try:
            with self.engine.begin() as conn:
                snap_df = pd.read_sql_query(
                    stmt, conn, params={"d": latest_trade_date, "codes": codes}
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取日线快照失败，将跳过最新快照：%s", exc)
            return pd.DataFrame()

        if snap_df is None or snap_df.empty:
            return pd.DataFrame()

        snap_df["code"] = snap_df["code"].astype(str)
        merged = snap_df.copy()
        stop_df = pd.DataFrame()
        if not stop_df.empty:
            stop_df = stop_df.rename(columns={"sig_date": "trade_date"})
            stop_df["trade_date"] = pd.to_datetime(stop_df["trade_date"]).dt.strftime("%Y-%m-%d")
            merged["trade_date"] = pd.to_datetime(merged["trade_date"]).dt.strftime("%Y-%m-%d")
            merged = merged.merge(stop_df, on=["trade_date", "code"], how="left")
        merged["trade_date"] = pd.to_datetime(merged["trade_date"]).dt.strftime("%Y-%m-%d")
        return merged

    def load_latest_indicators(self, latest_trade_date: str, codes: List[str]) -> pd.DataFrame:
        if not latest_trade_date or not codes:
            return pd.DataFrame()

        table = self._indicator_table()
        if not table or not self._table_exists(table):
            return pd.DataFrame()

        codes = [str(c) for c in codes if str(c).strip()]
        if not codes:
            return pd.DataFrame()

        stmt = (
            text(
                f"""
                SELECT `trade_date`, `code`,
                       `close`, `avg_volume_20`,
                       `ma5`, `ma20`, `ma60`, `ma250`,
                       `vol_ratio`, `macd_hist`, `kdj_k`, `kdj_d`, `atr14`
                FROM `{table}`
                WHERE `trade_date` = :d AND `code` IN :codes
                """
            )
            .bindparams(bindparam("codes", expanding=True))
        )
        try:
            with self.engine.begin() as conn:
                return pd.read_sql_query(stmt, conn, params={"d": latest_trade_date, "codes": codes})
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取指标表 %s 失败：%s", table, exc)
            return pd.DataFrame()

    def load_index_history(self, latest_trade_date: str) -> dict[str, Any]:
        code = str(self.params.index_code or "").strip()
        if not code or not latest_trade_date:
            return {}

        table = "history_index_daily_kline"
        lookback = max(int(self.params.index_hist_lookback_days or 0), 1)
        df = pd.DataFrame()
        start_date = None
        try:
            end_date = pd.to_datetime(latest_trade_date).date()
            start_date = (end_date - dt.timedelta(days=lookback * 3)).isoformat()
        except Exception:
            end_date = None

        if self._table_exists(table) and end_date is not None and start_date is not None:
            stmt = text(
                f"""
                SELECT `date`,`open`,`high`,`low`,`close`,`volume`,`amount`
                FROM `{table}`
                WHERE `code` = :code AND `date` BETWEEN :start_date AND :end_date
                ORDER BY `date`
                """
            )
            try:
                with self.engine.begin() as conn:
                    df = pd.read_sql_query(
                        stmt,
                        conn,
                        params={
                            "code": code,
                            "start_date": start_date,
                            "end_date": latest_trade_date,
                        },
                    )
            except Exception as exc:  # noqa: BLE001
                self.logger.debug("读取指数日线失败，将尝试 Baostock：%s", exc)

        if df.empty and end_date is not None and start_date is not None:
            try:
                client = BaostockDataFetcher(BaostockSession())
                df = client.get_kline(code, start_date, latest_trade_date)
            except Exception as exc:  # noqa: BLE001
                self.logger.debug("Baostock 指数日线兜底失败：%s", exc)

        if df.empty:
            return {"index_code": code}

        df = df.copy()
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.dropna(subset=["date"])
        df = df.sort_values("date")
        if len(df) > lookback:
            df = df.tail(lookback)

        for col in ["open", "high", "low", "close", "volume", "amount"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        if "preclose" not in df.columns:
            df["preclose"] = df["close"].shift(1)

        dif, dea, hist = _macd(df["close"])
        df["macd_hist"] = hist
        df["atr14"] = _atr(df["high"], df["low"], df["preclose"])
        df["ma20"] = df["close"].rolling(20, min_periods=1).mean()
        df["ma60"] = df["close"].rolling(60, min_periods=1).mean()

        asof_row = df.iloc[-1]
        asof_trade_date = asof_row.get("date")
        asof_trade_date_str = (
            asof_trade_date.date().isoformat() if not pd.isna(asof_trade_date) else None
        )

        return {
            "index_code": code,
            "asof_trade_date": asof_trade_date_str,
            "asof_close": _to_float(asof_row.get("close")),
            "asof_ma20": _to_float(asof_row.get("ma20")),
            "asof_ma60": _to_float(asof_row.get("ma60")),
            "asof_macd_hist": _to_float(asof_row.get("macd_hist")),
            "asof_atr14": _to_float(asof_row.get("atr14")),
            "history": df,
        }

    def load_avg_volume(
        self,
        latest_trade_date: str,
        codes: List[str],
        window: int = 20,
        *,
        asof_trade_date_map: Dict[str, str] | None = None,
    ) -> Dict[str, float]:
        if not codes:
            return {}

        table = self._daily_table()
        if not self._table_exists(table):
            return {}

        parsed_asof_map: Dict[str, dt.date] = {}
        if isinstance(asof_trade_date_map, dict):
            for code, raw_date in asof_trade_date_map.items():
                try:
                    parsed = pd.to_datetime(raw_date).date()
                except Exception:
                    parsed = None
                if parsed is not None:
                    parsed_asof_map[str(code)] = parsed

        default_end_date = None
        if latest_trade_date:
            try:
                default_end_date = pd.to_datetime(latest_trade_date).date()
            except Exception:
                default_end_date = None

        end_dates = [default_end_date] if default_end_date else []
        end_dates.extend(parsed_asof_map.values())
        if not end_dates:
            return {}

        max_end_date = max(end_dates)
        min_start_date = min(
            [d - dt.timedelta(days=max(window * 4, 60)) for d in end_dates]
        )

        stmt = text(
            f"""
            SELECT `date`,`code`,`volume`
            FROM `{table}`
            WHERE `code` IN :codes AND `date` BETWEEN :start_date AND :end_date
            """
        ).bindparams(bindparam("codes", expanding=True))

        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={
                        "codes": codes,
                        "start_date": min_start_date.isoformat(),
                        "end_date": max_end_date.isoformat(),
                    },
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取历史成交量失败，将跳过盘中量比：%s", exc)
            return {}

        if df.empty or "volume" not in df.columns:
            return {}

        df["code"] = df["code"].astype(str)
        df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
        df["volume"] = df["volume"].apply(_to_float)
        avg_map: Dict[str, float] = {}
        for code, grp in df.groupby("code", sort=False):
            end_date = parsed_asof_map.get(code, default_end_date)
            if end_date is None:
                continue
            recent = grp[grp["date"] <= end_date]
            if recent.empty:
                continue
            top = recent.sort_values("date", ascending=False).head(window)
            volumes = top["volume"].dropna()
            if not volumes.empty:
                avg_map[code] = float(volumes.mean())
        return avg_map

    def load_previous_strength(
        self, codes: List[str], as_of: dt.datetime | None = None
    ) -> Dict[str, float]:
        table = self.params.output_table
        run_table = self.params.run_table
        if (
            not codes
            or not run_table
            or not self._table_exists(table)
            or not self._table_exists(run_table)
        ):
            return {}

        if not self._column_exists(table, "signal_strength") or not self._column_exists(
            run_table, "checked_at"
        ):
            return {}

        stmt = text(
            f"""
            SELECT t1.`code`, t1.`signal_strength`
            FROM `{table}` t1
            JOIN `{run_table}` r1
              ON t1.`run_pk` = r1.`run_pk`
            JOIN (
                SELECT e.`code`, MAX(r.`checked_at`) AS latest_checked
                FROM `{table}` e
                JOIN `{run_table}` r
                  ON e.`run_pk` = r.`run_pk`
                WHERE e.`code` IN :codes
                  AND e.`signal_strength` IS NOT NULL
                  {"AND r.`checked_at` < :as_of" if as_of else ""}
                GROUP BY e.`code`
            ) t2
              ON t1.`code` = t2.`code` AND r1.`checked_at` = t2.`latest_checked`
            WHERE t1.`code` IN :codes
              AND t1.`signal_strength` IS NOT NULL
              {"AND r1.`checked_at` < :as_of" if as_of else ""}
            """
        ).bindparams(bindparam("codes", expanding=True))

        params: dict[str, Any] = {"codes": codes}
        if as_of:
            params["as_of"] = as_of

        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(stmt, conn, params=params)
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取历史信号强度失败，将跳过强度对比：%s", exc)
            return {}

        if df.empty or "code" not in df.columns:
            return {}

        df["code"] = df["code"].astype(str)
        strength_map: Dict[str, float] = {}
        for _, row in df.iterrows():
            score = _to_float(row.get("signal_strength"))
            if score is None:
                continue
            strength_map[str(row.get("code"))] = score
        return strength_map

    def load_stock_industry_dim(self) -> pd.DataFrame:
        candidates = ["dim_stock_industry", "a_share_stock_industry"]
        for table in candidates:
            if not self._table_exists(table):
                continue
            try:
                with self.engine.begin() as conn:
                    df = pd.read_sql_query(text(f"SELECT * FROM `{table}`"), conn)
            except Exception as exc:  # noqa: BLE001
                self.logger.debug("读取 %s 失败：%s", table, exc)
                continue
            if not df.empty:
                df["code"] = df["code"].astype(str)
                return df
        return pd.DataFrame()

    def load_board_constituent_dim(self) -> pd.DataFrame:
        table = "dim_stock_board_industry"
        if not self._table_exists(table):
            return pd.DataFrame()
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(text(f"SELECT * FROM `{table}`"), conn)
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取 %s 失败：%s", table, exc)
            return pd.DataFrame()
        if not df.empty and "code" in df.columns:
            df["code"] = df["code"].astype(str)
        return df


    def load_latest_run_context_by_stage(
        self,
        monitor_date: str,
        *,
        stage: str | None = None,
    ) -> dict[str, Any] | None:
        table = getattr(self.params, "run_table", None)
        if not (table and monitor_date and self._table_exists(table)):
            return None

        stage_norm = (str(stage).strip().upper() if stage else "") or None
        where_clauses = ["`monitor_date` = :d"]
        params: dict[str, Any] = {"d": monitor_date}

        has_run_stage = self._column_exists(table, "run_stage")
        if stage_norm in {"PREOPEN", "BREAK", "POSTCLOSE", "INTRADAY"}:
            if has_run_stage:
                params["stage"] = stage_norm
                fallback_parts = []
                if stage_norm in {"PREOPEN", "BREAK", "POSTCLOSE"}:
                    fallback_parts.append("`run_id` LIKE :p")
                    params["p"] = f"{stage_norm} %"
                elif stage_norm == "INTRADAY":
                    fallback_parts.append("`run_id` NOT LIKE 'PREOPEN %'")
                    fallback_parts.append("`run_id` NOT LIKE 'BREAK %'")
                    fallback_parts.append("`run_id` NOT LIKE 'POSTCLOSE %'")

                if fallback_parts:
                    where_clauses.append(
                        "(`run_stage` = :stage OR (`run_stage` IS NULL AND "
                        + " AND ".join(fallback_parts)
                        + "))"
                    )
                else:
                    where_clauses.append("`run_stage` = :stage")
            else:
                if stage_norm in {"PREOPEN", "BREAK", "POSTCLOSE"}:
                    where_clauses.append("`run_id` LIKE :p")
                    params["p"] = f"{stage_norm} %"
                elif stage_norm == "INTRADAY":
                    where_clauses.append("`run_id` NOT LIKE 'PREOPEN %'")
                    where_clauses.append("`run_id` NOT LIKE 'BREAK %'")
                    where_clauses.append("`run_id` NOT LIKE 'POSTCLOSE %'")

        stmt = text(
            f"""
            SELECT `run_pk`,`run_id`,`params_json`
            FROM `{table}`
            WHERE {" AND ".join(where_clauses)}
            ORDER BY `run_pk` DESC
            LIMIT 1
            """
        )
        try:
            with self.engine.begin() as conn:
                row = conn.execute(stmt, params).fetchone()
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取最新 run_context 失败：%s", exc)
            return None

        if not row:
            return None

        run_pk_val = row[0]
        run_id_val = row[1]
        params_json_raw = row[2]

        parsed: dict[str, Any] = {}
        if isinstance(params_json_raw, str) and params_json_raw.strip():
            try:
                parsed = json.loads(params_json_raw)
            except Exception:
                parsed = {}
        if not isinstance(parsed, dict):
            parsed = {}

        return {
            "run_pk": int(run_pk_val) if run_pk_val is not None else None,
            "run_id": str(run_id_val) if run_id_val is not None else None,
            "params_json": params_json_raw,
            "dedup_sig": parsed.get("dedup_sig"),
            "dedup_stage": parsed.get("dedup_stage"),
        }

    def ensure_run_context(
        self,
        monitor_date: str,
        run_id: str,
        *,
        checked_at: dt.datetime | None,
        triggered_at: dt.datetime | None = None,
        run_stage: str | None = None,
        params_json: str | dict[str, Any] | None = None,
    ) -> int | None:
        table = getattr(self.params, "run_table", None)
        if not (table and monitor_date and run_id and self._table_exists(table)):
            return None

        parsed_monitor = pd.to_datetime(monitor_date, errors="coerce")
        monitor_date_val = parsed_monitor.date() if not pd.isna(parsed_monitor) else monitor_date
        params_payload = params_json
        if isinstance(params_json, dict):
            params_payload = json.dumps(params_json, ensure_ascii=False, separators=(",", ":"))
        payload = {
            "monitor_date": monitor_date_val,
            "run_id": run_id,
            "run_stage": run_stage,
            "triggered_at": triggered_at,
            "checked_at": checked_at,
            "params_json": params_payload,
        }
        columns = list(payload.keys())
        update_clause = """
            `triggered_at` = COALESCE(`triggered_at`, VALUES(`triggered_at`)),
            `checked_at` = VALUES(`checked_at`),
            `run_stage` = COALESCE(VALUES(`run_stage`), `run_stage`),
            `params_json` = COALESCE(VALUES(`params_json`), `params_json`)
        """
        stmt = text(
            f"""
            INSERT INTO `{table}` ({", ".join(f"`{c}`" for c in columns)})
            VALUES ({", ".join(f":{c}" for c in columns)})
            ON DUPLICATE KEY UPDATE {update_clause}
            """
        )
        try:
            with self.engine.begin() as conn:
                conn.execute(stmt, payload)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("写入开盘监测运行记录失败：%s", exc)
            return None

        try:
            with self.engine.begin() as conn:
                row = conn.execute(
                    text(
                        f"""
                        SELECT `run_pk`
                        FROM `{table}`
                        WHERE `monitor_date` = :d AND `run_id` = :r
                        LIMIT 1
                        """
                    ),
                    {"d": monitor_date, "r": run_id},
                ).fetchone()
            if row:
                return int(row[0])
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取 run_pk 失败：%s", exc)
        return None

    def load_open_monitor_env_row(
        self, monitor_date: str, run_pk: int | None
    ) -> pd.DataFrame | None:
        table = self.params.open_monitor_env_table
        if not (table and monitor_date and self._table_exists(table)):
            return None
        if run_pk is None:
            self.logger.error("读取环境快照时缺少 run_pk（monitor_date=%s）。", monitor_date)
            return None

        stmt = text(
            f"""
            SELECT * FROM `{table}`
            WHERE `run_pk` = :b AND `monitor_date` = :d
            LIMIT 1
            """
        )

        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(stmt, conn, params={"d": monitor_date, "b": run_pk})
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取环境快照失败：%s", exc)
            return None

        if df.empty:
            self.logger.error("未找到环境快照（monitor_date=%s, run_pk=%s）。", monitor_date, run_pk)
            return None

        return df

    def load_open_monitor_env_view_row(
        self, monitor_date: str, run_pk: int | None
    ) -> dict[str, Any] | None:
        view = getattr(self.params, "open_monitor_env_view", None)
        if not (view and monitor_date and run_pk):
            return None
        if not self._table_exists(view):
            return None

        stmt = text(
            f"""
            SELECT * FROM `{view}`
            WHERE `run_pk` = :b AND `monitor_date` = :d
            LIMIT 1
            """
        )
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(stmt, conn, params={"d": monitor_date, "b": run_pk})
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取环境视图失败：%s", exc)
            return None

        if df.empty:
            return None

        return df.iloc[0].to_dict()

    def get_latest_weekly_indicator_date(self) -> dt.date | None:
        table = self.params.weekly_indicator_table
        if not (table and self._table_exists(table)):
            return None
        benchmark_code = self.params.weekly_benchmark_code
        stmt = text(
            f"""
            SELECT MAX(`weekly_asof_trade_date`) AS latest_date
            FROM `{table}`
            WHERE `benchmark_code` = :code
            """
        )
        try:
            with self.engine.begin() as conn:
                row = conn.execute(stmt, {"code": benchmark_code}).mappings().first()
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取周线指标最新日期失败：%s", exc)
            return None
        if not row:
            return None
        latest = row.get("latest_date")
        if isinstance(latest, dt.datetime):
            return latest.date()
        if isinstance(latest, dt.date):
            return latest
        if latest:
            try:
                return pd.to_datetime(latest).date()
            except Exception:
                return None
        return None

    def get_latest_daily_market_env_date(
        self, *, benchmark_code: str = "sh.000001"
    ) -> str | None:
        table = self.params.daily_indicator_table
        if not (table and self._table_exists(table)):
            return None
        stmt = text(
            f"""
            SELECT MAX(`asof_trade_date`) AS latest_date
            FROM `{table}`
            WHERE `benchmark_code` = :code
            """
        )
        try:
            with self.engine.begin() as conn:
                row = conn.execute(stmt, {"code": benchmark_code}).mappings().first()
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取日线环境最新日期失败：%s", exc)
            return None
        if not row:
            return None
        latest = row.get("latest_date")
        if isinstance(latest, (dt.datetime, dt.date)):
            if isinstance(latest, dt.datetime):
                return latest.date().isoformat()
            return latest.isoformat()
        if latest:
            try:
                return pd.to_datetime(latest).date().isoformat()
            except Exception:
                return None
        return None

    def load_weekly_indicator(self, asof_trade_date: str) -> dict[str, Any]:
        table = self.params.weekly_indicator_table
        if not (table and self._table_exists(table)):
            return {}
        benchmark_code = self.params.weekly_benchmark_code
        stmt = text(
            f"""
            SELECT *
            FROM `{table}`
            WHERE `weekly_asof_trade_date` = :d AND `benchmark_code` = :code
            LIMIT 1
            """
        )
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={"d": asof_trade_date, "code": benchmark_code},
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取周线指标失败：%s", exc)
            return {}
        if df.empty:
            return {}
        return df.iloc[0].to_dict()

    def load_daily_market_env(
        self, *, asof_trade_date: str, benchmark_code: str = "sh.000001"
    ) -> dict[str, Any] | None:
        table = self.params.daily_indicator_table
        if not (table and self._table_exists(table)):
            return None
        stmt = text(
            f"""
            SELECT *
            FROM `{table}`
            WHERE `asof_trade_date` = :d AND `benchmark_code` = :code
            LIMIT 1
            """
        )
        try:
            with self.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={"d": asof_trade_date, "code": benchmark_code},
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取日线环境失败：%s", exc)
            return None
        if df.empty:
            return None
        return df.iloc[0].to_dict()

    def upsert_weekly_indicator(self, rows: list[dict[str, Any]]) -> int:
        if not rows:
            return 0
        table = self.params.weekly_indicator_table
        if not (table and self._table_exists(table)):
            return 0
        df = pd.DataFrame(rows)
        if df.empty:
            return 0
        columns = df.columns.tolist()
        update_cols = [
            c
            for c in columns
            if c not in {"weekly_asof_trade_date", "benchmark_code"}
        ]
        stmt = text(
            f"""
            INSERT INTO `{table}` ({", ".join(f"`{c}`" for c in columns)})
            VALUES ({", ".join(f":{c}" for c in columns)})
            ON DUPLICATE KEY UPDATE {", ".join(f"`{c}` = VALUES(`{c}`)" for c in update_cols)}
            """
        )
        df = df.where(pd.notna(df), None)
        payloads = df.to_dict(orient="records")
        try:
            with self.engine.begin() as conn:
                conn.execute(stmt, payloads)
            return len(payloads)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("写入周线指标失败：%s", exc)
            return 0

    def upsert_daily_market_env(self, rows: list[dict[str, Any]]) -> int:
        if not rows:
            return 0
        table = self.params.daily_indicator_table
        if not (table and self._table_exists(table)):
            return 0
        df = pd.DataFrame(rows)
        if df.empty:
            return 0
        columns = df.columns.tolist()
        update_cols = [
            c for c in columns if c not in {"asof_trade_date", "benchmark_code"}
        ]
        stmt = text(
            f"""
            INSERT INTO `{table}` ({", ".join(f"`{c}`" for c in columns)})
            VALUES ({", ".join(f":{c}" for c in columns)})
            ON DUPLICATE KEY UPDATE {", ".join(f"`{c}` = VALUES(`{c}`)" for c in update_cols)}
            """
        )
        payloads = df.to_dict(orient="records")
        try:
            with self.engine.begin() as conn:
                conn.execute(stmt, payloads)
            return len(payloads)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("写入日线环境失败：%s", exc)
            return 0

    def attach_cycle_phase_from_weekly(
        self, rows: list[dict[str, Any]]
    ) -> list[dict[str, Any]]:
        if not rows:
            return rows
        table = self.params.weekly_indicator_table
        if not table:
            return rows

        df_daily = pd.DataFrame(rows)
        if df_daily.empty or "asof_trade_date" not in df_daily.columns:
            return rows

        df_daily = df_daily.copy()
        df_daily["__order"] = range(len(df_daily))
        df_daily["__asof_trade_date"] = pd.to_datetime(
            df_daily["asof_trade_date"], errors="coerce"
        )
        parsed_dates = df_daily["__asof_trade_date"].dropna()
        if parsed_dates.empty:
            df_daily["cycle_phase"] = None
            df_daily["cycle_weekly_asof_trade_date"] = None
            df_daily["cycle_weekly_scene_code"] = None
            df_daily = df_daily.sort_values("__order")
            df_daily = df_daily.drop(columns=["__order", "__asof_trade_date"])
            return df_daily.to_dict(orient="records")

        min_date = parsed_dates.min()
        max_date = parsed_dates.max()
        min_date_floor = (min_date - pd.Timedelta(days=14)).date()
        max_date_val = max_date.date()

        benchmark_code = self.params.weekly_benchmark_code
        stmt = text(
            f"""
            SELECT weekly_asof_trade_date, weekly_scene_code, weekly_phase
            FROM `{table}`
            WHERE benchmark_code = :benchmark_code
              AND weekly_asof_trade_date <= :max_date
              AND weekly_asof_trade_date >= :min_date
            """
        )
        try:
            with self.engine.begin() as conn:
                df_weekly = pd.read_sql_query(
                    stmt,
                    conn,
                    params={
                        "max_date": max_date_val,
                        "min_date": min_date_floor,
                        "benchmark_code": benchmark_code,
                    },
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("读取周线场景失败：%s", exc)
            df_daily = df_daily.sort_values("__order")
            df_daily = df_daily.drop(columns=["__order", "__asof_trade_date"])
            return df_daily.to_dict(orient="records")

        if df_weekly.empty:
            df_daily["cycle_phase"] = None
            df_daily["cycle_weekly_asof_trade_date"] = None
            df_daily["cycle_weekly_scene_code"] = None
            df_daily = df_daily.sort_values("__order")
            df_daily = df_daily.drop(columns=["__order", "__asof_trade_date"])
            return df_daily.to_dict(orient="records")

        df_weekly = df_weekly.copy()
        df_weekly["weekly_asof_trade_date"] = pd.to_datetime(
            df_weekly["weekly_asof_trade_date"], errors="coerce"
        )
        df_weekly = df_weekly.dropna(subset=["weekly_asof_trade_date"])
        if df_weekly.empty:
            df_daily["cycle_phase"] = None
            df_daily["cycle_weekly_asof_trade_date"] = None
            df_daily["cycle_weekly_scene_code"] = None
            df_daily = df_daily.sort_values("__order")
            df_daily = df_daily.drop(columns=["__order", "__asof_trade_date"])
            return df_daily.to_dict(orient="records")

        df_daily_sorted = df_daily.sort_values("__asof_trade_date")
        df_weekly_sorted = df_weekly.sort_values("weekly_asof_trade_date")

        df_daily_valid = df_daily_sorted.dropna(subset=["__asof_trade_date"])
        df_daily_invalid = df_daily_sorted[df_daily_sorted["__asof_trade_date"].isna()]

        merged = pd.merge_asof(
            df_daily_valid,
            df_weekly_sorted,
            left_on="__asof_trade_date",
            right_on="weekly_asof_trade_date",
            direction="backward",
        )
        merged["cycle_weekly_asof_trade_date"] = (
            merged["weekly_asof_trade_date"].dt.date
        )
        merged["cycle_weekly_scene_code"] = merged["weekly_scene_code"]
        merged["cycle_phase"] = merged["weekly_phase"]
        merged["cycle_phase"] = merged["cycle_phase"].where(
            pd.notna(merged["cycle_phase"]), None
        )
        merged["cycle_weekly_asof_trade_date"] = merged[
            "cycle_weekly_asof_trade_date"
        ].where(pd.notna(merged["cycle_weekly_asof_trade_date"]), None)
        merged["cycle_weekly_scene_code"] = merged[
            "cycle_weekly_scene_code"
        ].where(pd.notna(merged["cycle_weekly_scene_code"]), None)

        if not df_daily_invalid.empty:
            df_daily_invalid = df_daily_invalid.copy()
            df_daily_invalid["cycle_phase"] = None
            df_daily_invalid["cycle_weekly_asof_trade_date"] = None
            df_daily_invalid["cycle_weekly_scene_code"] = None
            merged = pd.concat([merged, df_daily_invalid], ignore_index=True)

        merged = merged.sort_values("__order")
        merged = merged.drop(
            columns=[
                "__order",
                "__asof_trade_date",
                "weekly_asof_trade_date",
                "weekly_scene_code",
                "weekly_phase",
            ],
            errors="ignore",
        )
        return merged.to_dict(orient="records")

    def persist_open_monitor_env(
        self,
        env_context: dict[str, Any] | None,
        monitor_date: str,
        run_pk: int,
    ) -> None:
        if not env_context:
            return

        table = self.params.open_monitor_env_table
        if not table:
            return

        monitor_dt = pd.to_datetime(monitor_date, errors="coerce")
        monitor_date_val = monitor_dt.date() if not pd.isna(monitor_dt) else monitor_date

        payload: dict[str, Any] = {}

        payload["monitor_date"] = monitor_date_val
        payload["run_pk"] = run_pk
        env_weekly_asof = env_context.get("weekly_asof_trade_date")
        if env_weekly_asof is not None:
            parsed_weekly = pd.to_datetime(env_weekly_asof, errors="coerce")
            env_weekly_asof = parsed_weekly.date() if not pd.isna(parsed_weekly) else env_weekly_asof
        payload["env_weekly_asof_trade_date"] = env_weekly_asof
        env_daily_asof = env_context.get("daily_asof_trade_date")
        if env_daily_asof is not None:
            parsed_daily = pd.to_datetime(env_daily_asof, errors="coerce")
            env_daily_asof = parsed_daily.date() if not pd.isna(parsed_daily) else env_daily_asof
        payload["env_daily_asof_trade_date"] = env_daily_asof
        index_snapshot = {}
        if isinstance(env_context, dict):
            raw_index_snapshot = env_context.get("index_intraday")
            if isinstance(raw_index_snapshot, dict):
                index_snapshot = raw_index_snapshot
        env_index_hash = index_snapshot.get("env_index_snapshot_hash")
        payload["env_index_snapshot_hash"] = env_index_hash
        payload["env_final_gate_action"] = env_context.get("env_final_gate_action")
        if payload["env_final_gate_action"] is None:
            self.logger.error(
                "环境快照缺少 env_final_gate_action，已跳过写入（monitor_date=%s, run_pk=%s）。",
                monitor_date,
                run_pk,
            )
            return
        payload["env_live_override_action"] = env_context.get("env_live_override_action")
        payload["env_live_cap_multiplier"] = _to_float(env_context.get("env_live_cap_multiplier"))
        payload["env_live_event_tags"] = env_context.get("env_live_event_tags")
        payload["env_live_reason"] = env_context.get("env_live_reason")
        cap_candidates = [
            env_context.get("env_final_cap_pct") if isinstance(env_context, dict) else None,
            env_context.get("effective_position_hint") if isinstance(env_context, dict) else None,
            env_context.get("position_hint") if isinstance(env_context, dict) else None,
            index_snapshot.get("env_index_position_cap") if isinstance(index_snapshot, dict) else None,
        ]
        payload["env_final_cap_pct"] = next(
            (_to_float(c) for c in cap_candidates if _to_float(c) is not None),
            None,
        )
        payload["env_final_reason_json"] = env_context.get("env_final_reason_json")

        if not self._table_exists(table):
            self.logger.error("环境快照表 %s 不存在，已跳过写入。", table)
            return
        columns = list(payload.keys())
        update_cols = [c for c in columns if c not in {"monitor_date", "run_pk"}]
        col_clause = ", ".join(f"`{c}`" for c in columns)
        value_clause = ", ".join(f":{c}" for c in columns)
        update_clause = ", ".join(f"`{c}` = VALUES(`{c}`)" for c in update_cols)
        stmt = text(
            f"""
            INSERT INTO `{table}` ({col_clause})
            VALUES ({value_clause})
            ON DUPLICATE KEY UPDATE {update_clause}
            """
        )

        try:
            with self.engine.begin() as conn:
                conn.execute(stmt, payload)
            self.logger.info(
                "环境快照已写入表 %s（monitor_date=%s, run_pk=%s）",
                table,
                monitor_date,
                run_pk,
            )
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("写入环境快照失败：%s", exc)

    def _delete_existing_run_rows(
        self, table: str, monitor_date: str, run_pk: int, codes: List[str]
    ) -> int:
        if not (
            table
            and monitor_date
            and run_pk
            and codes
            and self._table_exists(table)
        ):
            return 0

        stmt = text(
            "DELETE FROM `{table}` WHERE `monitor_date` = :d AND `run_pk` = :b AND `code` IN :codes".format(
                table=table
            )
        ).bindparams(bindparam("codes", expanding=True))

        try:
            with self.engine.begin() as conn:
                result = conn.execute(
                    stmt, {"d": monitor_date, "b": run_pk, "codes": codes}
                )
                return int(getattr(result, "rowcount", 0) or 0)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("覆盖写入时清理旧快照失败：%s", exc)
            return 0

    def persist_quote_snapshots(self, df: pd.DataFrame) -> None:
        if df.empty or not self.params.write_to_db:
            return
        table = self.params.quote_table
        if not table:
            return

        keep_cols = [
            "monitor_date",
            "run_pk",
            "code",
            "live_trade_date",
            "live_open",
            "live_high",
            "live_low",
            "live_latest",
            "live_volume",
            "live_amount",
        ]
        for col in keep_cols:
            if col not in df.columns:
                df[col] = None
        quotes = df[keep_cols].copy()
        quotes["monitor_date"] = pd.to_datetime(quotes["monitor_date"]).dt.date
        quotes["live_trade_date"] = pd.to_datetime(quotes["live_trade_date"]).dt.date
        quotes["run_pk"] = pd.to_numeric(quotes["run_pk"], errors="coerce")
        quotes["code"] = quotes["code"].astype(str)
        quotes = quotes.dropna(subset=["run_pk"]).copy()
        if quotes.empty:
            return

        monitor_dates = quotes["monitor_date"].dropna().astype(str).unique().tolist()
        if self._table_exists(table) and monitor_dates:
            for monitor in monitor_dates:
                run_pks = (
                    quotes.loc[quotes["monitor_date"].astype(str) == monitor, "run_pk"]
                    .dropna()
                    .unique()
                    .tolist()
                )
                for run_pk in run_pks:
                    run_pk_codes = quotes.loc[
                        (quotes["monitor_date"].astype(str) == monitor)
                        & (quotes["run_pk"] == run_pk),
                        "code",
                    ].dropna().astype(str).unique().tolist()
                    if run_pk_codes:
                        self._delete_existing_run_rows(
                            table,
                            monitor,
                            int(run_pk),
                            run_pk_codes,
                        )

        try:
            self.db_writer.write_dataframe(quotes, table, if_exists="append")
            self.logger.info("开盘监测实时行情快照已写入表 %s：%s 条", table, len(quotes))
        except Exception as exc:  # noqa: BLE001
            self.logger.error("写入开盘监测行情快照失败：%s", exc)

    def persist_results(self, df: pd.DataFrame) -> None:
        if df.empty:
            return
        df = df.copy()
        table = self.params.output_table
        if not self.params.write_to_db:
            return

        codes = df["code"].dropna().astype(str).unique().tolist()
        table_exists = self._table_exists(table)
        table_columns = set(self._get_table_columns(table))

        for col in ["signal_strength", "strength_delta"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        if "run_pk" not in df.columns:
            df["run_pk"] = None
        df["run_pk"] = pd.to_numeric(df["run_pk"], errors="coerce")
        if "strategy_code" not in df.columns:
            df["strategy_code"] = str(getattr(self.params, "strategy_code", "") or "").strip() or None
        else:
            fallback_strategy = str(getattr(self.params, "strategy_code", "") or "").strip() or None
            df["strategy_code"] = df["strategy_code"].fillna(fallback_strategy)
        if "run_pk" in df.columns:
            missing_run_pk = df["run_pk"].isna()
            if missing_run_pk.any():
                self.logger.warning(
                    "检测到缺失 run_pk 的开盘监测结果 %s 条，已跳过写入。",
                    int(missing_run_pk.sum()),
                )
                df = df.loc[~missing_run_pk].copy()
                if df.empty:
                    return

        for col in ["monitor_date", "sig_date", "asof_trade_date", "live_trade_date"]:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors="coerce").dt.date

        monitor_date_val = (
            df["monitor_date"].iloc[0] if "monitor_date" in df.columns and not df.empty else None
        )
        monitor_date = monitor_date_val.isoformat() if monitor_date_val is not None else ""

        for col in [
            "risk_tag",
            "risk_note",
            "sig_reason",
            "action_reason",
            "status_reason",
        ]:
            if col in df.columns:
                df[col] = df[col].fillna("").astype(str).str.slice(0, 250)

        if "summary_line" in df.columns:
            df["summary_line"] = df["summary_line"].fillna("").astype(str).str.slice(0, 512)

        if "rule_hits_json" in df.columns:
            df["rule_hits_json"] = df["rule_hits_json"].fillna("").astype(str).str.slice(0, 4000)

        df["snapshot_hash"] = df.apply(lambda row: make_snapshot_hash(row.to_dict()), axis=1)
        df = df.drop_duplicates(subset=["run_pk", "strategy_code", "sig_date", "code"])

        self.persist_quote_snapshots(df)

        if table_columns:
            keep_cols = [c for c in df.columns if c in table_columns]
            df = df[keep_cols]

        if (not self.params.incremental_write) and monitor_date and codes and table_exists:
            delete_stmt = text(
                "DELETE FROM `{table}` WHERE `monitor_date` = :d AND `code` IN :codes".format(
                    table=table
                )
            ).bindparams(bindparam("codes", expanding=True))
            try:
                with self.engine.begin() as conn:
                    conn.execute(delete_stmt, {"d": monitor_date, "codes": codes})
            except Exception as exc:  # noqa: BLE001
                self.logger.warning("开盘监测表去重删除失败，将直接追加：%s", exc)

        if table_exists and monitor_date and codes:
            deleted_total = 0
            run_pks = (
                df["run_pk"].dropna().unique().tolist() if "run_pk" in df.columns else []
            )
            for run_pk_val in run_pks:
                if pd.isna(run_pk_val):
                    continue
                deleted_total += self._delete_existing_run_rows(
                    table, monitor_date, int(run_pk_val), codes
                )
            if deleted_total > 0:
                self.logger.info("检测到同 run_pk 旧快照：已覆盖删除 %s 条。", deleted_total)

        if df.empty:
            self.logger.info("本次开盘监测结果全部为重复快照，跳过写入。")
            return

        try:
            self.db_writer.write_dataframe(df, table, if_exists="append")
            self.logger.info("开盘监测结果已写入表 %s：%s 条", table, len(df))
        except Exception as exc:  # noqa: BLE001
            self.logger.error("写入开盘监测表失败：%s", exc)

    def load_open_monitor_view_data(
        self, monitor_date: str, run_pk: int | None
    ) -> pd.DataFrame:
        view = self.params.open_monitor_view or self.params.open_monitor_wide_view
        fallback_view = self.params.open_monitor_wide_view
        if not (view and monitor_date and self._table_exists(view)):
            if fallback_view and self._table_exists(fallback_view):
                view = fallback_view
            else:
                return pd.DataFrame()

        clauses = ["`monitor_date` = :d"]
        params: dict[str, Any] = {"d": monitor_date}
        if run_pk:
            clauses.append("`run_pk` = :b")
            params["b"] = run_pk
        where_clause = " AND ".join(clauses)
        stmt = text(
            f"""
            SELECT *
            FROM `{view}`
            WHERE {where_clause}
            ORDER BY `checked_at` DESC, `sig_date` DESC, `code`
            """
        )
        try:
            with self.engine.begin() as conn:
                return pd.read_sql_query(stmt, conn, params=params)
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取开盘监测视图 %s 失败：%s", view, exc)
            return pd.DataFrame()

================================================================================
FILE: ashare/open_monitor_rules.py
================================================================================

from __future__ import annotations

import json
from dataclasses import dataclass, field
from typing import Any, Callable

# 注意：open_monitor_rules 是“唯一规则引擎真源”。
# open_monitor.py 仅做 orchestrator：拼装 ctx / 拉行情 / 落库 / 导出。


ACTION_PRIORITY = ["STOP", "SKIP", "REDUCE_50%", "WAIT", "EXECUTE", "UNKNOWN"]


def _action_rank(val: str | None) -> int:
    if val is None:
        return len(ACTION_PRIORITY)
    val_norm = str(val).strip().upper()
    try:
        return ACTION_PRIORITY.index(val_norm)
    except ValueError:
        return len(ACTION_PRIORITY)


@dataclass(frozen=True)
class MarketEnvironment:
    """开盘监测所需的核心环境信息。"""

    gate_action: str | None = None  # STOP / WAIT / ALLOW
    position_cap_pct: float | None = None
    reason_json: Any | None = None
    index_snapshot_hash: str | None = None
    regime: str | None = None
    position_hint: float | None = None
    score: float | None = None
    weekly_asof_trade_date: str | None = None
    weekly_risk_level: str | None = None
    weekly_scene: str | None = None
    raw: dict[str, Any] = field(default_factory=dict)

    @classmethod
    def from_snapshot(cls, snapshot: dict[str, Any] | None) -> "MarketEnvironment":
        """从环境快照 dict 解析出强类型 MarketEnvironment。

        约定快照 key：
        - env_final_gate_action / env_final_cap_pct / env_final_reason_json
        - env_index_snapshot_hash / index_score / regime / position_hint
        - weekly_asof_trade_date / weekly_risk_level / weekly_scene_code

        如果 snapshot 非 dict，则返回空对象；关键字段缺失时直接抛错。
        """

        if not isinstance(snapshot, dict):
            return cls()

        def _to_float(val: Any) -> float | None:
            if val is None:
                return None
            if isinstance(val, bool):
                return float(val)
            if isinstance(val, (int, float)):
                fv = float(val)
                if fv != fv:  # NaN
                    return None
                return fv
            try:
                s = str(val).strip()
                if not s:
                    return None
                fv = float(s)
                if fv != fv:
                    return None
                return fv
            except Exception:
                return None

        gate_action = snapshot.get("env_final_gate_action")
        if gate_action is None:
            gate_action = snapshot.get("gate_action")
        if gate_action is not None:
            gate_action = str(gate_action).strip().upper() or None

        position_cap = snapshot.get("env_final_cap_pct")
        if position_cap is None:
            position_cap = snapshot.get("position_cap_pct")

        weekly_scene = snapshot.get("weekly_scene_code")
        if weekly_scene is None:
            weekly_scene = snapshot.get("weekly_scene")

        score_val = snapshot.get("index_score")
        regime = snapshot.get("regime")
        position_hint = snapshot.get("position_hint")

        return cls(
            gate_action=gate_action,
            position_cap_pct=_to_float(position_cap),
            reason_json=snapshot.get("env_final_reason_json", snapshot.get("reason_json")),
            index_snapshot_hash=snapshot.get("env_index_snapshot_hash", snapshot.get("index_snapshot_hash")),
            regime=regime,
            position_hint=_to_float(position_hint),
            score=_to_float(score_val),
            weekly_asof_trade_date=snapshot.get("weekly_asof_trade_date"),
            weekly_risk_level=snapshot.get("weekly_risk_level"),
            weekly_scene=weekly_scene,
            raw=snapshot,
        )


@dataclass(frozen=True)
class RuleHit:
    name: str
    category: str  # STRUCTURE / ACTION / ENV_OVERLAY
    severity: int
    reason: str
    action_override: str | None = None
    state_override: str | None = None
    cap_override: float | None = None

    def to_dict(self) -> dict[str, Any]:
        payload: dict[str, Any] = {
            "name": self.name,
            "category": self.category,
            "severity": self.severity,
            "reason": self.reason,
        }
        if self.action_override is not None:
            payload["action_override"] = self.action_override
        if self.state_override is not None:
            payload["state_override"] = self.state_override
        if self.cap_override is not None:
            payload["cap_override"] = self.cap_override
        return payload


@dataclass(frozen=True)
class Rule:
    id: str
    category: str  # STRUCTURE / ACTION / ENV_OVERLAY
    severity: int
    predicate: Callable[["DecisionContext"], bool]
    effect: Callable[["DecisionContext"], "RuleResult"]


@dataclass(frozen=True)
class RuleResult:
    reason: str
    action_override: str | None = None
    state_override: str | None = None
    cap_override: float | None = None
    env_gate_action: str | None = None
    env_position_cap: float | None = None


@dataclass
class DecisionContext:
    """每一行信号的决策上下文。"""

    # --- decision outputs ---
    state: str = "OK"
    state_reason: str = "结构/时效通过"
    state_severity: int = 0

    action: str = "EXECUTE"
    action_reason: str = "OK"
    action_rank: int = _action_rank("EXECUTE")

    entry_exposure_cap: float | None = None

    # env_overlay 输出（独立于 env.gate_action；env.gate_action 由 ACTION 类规则处理）
    env_gate_action: str | None = None
    env_gate_reason: str | None = None
    env_position_cap: float | None = None

    # --- inputs for RuleEngine predicates (optional) ---
    env: MarketEnvironment | None = None
    chip_score: float | None = None
    chip_reason: str | None = None
    chip_age_days: float | None = None
    chip_stale_hit: bool | None = None
    price_now: float | None = None
    live_gap: float | None = None
    live_pct: float | None = None
    threshold_gap_up: float | None = None
    max_gap_down: float | None = None
    sig_ma20: float | None = None
    ma20_thresh: float | None = None
    ma20_prewarn: bool = False
    ma20_prewarn_reason: str | None = None
    limit_up_trigger: float | None = None
    runup_breach: bool = False
    runup_breach_reason: str | None = None

    signal_age: int | None = None
    rule_hits: list[RuleHit] = field(default_factory=list)

    def record_hit(self, rule: Rule, result: RuleResult) -> None:
        cap_override = result.cap_override
        if result.env_position_cap is not None:
            cap_override = (
                result.env_position_cap
                if cap_override is None
                else min(cap_override, result.env_position_cap)
            )
        self.rule_hits.append(
            RuleHit(
                name=rule.id,
                category=rule.category,
                severity=rule.severity,
                reason=result.reason,
                action_override=result.action_override,
                state_override=result.state_override,
                cap_override=cap_override,
            )
        )

    def apply_state(self, rule: Rule, result: RuleResult) -> None:
        target_state = result.state_override or rule.id
        if rule.severity >= self.state_severity:
            self.state = target_state
            self.state_reason = result.reason
            self.state_severity = rule.severity
        self.record_hit(rule, result)

    def apply_action(self, rule: Rule, result: RuleResult) -> None:
        if result.cap_override is not None:
            if self.entry_exposure_cap is None:
                self.entry_exposure_cap = result.cap_override
            else:
                self.entry_exposure_cap = min(self.entry_exposure_cap, result.cap_override)

        target_action = result.action_override or self.action
        target_rank = _action_rank(target_action)
        if target_rank < self.action_rank:
            self.action = target_action
            self.action_rank = target_rank
            self.action_reason = result.reason
        elif target_rank == self.action_rank and self.action_reason in {"", None, "OK"}:
            self.action_reason = result.reason

        self.record_hit(rule, result)

    def apply_env_overlay(
        self,
        rule: Rule,
        result: RuleResult,
        merge_gate_actions: Callable[..., str | None],
    ) -> None:
        if result.env_gate_action:
            self.env_gate_action = merge_gate_actions(self.env_gate_action, result.env_gate_action)
            gate_note = result.reason or ""
            if self.env_gate_reason and gate_note:
                self.env_gate_reason = f"{self.env_gate_reason}; {gate_note}"
            elif gate_note:
                self.env_gate_reason = gate_note

        if result.env_position_cap is not None:
            if self.env_position_cap is None:
                self.env_position_cap = result.env_position_cap
            else:
                self.env_position_cap = min(self.env_position_cap, result.env_position_cap)

        self.record_hit(rule, result)

    def finalize_env_overlay(self, merge_gate_actions: Callable[..., str | None]) -> None:
        if self.env_position_cap is not None:
            if self.entry_exposure_cap is None:
                self.entry_exposure_cap = self.env_position_cap
            else:
                self.entry_exposure_cap = min(self.entry_exposure_cap, self.env_position_cap)

        final_gate = merge_gate_actions(self.env_gate_action)
        if final_gate is not None:
            self.env_gate_action = final_gate

        if final_gate in {"STOP", "WAIT"} and _action_rank(self.action) > _action_rank("WAIT"):
            self.action = "WAIT"
            self.action_rank = _action_rank("WAIT")
            if self.action_reason in {"", None, "OK"} and self.env_gate_reason:
                self.action_reason = self.env_gate_reason

        if not self.rule_hits:
            self.rule_hits.append(
                RuleHit(
                    name="OK",
                    category="FINAL",
                    severity=0,
                    reason="OK",
                )
            )

    def export_result(self) -> "DecisionResult":
        state = self.state
        if self.action == "SKIP":
            state = "INVALID"
        elif self.action == "WAIT":
            state = "PENDING"
        elif self.action == "UNKNOWN":
            state = "UNKNOWN"

        rule_hits_json = json.dumps(
            [hit.to_dict() for hit in self.rule_hits],
            ensure_ascii=False,
            separators=(",", ":"),
        )
        status_reason = self.action_reason
        summary_line = f"{state} {self.action} | {self.action_reason}"

        entry_exposure_cap = self.entry_exposure_cap
        if self.action != "EXECUTE":
            entry_exposure_cap = 0.0

        return DecisionResult(
            state=state,
            action=self.action,
            action_reason=self.action_reason,
            status_reason=status_reason,
            entry_exposure_cap=entry_exposure_cap,
            env_gate_action=self.env_gate_action,
            rule_hits_json=rule_hits_json,
            summary_line=summary_line,
        )


@dataclass(frozen=True)
class DecisionResult:
    state: str
    action: str
    action_reason: str
    status_reason: str
    entry_exposure_cap: float | None
    env_gate_action: str | None
    rule_hits_json: str
    summary_line: str


class RuleEngine:
    """开盘监测规则引擎（唯一实现）。"""

    def __init__(self, merge_gate_actions: Callable[..., str | None]) -> None:
        self.merge_gate_actions = merge_gate_actions

    def apply(self, ctx: DecisionContext, rules: list[Rule]) -> None:
        by_category: dict[str, list[Rule]] = {
            "STRUCTURE": [],
            "ACTION": [],
            "ENV_OVERLAY": [],
        }
        for rule in rules:
            bucket = by_category.get(rule.category)
            if bucket is not None:
                bucket.append(rule)

        for category in ("STRUCTURE", "ACTION", "ENV_OVERLAY"):
            for rule in sorted(by_category[category], key=lambda r: r.severity, reverse=True):
                if rule.predicate(ctx):
                    result = rule.effect(ctx)
                    if category == "STRUCTURE":
                        ctx.apply_state(rule, result)
                    elif category == "ACTION":
                        ctx.apply_action(rule, result)
                    else:
                        ctx.apply_env_overlay(rule, result, self.merge_gate_actions)

        ctx.finalize_env_overlay(self.merge_gate_actions)

================================================================================
FILE: ashare/schema_manager.py
================================================================================

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Dict, Iterable

from sqlalchemy import inspect, text
from sqlalchemy.engine import Engine

from .config import get_section
from .db import DatabaseConfig, MySQLWriter

STRATEGY_CODE_MA5_MA20_TREND = "MA5_MA20_TREND"

# 维度/事实视图：拆分 a_share_universe
VIEW_DIM_STOCK_BASIC = "dim_stock_basic"
VIEW_FACT_STOCK_DAILY = "fact_stock_daily"
VIEW_DIM_INDEX_MEMBERSHIP_SNAPSHOT = "dim_index_membership_snapshot"
TABLE_A_SHARE_UNIVERSE = "a_share_universe"

# 统一策略信号体系表命名：按单一职责拆分
TABLE_STRATEGY_INDICATOR_DAILY = "strategy_indicator_daily"
TABLE_STRATEGY_SIGNAL_EVENTS = "strategy_signal_events"
TABLE_STRATEGY_CANDIDATES = "strategy_candidates"
# 策略准备就绪信号（含筹码）
TABLE_STRATEGY_READY_SIGNALS = "strategy_ready_signals"
TABLE_STRATEGY_CHIP_FILTER = "strategy_chip_filter"
TABLE_STRATEGY_TRADE_METRICS = "strategy_trade_metrics"
VIEW_STRATEGY_BACKTEST = "v_backtest"
VIEW_STRATEGY_PNL = "v_pnl"

# 开盘监测输出
TABLE_STRATEGY_OPEN_MONITOR_EVAL = "strategy_open_monitor_eval"
TABLE_STRATEGY_OPEN_MONITOR_ENV = "strategy_open_monitor_env"
TABLE_STRATEGY_WEEKLY_MARKET_ENV = "strategy_weekly_market_env"
WEEKLY_MARKET_BENCHMARK_CODE = "sh.000001"
TABLE_STRATEGY_DAILY_MARKET_ENV = "strategy_daily_market_env"
TABLE_STRATEGY_OPEN_MONITOR_QUOTE = "strategy_open_monitor_quote"
TABLE_STRATEGY_OPEN_MONITOR_RUN = "strategy_open_monitor_run"
VIEW_STRATEGY_OPEN_MONITOR_WIDE = "v_strategy_open_monitor_wide"
# 开盘监测环境视图（env 快照）
VIEW_STRATEGY_OPEN_MONITOR_ENV = "v_strategy_open_monitor_env"
# 开盘监测默认查询视图（精简字段；完整字段请查 v_strategy_open_monitor_wide）
VIEW_STRATEGY_OPEN_MONITOR = "v_strategy_open_monitor"

READY_SIGNALS_COLUMNS: Dict[str, str] = {
    "sig_date": "DATE NOT NULL",
    "code": "VARCHAR(20) NOT NULL",
    "strategy_code": "VARCHAR(32) NOT NULL",
    "signal": "VARCHAR(64) NULL",
    "final_action": "VARCHAR(16) NULL",
    "final_reason": "VARCHAR(255) NULL",
    "final_cap": "DOUBLE NULL",
    "reason": "VARCHAR(255) NULL",
    "risk_tag": "VARCHAR(255) NULL",
    "risk_note": "VARCHAR(255) NULL",
    "extra_json": "TEXT NULL",
    "valid_days": "INT NULL",
    "expires_on": "DATE NULL",
    "stop_ref": "DOUBLE NULL",
    "macd_event": "VARCHAR(32) NULL",
    "fear_score": "DOUBLE NULL",
    "wave_type": "VARCHAR(64) NULL",
    "yearline_state": "VARCHAR(50) NULL",
    "close": "DOUBLE NULL",
    "ma5": "DOUBLE NULL",
    "ma20": "DOUBLE NULL",
    "ma60": "DOUBLE NULL",
    "ma250": "DOUBLE NULL",
    "vol_ratio": "DOUBLE NULL",
    "macd_hist": "DOUBLE NULL",
    "kdj_k": "DOUBLE NULL",
    "kdj_d": "DOUBLE NULL",
    "atr14": "DOUBLE NULL",
    "avg_volume_20": "DOUBLE NULL",
    "gdhs_delta_pct": "DOUBLE NULL",
    "gdhs_announce_date": "DATE NULL",
    "chip_score": "DOUBLE NULL",
    "chip_reason": "VARCHAR(255) NULL",
    "chip_penalty": "DOUBLE NULL",
    "chip_note": "VARCHAR(255) NULL",
    "age_days": "INT NULL",
    "deadzone_hit": "TINYINT(1) NULL",
    "stale_hit": "TINYINT(1) NULL",
    "industry": "VARCHAR(255) NULL",
    "industry_classification": "VARCHAR(255) NULL",
    "board_name": "VARCHAR(255) NULL",
    "board_code": "VARCHAR(64) NULL",
}

@dataclass(frozen=True)
class TableNames:
    indicator_table: str
    signal_events_table: str
    ready_signals_view: str
    open_monitor_eval_table: str
    open_monitor_env_table: str
    open_monitor_run_table: str
    open_monitor_env_view: str
    open_monitor_view: str
    open_monitor_wide_view: str
    open_monitor_quote_table: str
    weekly_indicator_table: str
    daily_indicator_table: str


def _to_bool(value: object, default: bool = False) -> bool:
    if isinstance(value, bool):
        return value
    if value is None:
        return default
    if isinstance(value, (int, float)):
        return bool(value)
    if isinstance(value, str):
        lowered = value.strip().lower()
        if lowered in {"1", "true", "yes", "y", "on"}:
            return True
        if lowered in {"0", "false", "no", "n", "off"}:
            return False
    return default


class SchemaManager:
    def __init__(self, engine: Engine, *, db_name: str | None = None) -> None:
        self.engine = engine
        self.db_name = db_name or engine.url.database
        self.logger = logging.getLogger(__name__)

    def ensure_all(self) -> None:
        tables = self._resolve_table_names()

        self._ensure_history_daily_kline_table()
        self._ensure_history_index_daily_kline_table()
        self._ensure_external_signal_tables()

        self._ensure_dim_stock_basic_view()
        self._ensure_fact_stock_daily_view()
        self._ensure_index_membership_view()
        self._ensure_universe_table()

        self._ensure_indicator_table(tables.indicator_table)
        self._ensure_signal_events_table(tables.signal_events_table)
        self._ensure_strategy_candidates_table()
        self._ensure_trade_metrics_table()
        self._ensure_backtest_view()
        self._ensure_chip_filter_table()
        self._ensure_ready_signals_view(
            tables.ready_signals_view,
            tables.signal_events_table,
            tables.indicator_table,
            TABLE_STRATEGY_CHIP_FILTER,
        )
        self._ensure_v_pnl_view()

        self._ensure_open_monitor_eval_table(tables.open_monitor_eval_table)
        self._ensure_open_monitor_run_table(tables.open_monitor_run_table)
        self._ensure_open_monitor_quote_table(tables.open_monitor_quote_table)
        self._ensure_weekly_indicator_table(tables.weekly_indicator_table)
        self._ensure_daily_market_env_table(tables.daily_indicator_table)
        self._ensure_open_monitor_env_table(tables.open_monitor_env_table)
        self._ensure_open_monitor_env_view(
            tables.open_monitor_env_view,
            tables.open_monitor_env_table,
            tables.weekly_indicator_table,
            tables.daily_indicator_table,
            tables.open_monitor_run_table,
            tables.open_monitor_quote_table,
        )

        self._ensure_open_monitor_view(
            tables.open_monitor_view,
            tables.open_monitor_wide_view,
            tables.open_monitor_eval_table,
            tables.open_monitor_env_view,
            tables.open_monitor_quote_table,
            tables.open_monitor_run_table,
        )

    def get_table_names(self) -> TableNames:
        return self._resolve_table_names()

    def _resolve_table_names(self) -> TableNames:
        strat_cfg = get_section("strategy_ma5_ma20_trend") or {}
        open_monitor_cfg = get_section("open_monitor") or {}

        default_indicator = strat_cfg.get("indicator_table", TABLE_STRATEGY_INDICATOR_DAILY)
        indicator_table = (
                str(open_monitor_cfg.get("indicator_table", default_indicator)).strip()
                or TABLE_STRATEGY_INDICATOR_DAILY
        )
        default_events = (
                strat_cfg.get("signal_events_table")
                or strat_cfg.get("signals_table")
                or TABLE_STRATEGY_SIGNAL_EVENTS
        )
        signal_events_table = (
                str(open_monitor_cfg.get("signal_events_table", default_events)).strip()
                or TABLE_STRATEGY_SIGNAL_EVENTS
        )
        ready_signals_view = (
                str(open_monitor_cfg.get("ready_signals_view", TABLE_STRATEGY_READY_SIGNALS)).strip()
                or TABLE_STRATEGY_READY_SIGNALS
        )
        open_monitor_eval_table = (
                str(open_monitor_cfg.get("output_table", TABLE_STRATEGY_OPEN_MONITOR_EVAL)).strip()
                or TABLE_STRATEGY_OPEN_MONITOR_EVAL
        )
        open_monitor_run_table = (
                str(open_monitor_cfg.get("run_table", TABLE_STRATEGY_OPEN_MONITOR_RUN)).strip()
                or TABLE_STRATEGY_OPEN_MONITOR_RUN
        )
        open_monitor_env_table = (
            str(
                open_monitor_cfg.get(
                    "open_monitor_env_table",
                    TABLE_STRATEGY_OPEN_MONITOR_ENV,
                )
            ).strip()
            or TABLE_STRATEGY_OPEN_MONITOR_ENV
        )
        open_monitor_env_view = (
                str(
                    open_monitor_cfg.get(
                        "open_monitor_env_view",
                        VIEW_STRATEGY_OPEN_MONITOR_ENV,
                    )
                ).strip()
                or VIEW_STRATEGY_OPEN_MONITOR_ENV
        )
        open_monitor_view = (
                str(
                    open_monitor_cfg.get(
                        "open_monitor_view",
                        VIEW_STRATEGY_OPEN_MONITOR,
                    )
                ).strip()
                or VIEW_STRATEGY_OPEN_MONITOR
        )
        open_monitor_wide_view = (
                str(
                    open_monitor_cfg.get(
                        "open_monitor_wide_view",
                        VIEW_STRATEGY_OPEN_MONITOR_WIDE,
                    )
                ).strip()
                or VIEW_STRATEGY_OPEN_MONITOR_WIDE
        )
        open_monitor_quote_table = (
                str(
                    open_monitor_cfg.get(
                        "quote_table",
                        TABLE_STRATEGY_OPEN_MONITOR_QUOTE,
                    )
                ).strip()
                or TABLE_STRATEGY_OPEN_MONITOR_QUOTE
        )
        weekly_indicator_table = (
            str(
                open_monitor_cfg.get(
                    "weekly_indicator_table",
                    TABLE_STRATEGY_WEEKLY_MARKET_ENV,
                )
            ).strip()
            or TABLE_STRATEGY_WEEKLY_MARKET_ENV
        )
        if weekly_indicator_table == "strategy_weekly_market_indicator":
            weekly_indicator_table = TABLE_STRATEGY_WEEKLY_MARKET_ENV
        daily_indicator_table = (
                str(
                    open_monitor_cfg.get(
                        "daily_indicator_table",
                        TABLE_STRATEGY_DAILY_MARKET_ENV,
                    )
                ).strip()
                or TABLE_STRATEGY_DAILY_MARKET_ENV
        )

        return TableNames(
            indicator_table=indicator_table,
            signal_events_table=signal_events_table,
            ready_signals_view=ready_signals_view,
            open_monitor_eval_table=open_monitor_eval_table,
            open_monitor_env_table=open_monitor_env_table,
            open_monitor_run_table=open_monitor_run_table,
            open_monitor_env_view=open_monitor_env_view,
            open_monitor_view=open_monitor_view,
            open_monitor_wide_view=open_monitor_wide_view,
            open_monitor_quote_table=open_monitor_quote_table,
            weekly_indicator_table=weekly_indicator_table,
            daily_indicator_table=daily_indicator_table,
        )

    def _rename_table_if_needed(self, old_name: str, new_name: str) -> None:
        if not old_name or not new_name or old_name == new_name:
            return
        if not self._table_exists(old_name):
            return
        if self._table_exists(new_name):
            self.logger.warning(
                "检测到旧表 %s 但新表 %s 已存在，跳过重命名。",
                old_name,
                new_name,
            )
            return
        with self.engine.begin() as conn:
            conn.execute(text(f"RENAME TABLE `{old_name}` TO `{new_name}`"))
        self.logger.info("已将旧表 %s 重命名为 %s。", old_name, new_name)

    # ---------- generic helpers ----------
    def _table_exists(self, table: str) -> bool:
        with self.engine.connect() as conn:
            inspector = inspect(conn)
            return inspector.has_table(table)

    def _view_exists(self, view: str) -> bool:
        condition = "TABLE_SCHEMA = :schema" if self.db_name else "TABLE_SCHEMA = DATABASE()"
        stmt = text(
            f"""
            SELECT COUNT(*) AS cnt
            FROM information_schema.TABLES
            WHERE {condition} AND TABLE_NAME = :view AND TABLE_TYPE = 'VIEW'
            """
        )
        params: Dict[str, str] = {"view": view}
        if self.db_name:
            params["schema"] = str(self.db_name)
        with self.engine.connect() as conn:
            row = conn.execute(stmt, params).mappings().first()
        return bool(row and row.get("cnt"))

    def _relation_type(self, name: str) -> str | None:
        if not name:
            return None
        condition = "TABLE_SCHEMA = :schema" if self.db_name else "TABLE_SCHEMA = DATABASE()"
        stmt = text(
            f"""
            SELECT TABLE_TYPE
            FROM information_schema.TABLES
            WHERE {condition} AND TABLE_NAME = :name
            """
        )
        params: Dict[str, str] = {"name": name}
        if self.db_name:
            params["schema"] = str(self.db_name)
        with self.engine.connect() as conn:
            row = conn.execute(stmt, params).mappings().first()
        if not row:
            return None
        return str(row.get("TABLE_TYPE") or "").strip() or None

    def _index_exists(self, table: str, index: str) -> bool:
        condition = "table_schema = :schema" if self.db_name else "table_schema = DATABASE()"
        stmt = text(
            f"""
            SELECT COUNT(*) AS cnt
            FROM information_schema.statistics
            WHERE {condition} AND table_name = :table AND index_name = :index
            """
        )
        params: Dict[str, str] = {"table": table, "index": index}
        if self.db_name:
            params["schema"] = str(self.db_name)
        with self.engine.connect() as conn:
            row = conn.execute(stmt, params).mappings().first()
        return bool(row and row.get("cnt"))

    def _column_meta(self, table: str) -> Dict[str, Dict[str, str]]:
        condition = "TABLE_SCHEMA = :schema" if self.db_name else "TABLE_SCHEMA = DATABASE()"
        stmt = text(
            f"""
            SELECT COLUMN_NAME, DATA_TYPE, COLUMN_TYPE, CHARACTER_MAXIMUM_LENGTH
            FROM information_schema.COLUMNS
            WHERE {condition} AND TABLE_NAME = :table
            """
        )
        params: Dict[str, str] = {"table": table}
        if self.db_name:
            params["schema"] = str(self.db_name)
        with self.engine.connect() as conn:
            rows = conn.execute(stmt, params).mappings().all()
        meta: Dict[str, Dict[str, str]] = {}
        for row in rows:
            meta[str(row["COLUMN_NAME"])] = {
                "data_type": str(row.get("DATA_TYPE") or "").lower(),
                "column_type": str(row.get("COLUMN_TYPE") or "").lower(),
                "char_len": str(row.get("CHARACTER_MAXIMUM_LENGTH") or ""),
            }
        return meta

    def _primary_key_columns(self, table: str) -> list[str]:
        condition = "TABLE_SCHEMA = :schema" if self.db_name else "TABLE_SCHEMA = DATABASE()"
        stmt = text(
            f"""
            SELECT COLUMN_NAME
            FROM information_schema.KEY_COLUMN_USAGE
            WHERE {condition}
              AND TABLE_NAME = :table
              AND CONSTRAINT_NAME = 'PRIMARY'
            ORDER BY ORDINAL_POSITION
            """
        )
        params: Dict[str, str] = {"table": table}
        if self.db_name:
            params["schema"] = str(self.db_name)
        with self.engine.connect() as conn:
            rows = conn.execute(stmt, params).fetchall()
        return [str(row[0]) for row in rows]

    def _ensure_primary_key(self, table: str, columns: Iterable[str]) -> None:
        target = [str(col) for col in columns]
        if not target:
            return
        existing = self._primary_key_columns(table)
        if existing == target:
            return
        with self.engine.begin() as conn:
            if existing:
                conn.execute(text(f"ALTER TABLE `{table}` DROP PRIMARY KEY"))
            pk_clause = ", ".join(f"`{col}`" for col in target)
            conn.execute(text(f"ALTER TABLE `{table}` ADD PRIMARY KEY ({pk_clause})"))
        self.logger.info("表 %s 主键已调整为 (%s)。", table, ", ".join(target))

    def _add_missing_columns(self, table: str, columns: Dict[str, str]) -> None:
        existing = set(self._column_meta(table).keys())
        to_add = [(col, ddl) for col, ddl in columns.items() if col not in existing]
        if not to_add:
            return
        with self.engine.begin() as conn:
            for col, ddl in to_add:
                conn.execute(text(f"ALTER TABLE `{table}` ADD COLUMN `{col}` {ddl}"))
                self.logger.info("表 %s 已新增列 %s。", table, col)

    def _drop_columns(self, table: str, columns: Iterable[str]) -> None:
        existing = set(self._column_meta(table).keys())
        to_drop = [col for col in columns if col in existing]
        if not to_drop:
            return
        with self.engine.begin() as conn:
            for col in to_drop:
                conn.execute(text(f"ALTER TABLE `{table}` DROP COLUMN `{col}`"))
                self.logger.info("表 %s 已删除旧列 %s。", table, col)

    def _ensure_varchar_length(self, table: str, column: str, length: int) -> None:
        meta = self._column_meta(table)
        if column not in meta:
            return
        info = meta[column]
        data_type = info["data_type"]
        column_type = info["column_type"]
        char_len_raw = info["char_len"]

        mysql_safe_max = 16383
        safe_len = min(length, mysql_safe_max)
        is_text_like = ("text" in data_type) or ("blob" in data_type) or ("text" in column_type)
        is_varchar_like = data_type in {"varchar", "char"}
        if not (is_text_like or is_varchar_like):
            return

        try:
            current_len = int(char_len_raw or 0)
        except ValueError:
            current_len = 0

        if is_varchar_like and current_len >= safe_len:
            return

        target_len = safe_len if is_text_like else max(current_len, safe_len)
        with self.engine.begin() as conn:
            conn.execute(
                text(
                    f"ALTER TABLE `{table}` MODIFY COLUMN `{column}` VARCHAR({target_len})"
                )
            )
        self.logger.info("表 %s.%s 已调整为 VARCHAR(%s)。", table, column, target_len)

    def _ensure_numeric_column(self, table: str, column: str, definition: str) -> None:
        meta = self._column_meta(table)
        info = meta.get(column)
        if not info:
            self._add_missing_columns(table, {column: definition})
            return

        numeric_types = {
            "double",
            "float",
            "decimal",
            "int",
            "bigint",
            "smallint",
            "tinyint",
        }
        if info["data_type"] in numeric_types:
            return

        with self.engine.begin() as conn:
            conn.execute(text(f"ALTER TABLE `{table}` MODIFY COLUMN `{column}` {definition}"))
        self.logger.info("表 %s.%s 已调整为数值类型 %s。", table, column, definition)

    def _ensure_datetime_column(self, table: str, column: str) -> None:
        meta = self._column_meta(table)
        info = meta.get(column)
        if not info:
            self._add_missing_columns(table, {column: "DATETIME(6) NULL"})
            return
        if info["data_type"] != "datetime" or "datetime(6)" not in info["column_type"]:
            with self.engine.begin() as conn:
                conn.execute(
                    text(f"ALTER TABLE `{table}` MODIFY COLUMN `{column}` DATETIME(6) NULL")
                )
            self.logger.info("表 %s.%s 已调整为 DATETIME(6)。", table, column)

    def _ensure_date_column(self, table: str, column: str, *, not_null: bool) -> None:
        meta = self._column_meta(table)
        info = meta.get(column)
        target_def = "DATE NOT NULL" if not_null else "DATE NULL"
        if not info:
            self._add_missing_columns(table, {column: target_def})
            return
        if info["data_type"] != "date":
            with self.engine.begin() as conn:
                conn.execute(text(f"ALTER TABLE `{table}` MODIFY COLUMN `{column}` {target_def}"))
            self.logger.info("表 %s.%s 已调整为 DATE。", table, column)

    def _create_table(
            self,
            table: str,
            columns: Dict[str, str],
            *,
            primary_key: Iterable[str] | None = None,
    ) -> None:
        cols_clause = ",\n".join(f"  `{name}` {ddl}" for name, ddl in columns.items())
        pk_clause = ""
        if primary_key:
            pk = ", ".join(f"`{c}`" for c in primary_key)
            pk_clause = f",\n  PRIMARY KEY ({pk})"
        ddl = f"CREATE TABLE IF NOT EXISTS `{table}` (\n{cols_clause}{pk_clause}\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;"
        with self.engine.begin() as conn:
            conn.execute(text(ddl))
        self.logger.info("已创建表 %s。", table)

    def _drop_relation(self, name: str) -> None:
        if not name:
            return
        with self.engine.begin() as conn:
            conn.execute(text(f"DROP VIEW IF EXISTS `{name}`"))

    def _drop_relation_any(self, name: str) -> None:
        """删除同名关系对象（先视图后表），用于视图/表互转兼容。"""

        if not name:
            return
        with self.engine.begin() as conn:
            conn.execute(text(f"DROP VIEW IF EXISTS `{name}`"))
            conn.execute(text(f"DROP TABLE IF EXISTS `{name}`"))

    # ---------- History tables ----------
    def _ensure_history_daily_kline_table(self) -> None:
        self._ensure_history_kline_table("history_daily_kline")

    def _ensure_history_index_daily_kline_table(self) -> None:
        self._ensure_history_kline_table("history_index_daily_kline")

    def _ensure_history_kline_table(self, table: str) -> None:
        columns = {
            "date": "VARCHAR(10) NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "trade_date": "DATE NOT NULL",
            "open": "DOUBLE NULL",
            "high": "DOUBLE NULL",
            "low": "DOUBLE NULL",
            "close": "DOUBLE NULL",
            "preclose": "DOUBLE NULL",
            "volume": "DOUBLE NULL",
            "amount": "DOUBLE NULL",
            "pctChg": "DOUBLE NULL",
            "adjustflag": "VARCHAR(8) NULL",
            "tradestatus": "VARCHAR(8) NULL",
            "isST": "VARCHAR(8) NULL",
            "source": "VARCHAR(16) NULL",
            "created_at": "DATETIME(6) NULL",
        }

        if not self._table_exists(table):
            self._create_table(table, columns)
        else:
            alter_columns = columns.copy()
            alter_columns["trade_date"] = "DATE NULL"
            self._add_missing_columns(table, alter_columns)
            self._ensure_varchar_length(table, "date", 10)
            self._ensure_varchar_length(table, "code", 20)
            self._ensure_date_column(table, "trade_date", not_null=False)
            self._backfill_trade_date(table)
            self._ensure_trade_date_not_null(table)

        self._ensure_history_kline_indexes(table)

    def _backfill_trade_date(self, table: str) -> None:
        meta = self._column_meta(table)
        if "trade_date" not in meta or "date" not in meta:
            return
        stmt = text(
            f"""
            UPDATE `{table}`
            SET `trade_date` = STR_TO_DATE(`date`, '%Y-%m-%d')
            WHERE `trade_date` IS NULL
              AND `date` REGEXP '^[0-9]{{4}}-[0-9]{{2}}-[0-9]{{2}}$'
            """
        )
        with self.engine.begin() as conn:
            result = conn.execute(stmt)
        updated = int(getattr(result, "rowcount", 0) or 0)
        if updated:
            self.logger.info("表 %s 已回填 trade_date：%s 条。", table, updated)

    def _ensure_trade_date_not_null(self, table: str) -> None:
        meta = self._column_meta(table)
        if "trade_date" not in meta:
            return
        stmt = text(f"SELECT COUNT(*) AS cnt FROM `{table}` WHERE `trade_date` IS NULL")
        with self.engine.connect() as conn:
            row = conn.execute(stmt).mappings().first()
        if row and int(row.get("cnt") or 0) > 0:
            raise RuntimeError(
                f"{table}.trade_date 仍存在空值，请先清理异常 date 再重试。"
            )
        self._ensure_date_column(table, "trade_date", not_null=True)

    def _ensure_history_kline_indexes(self, table: str) -> None:
        trade_date_index = f"idx_{table}_trade_date"
        if not self._index_exists(table, trade_date_index):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE INDEX `{trade_date_index}`
                        ON `{table}` (`trade_date`)
                        """
                    )
                )
            self.logger.info("表 %s 已新增索引 %s。", table, trade_date_index)

        code_date_index = f"idx_{table}_code_trade_date"
        if not self._index_exists(table, code_date_index):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE INDEX `{code_date_index}`
                        ON `{table}` (`code`, `trade_date`)
                        """
                    )
                )
            self.logger.info("表 %s 已新增索引 %s。", table, code_date_index)

        unique_index = f"ux_{table}_code_trade_date"
        if not self._index_exists(table, unique_index):
            dup_stmt = text(
                f"""
                SELECT `code`, `trade_date`, COUNT(*) AS cnt
                FROM `{table}`
                WHERE `trade_date` IS NOT NULL
                GROUP BY `code`, `trade_date`
                HAVING cnt > 1
                LIMIT 20
                """
            )
            with self.engine.connect() as conn:
                dup_rows = conn.execute(dup_stmt).mappings().all()
            if dup_rows:
                sample = ", ".join(
                    f"{row['code']}@{row['trade_date']}({row['cnt']})"
                    for row in dup_rows
                )
                self.logger.error(
                    "表 %s 存在重复 (code, trade_date)：%s。", table, sample
                )
                raise RuntimeError(
                    f"{table} 存在重复 (code, trade_date)，请先清理后再创建唯一索引。"
                )
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE UNIQUE INDEX `{unique_index}`
                        ON `{table}` (`code`, `trade_date`)
                        """
                    )
                )
            self.logger.info("表 %s 已新增唯一索引 %s。", table, unique_index)

    # ---------- External signal tables ----------
    def _ensure_external_signal_tables(self) -> None:
        self._ensure_lhb_detail_table()
        self._ensure_margin_detail_table()
        self._ensure_gdhs_tables()

    def _ensure_yyyymmdd_date_column(
        self, table: str, column: str, *, not_null: bool
    ) -> None:
        meta = self._column_meta(table)
        info = meta.get(column)
        target_def = "DATE NOT NULL" if not_null else "DATE NULL"
        if not info:
            self._add_missing_columns(table, {column: target_def})
            return
        if info["data_type"] == "date":
            if not_null:
                self._ensure_date_column(table, column, not_null=True)
            return

        invalid_stmt = text(
            f"""
            SELECT COUNT(*) AS cnt
            FROM `{table}`
            WHERE `{column}` IS NOT NULL
              AND `{column}` NOT REGEXP '^[0-9]{{8}}$'
              AND `{column}` NOT REGEXP '^[0-9]{{4}}-[0-9]{{2}}-[0-9]{{2}}$'
            """
        )
        with self.engine.connect() as conn:
            row = conn.execute(invalid_stmt).mappings().first()
        invalid = int(row.get("cnt") or 0) if row else 0
        if invalid:
            self.logger.warning(
                "表 %s.%s 存在非日期值(%s)，已跳过 DATE 转换。",
                table,
                column,
                invalid,
            )
            return

        update_ymd = text(
            f"""
            UPDATE `{table}`
            SET `{column}` = STR_TO_DATE(`{column}`, '%Y%m%d')
            WHERE `{column}` REGEXP '^[0-9]{{8}}$'
            """
        )
        update_iso = text(
            f"""
            UPDATE `{table}`
            SET `{column}` = STR_TO_DATE(`{column}`, '%Y-%m-%d')
            WHERE `{column}` REGEXP '^[0-9]{{4}}-[0-9]{{2}}-[0-9]{{2}}$'
            """
        )
        with self.engine.begin() as conn:
            conn.execute(update_ymd)
            conn.execute(update_iso)
            conn.execute(text(f"ALTER TABLE `{table}` MODIFY COLUMN `{column}` {target_def}"))
        self.logger.info("表 %s.%s 已调整为 %s。", table, column, target_def)

    def _ensure_unique_index_if_clean(
        self, table: str, index: str, columns: Iterable[str]
    ) -> None:
        if self._index_exists(table, index):
            return
        cols = [str(c) for c in columns if c]
        if not cols:
            return
        col_sql = ", ".join(f"`{c}`" for c in cols)
        dup_cols = ", ".join(f"`{c}`" for c in cols)
        dup_stmt = text(
            f"""
            SELECT {dup_cols}, COUNT(*) AS cnt
            FROM `{table}`
            GROUP BY {dup_cols}
            HAVING cnt > 1
            LIMIT 1
            """
        )
        with self.engine.connect() as conn:
            dup_row = conn.execute(dup_stmt).mappings().first()
        if dup_row:
            self.logger.warning(
                "表 %s 存在重复键，已跳过创建唯一索引 %s。", table, index
            )
            return
        with self.engine.begin() as conn:
            conn.execute(text(f"CREATE UNIQUE INDEX `{index}` ON `{table}` ({col_sql})"))
        self.logger.info("表 %s 已新增唯一索引 %s。", table, index)

    def _ensure_lhb_detail_table(self) -> None:
        table = "a_share_lhb_detail"
        columns = {
            "序号": "BIGINT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "名称": "VARCHAR(255) NULL",
            "上榜日": "DATE NULL",
            "解读": "VARCHAR(255) NULL",
            "收盘价": "DOUBLE NULL",
            "涨跌幅": "DOUBLE NULL",
            "龙虎榜净买额": "DOUBLE NULL",
            "龙虎榜买入额": "DOUBLE NULL",
            "龙虎榜卖出额": "DOUBLE NULL",
            "龙虎榜成交额": "DOUBLE NULL",
            "市场总成交额": "DOUBLE NULL",
            "净买额占总成交比": "DOUBLE NULL",
            "成交额占总成交比": "DOUBLE NULL",
            "换手率": "DOUBLE NULL",
            "流通市值": "DOUBLE NULL",
            "上榜原因": "VARCHAR(255) NULL",
            "上榜后1日": "DOUBLE NULL",
            "上榜后2日": "DOUBLE NULL",
            "上榜后5日": "DOUBLE NULL",
            "上榜后10日": "DOUBLE NULL",
            "trade_date": "DATE NOT NULL",
        }
        if not self._table_exists(table):
            self._create_table(table, columns)
        else:
            self._add_missing_columns(table, columns)
        self._ensure_varchar_length(table, "code", 20)
        self._ensure_varchar_length(table, "名称", 255)
        self._ensure_varchar_length(table, "上榜原因", 255)
        self._ensure_yyyymmdd_date_column(table, "trade_date", not_null=False)
        self._ensure_yyyymmdd_date_column(table, "上榜日", not_null=False)

        trade_idx = "idx_a_share_lhb_detail_trade_date"
        if not self._index_exists(table, trade_idx):
            with self.engine.begin() as conn:
                conn.execute(text(f"CREATE INDEX `{trade_idx}` ON `{table}` (`trade_date`)"))
            self.logger.info("表 %s 已新增索引 %s。", table, trade_idx)

        code_trade_idx = "idx_a_share_lhb_detail_code_trade_date"
        if not self._index_exists(table, code_trade_idx):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"CREATE INDEX `{code_trade_idx}` ON `{table}` (`code`, `trade_date`)"
                    )
                )
            self.logger.info("表 %s 已新增索引 %s。", table, code_trade_idx)

    def _ensure_margin_detail_table(self) -> None:
        table = "a_share_margin_detail"
        columns = {
            "信用交易日期": "DATE NULL",
            "code": "VARCHAR(20) NOT NULL",
            "标的证券简称": "VARCHAR(255) NULL",
            "融资余额": "DOUBLE NULL",
            "融资买入额": "DOUBLE NULL",
            "融资偿还额": "DOUBLE NULL",
            "融券余量": "DOUBLE NULL",
            "融券卖出量": "DOUBLE NULL",
            "融券偿还量": "DOUBLE NULL",
            "trade_date": "DATE NOT NULL",
            "exchange": "VARCHAR(8) NULL",
            "证券简称": "VARCHAR(255) NULL",
            "融券余额": "DOUBLE NULL",
            "融资融券余额": "DOUBLE NULL",
        }
        if not self._table_exists(table):
            self._create_table(table, columns)
        else:
            self._add_missing_columns(table, columns)
        self._ensure_varchar_length(table, "code", 20)
        self._ensure_varchar_length(table, "exchange", 8)
        self._ensure_varchar_length(table, "标的证券简称", 255)
        self._ensure_varchar_length(table, "证券简称", 255)
        self._ensure_yyyymmdd_date_column(table, "trade_date", not_null=False)
        self._ensure_yyyymmdd_date_column(table, "信用交易日期", not_null=False)

        trade_idx = "idx_a_share_margin_detail_trade_date"
        if not self._index_exists(table, trade_idx):
            with self.engine.begin() as conn:
                conn.execute(text(f"CREATE INDEX `{trade_idx}` ON `{table}` (`trade_date`)"))
            self.logger.info("表 %s 已新增索引 %s。", table, trade_idx)

        code_trade_idx = "idx_a_share_margin_detail_code_trade_date"
        if not self._index_exists(table, code_trade_idx):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"CREATE INDEX `{code_trade_idx}` ON `{table}` (`code`, `trade_date`)"
                    )
                )
            self.logger.info("表 %s 已新增索引 %s。", table, code_trade_idx)

        unique_idx = "ux_a_share_margin_detail_exchange_code_trade_date"
        self._ensure_unique_index_if_clean(table, unique_idx, ("exchange", "code", "trade_date"))

    def _ensure_gdhs_tables(self) -> None:
        summary = "a_share_gdhs"
        summary_cols = {
            "code": "VARCHAR(20) NOT NULL",
            "名称": "VARCHAR(255) NULL",
            "最新价": "DOUBLE NULL",
            "涨跌幅": "DOUBLE NULL",
            "股东户数-本次": "BIGINT NULL",
            "股东户数-上次": "BIGINT NULL",
            "股东户数-增减": "BIGINT NULL",
            "股东户数-增减比例": "DOUBLE NULL",
            "区间涨跌幅": "DOUBLE NULL",
            "period": "DATE NOT NULL",
            "股东户数统计截止日-上次": "DATE NULL",
            "户均持股市值": "DOUBLE NULL",
            "户均持股数量": "DOUBLE NULL",
            "总市值": "DOUBLE NULL",
            "总股本": "DOUBLE NULL",
            "公告日期": "DATE NULL",
        }
        if not self._table_exists(summary):
            self._create_table(summary, summary_cols)
        else:
            self._add_missing_columns(summary, summary_cols)
        self._ensure_varchar_length(summary, "code", 20)
        self._ensure_varchar_length(summary, "名称", 255)
        self._ensure_yyyymmdd_date_column(summary, "period", not_null=False)
        self._ensure_yyyymmdd_date_column(summary, "股东户数统计截止日-上次", not_null=False)
        self._ensure_yyyymmdd_date_column(summary, "公告日期", not_null=False)
        self._ensure_unique_index_if_clean(summary, "ux_a_share_gdhs_code_period", ("code", "period"))

        detail = "a_share_gdhs_detail"
        detail_cols = {
            "period": "DATE NOT NULL",
            "区间涨跌幅": "DOUBLE NULL",
            "股东户数-本次": "BIGINT NULL",
            "股东户数-上次": "BIGINT NULL",
            "股东户数-增减": "BIGINT NULL",
            "股东户数-增减比例": "DOUBLE NULL",
            "户均持股市值": "DOUBLE NULL",
            "户均持股数量": "DOUBLE NULL",
            "总市值": "DOUBLE NULL",
            "总股本": "DOUBLE NULL",
            "股本变动": "DOUBLE NULL",
            "股本变动原因": "TEXT NULL",
            "股东户数公告日期": "DATE NULL",
            "code": "VARCHAR(20) NOT NULL",
            "名称": "VARCHAR(255) NULL",
        }
        if not self._table_exists(detail):
            self._create_table(detail, detail_cols)
        else:
            self._add_missing_columns(detail, detail_cols)
        self._ensure_varchar_length(detail, "code", 20)
        self._ensure_varchar_length(detail, "名称", 255)
        self._ensure_yyyymmdd_date_column(detail, "period", not_null=False)
        self._ensure_yyyymmdd_date_column(detail, "股东户数公告日期", not_null=False)
        self._ensure_unique_index_if_clean(detail, "ux_a_share_gdhs_detail_code_period", ("code", "period"))

    # ---------- Base dims/facts ----------
    def _ensure_dim_stock_basic_view(self) -> None:
        source = "a_share_stock_list"
        if not self._table_exists(source):
            self.logger.debug(
                "源表 %s 不存在，将创建空视图 %s。",
                source,
                VIEW_DIM_STOCK_BASIC,
            )
            self._drop_relation(VIEW_DIM_STOCK_BASIC)
            empty_stmt = text(
                f"""
                CREATE OR REPLACE VIEW `{VIEW_DIM_STOCK_BASIC}` AS
                SELECT
                  CAST(NULL AS CHAR(20)) AS `code`,
                  CAST(NULL AS CHAR(64)) AS `code_name`,
                  CAST(NULL AS CHAR(8)) AS `tradeStatus`,
                  CAST(NULL AS DATE) AS `ipoDate`,
                  CAST(NULL AS DATE) AS `outDate`,
                  CAST(NULL AS CHAR(8)) AS `type`,
                  CAST(NULL AS CHAR(8)) AS `status`
                WHERE 1 = 0
                """
            )
            with self.engine.begin() as conn:
                conn.execute(empty_stmt)
            return

        meta = self._column_meta(source)
        mapping = {
            "code": ["code"],
            "code_name": ["code_name"],
            "tradeStatus": ["tradeStatus", "tradestatus"],
            "ipoDate": ["ipoDate"],
            "outDate": ["outDate"],
            "type": ["type"],
            "status": ["status"],
        }
        select_cols: list[str] = []
        for target, candidates in mapping.items():
            for col in candidates:
                if col in meta:
                    expr = f"`{col}` AS `{target}`" if col != target else f"`{col}`"
                    select_cols.append(expr)
                    break
        if "code" not in {c.split(" AS ")[-1].strip("`") for c in select_cols}:
            self.logger.warning(
                "源表 %s 缺少 code 列，将创建空视图 %s。",
                source,
                VIEW_DIM_STOCK_BASIC,
            )
            self._drop_relation(VIEW_DIM_STOCK_BASIC)
            empty_stmt = text(
                f"""
                CREATE OR REPLACE VIEW `{VIEW_DIM_STOCK_BASIC}` AS
                SELECT
                  CAST(NULL AS CHAR(20)) AS `code`,
                  CAST(NULL AS CHAR(64)) AS `code_name`,
                  CAST(NULL AS CHAR(8)) AS `tradeStatus`,
                  CAST(NULL AS DATE) AS `ipoDate`,
                  CAST(NULL AS DATE) AS `outDate`,
                  CAST(NULL AS CHAR(8)) AS `type`,
                  CAST(NULL AS CHAR(8)) AS `status`
                WHERE 1 = 0
                """
            )
            with self.engine.begin() as conn:
                conn.execute(empty_stmt)
            return

        columns_sql = ", ".join(select_cols)
        self._drop_relation(VIEW_DIM_STOCK_BASIC)
        stmt = text(
            f"""
            CREATE OR REPLACE VIEW `{VIEW_DIM_STOCK_BASIC}` AS
            SELECT {columns_sql}
            FROM `{source}`
            """
        )
        with self.engine.begin() as conn:
            conn.execute(stmt)
        self.logger.info("已创建/更新维度视图 %s。", VIEW_DIM_STOCK_BASIC)

    def _ensure_fact_stock_daily_view(self) -> None:
        source = "history_daily_kline"
        if not self._table_exists(source):
            self.logger.debug(
                "源表 %s 不存在，将创建空视图 %s。",
                source,
                VIEW_FACT_STOCK_DAILY,
            )
            self._drop_relation(VIEW_FACT_STOCK_DAILY)
            empty_stmt = text(
                f"""
                CREATE OR REPLACE VIEW `{VIEW_FACT_STOCK_DAILY}` AS
                SELECT
                  CAST(NULL AS CHAR(20)) AS `code`,
                  CAST(NULL AS DATE) AS `date`,
                  CAST(NULL AS DATE) AS `trade_date`,
                  CAST(NULL AS DECIMAL(18,6)) AS `open`,
                  CAST(NULL AS DECIMAL(18,6)) AS `high`,
                  CAST(NULL AS DECIMAL(18,6)) AS `low`,
                  CAST(NULL AS DECIMAL(18,6)) AS `close`,
                  CAST(NULL AS DECIMAL(18,6)) AS `volume`,
                  CAST(NULL AS DECIMAL(18,6)) AS `amount`,
                  CAST(NULL AS DECIMAL(18,6)) AS `preclose`,
                  CAST(NULL AS CHAR(8)) AS `tradestatus`
                WHERE 1 = 0
                """
            )
            with self.engine.begin() as conn:
                conn.execute(empty_stmt)
            return

        meta = self._column_meta(source)
        mapping = {
            "code": ["code"],
            "date": ["date"],
            "open": ["open"],
            "high": ["high"],
            "low": ["low"],
            "close": ["close"],
            "volume": ["volume"],
            "amount": ["amount"],
            "tradestatus": ["tradestatus", "tradeStatus"],
            "preclose": ["preclose"],
        }
        select_cols: list[str] = []
        for target, candidates in mapping.items():
            for col in candidates:
                if col in meta:
                    expr = f"`{col}` AS `{target}`" if col != target else f"`{col}`"
                    select_cols.append(expr)
                    break
        if "date" in meta and "trade_date" not in {c.split(" AS ")[-1].strip("`") for c in select_cols}:
            select_cols.append("CAST(`date` AS DATE) AS `trade_date`")

        if not select_cols or "code" not in {c.split(" AS ")[-1].strip('`') for c in select_cols}:
            self.logger.warning(
                "源表 %s 缺少必需列，将创建空视图 %s。",
                source,
                VIEW_FACT_STOCK_DAILY,
            )
            self._drop_relation(VIEW_FACT_STOCK_DAILY)
            empty_stmt = text(
                f"""
                CREATE OR REPLACE VIEW `{VIEW_FACT_STOCK_DAILY}` AS
                SELECT
                  CAST(NULL AS CHAR(20)) AS `code`,
                  CAST(NULL AS DATE) AS `date`,
                  CAST(NULL AS DATE) AS `trade_date`,
                  CAST(NULL AS DECIMAL(18,6)) AS `open`,
                  CAST(NULL AS DECIMAL(18,6)) AS `high`,
                  CAST(NULL AS DECIMAL(18,6)) AS `low`,
                  CAST(NULL AS DECIMAL(18,6)) AS `close`,
                  CAST(NULL AS DECIMAL(18,6)) AS `volume`,
                  CAST(NULL AS DECIMAL(18,6)) AS `amount`,
                  CAST(NULL AS DECIMAL(18,6)) AS `preclose`,
                  CAST(NULL AS CHAR(8)) AS `tradestatus`
                WHERE 1 = 0
                """
            )
            with self.engine.begin() as conn:
                conn.execute(empty_stmt)
            return

        columns_sql = ", ".join(select_cols)
        self._drop_relation(VIEW_FACT_STOCK_DAILY)
        stmt = text(
            f"""
            CREATE OR REPLACE VIEW `{VIEW_FACT_STOCK_DAILY}` AS
            SELECT {columns_sql}
            FROM `{source}`
            """
        )
        with self.engine.begin() as conn:
            conn.execute(stmt)
        self.logger.info("已创建/更新事实视图 %s。", VIEW_FACT_STOCK_DAILY)

    def _ensure_index_membership_view(self) -> None:
        parts: list[str] = []
        index_tables = [
            ("hs300", "index_hs300_members"),
            ("zz500", "index_zz500_members"),
            ("sz50", "index_sz50_members"),
        ]
        for name, table in index_tables:
            if not self._table_exists(table):
                continue
            meta = self._column_meta(table)
            meta_norm = {str(col).lower(): str(col) for col in meta.keys()}

            code_col = meta_norm.get("code")
            date_col: str | None = None
            for cand in ("snapshot_date", "date", "updatedate", "update_date", "trade_date"):
                if cand in meta_norm:
                    date_col = meta_norm[cand]
                    break

            if not code_col or not date_col:
                self.logger.warning(
                    "指数成分表 %s 缺少必需列（code/date），将跳过并继续。",
                    table,
                )
                continue

            parts.append(
                "SELECT "
                f"'{name}' AS `index_name`, "
                f"CAST(`{date_col}` AS DATE) AS `snapshot_date`, "
                f"`{code_col}` AS `code` "
                f"FROM `{table}`"
            )

        self._drop_relation(VIEW_DIM_INDEX_MEMBERSHIP_SNAPSHOT)
        if not parts:
            stmt = text(
                f"""
                CREATE OR REPLACE VIEW `{VIEW_DIM_INDEX_MEMBERSHIP_SNAPSHOT}` AS
                SELECT CAST(NULL AS CHAR(32)) AS `index_name`,
                       CAST(NULL AS DATE) AS `snapshot_date`,
                       CAST(NULL AS CHAR(20)) AS `code`
                WHERE 1 = 0
                """
            )
        else:
            union_sql = " UNION ALL ".join(parts)
            stmt = text(
                f"""
                CREATE OR REPLACE VIEW `{VIEW_DIM_INDEX_MEMBERSHIP_SNAPSHOT}` AS
                {union_sql}
                """
            )
        with self.engine.begin() as conn:
            conn.execute(stmt)
        self.logger.info("已创建/更新指数成分维度视图 %s。", VIEW_DIM_INDEX_MEMBERSHIP_SNAPSHOT)

    def _ensure_universe_table(self) -> None:
        """确保 a_share_universe 作为表存在（不再使用视图）。

        说明：
        - 该表用于承载你运行时生成的 universe 快照数据；
        - 如历史版本曾创建同名 VIEW，这里会先安全地 drop 掉 view。
        """

        table = TABLE_A_SHARE_UNIVERSE

        # 兼容旧版本：如果同名对象是 VIEW，则先删除，避免 CREATE TABLE 冲突
        if self._view_exists(table):
            self._drop_relation_any(table)

        columns = {
            "date": "DATE NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "code_name": "VARCHAR(64) NULL",
            "tradeStatus": "VARCHAR(8) NULL",
            "amount": "DOUBLE NULL",
            "volume": "DOUBLE NULL",
            "open": "DOUBLE NULL",
            "high": "DOUBLE NULL",
            "low": "DOUBLE NULL",
            "close": "DOUBLE NULL",
            "ipoDate": "DATE NULL",
            "type": "VARCHAR(8) NULL",
            "status": "VARCHAR(8) NULL",
            "in_hs300": "TINYINT NULL",
            "in_zz500": "TINYINT NULL",
            "in_sz50": "TINYINT NULL",
        }

        if not self._table_exists(table):
            self._create_table(table, columns, primary_key=("date", "code"))
        else:
            self._add_missing_columns(table, columns)

        idx_name = "idx_a_share_universe_code_date"
        if not self._index_exists(table, idx_name):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE INDEX `{idx_name}`
                        ON `{table}` (`code`, `date`)
                        """
                    )
                )

        self.logger.info("已创建/更新 Universe 表 %s。", table)

    # ---------- MA5-MA20 strategy ----------
    def _ensure_indicator_table(self, table: str) -> None:
        columns = {
            "trade_date": "DATE NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "close": "DOUBLE NULL",
            "volume": "DOUBLE NULL",
            "amount": "DOUBLE NULL",
            "avg_volume_20": "DOUBLE NULL",
            "ma5": "DOUBLE NULL",
            "ma10": "DOUBLE NULL",
            "ma20": "DOUBLE NULL",
            "ma60": "DOUBLE NULL",
            "ma250": "DOUBLE NULL",
            "vol_ratio": "DOUBLE NULL",
            "macd_dif": "DOUBLE NULL",
            "macd_dea": "DOUBLE NULL",
            "macd_hist": "DOUBLE NULL",
            "prev_macd_hist": "DOUBLE NULL",
            "kdj_k": "DOUBLE NULL",
            "kdj_d": "DOUBLE NULL",
            "kdj_j": "DOUBLE NULL",
            "atr14": "DOUBLE NULL",
            "rsi14": "DOUBLE NULL",
            "ret_10": "DOUBLE NULL",
            "ret_20": "DOUBLE NULL",
            "limit_up_cnt_20": "DOUBLE NULL",
            "ma20_bias": "DOUBLE NULL",
            "yearline_state": "VARCHAR(50) NULL",
        }
        if not self._table_exists(table):
            self._create_table(table, columns, primary_key=("trade_date", "code"))
            return
        self._add_missing_columns(table, columns)

    def _ensure_signal_events_table(self, table: str) -> None:
        columns = {
            "sig_date": "DATE NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "strategy_code": "VARCHAR(32) NOT NULL",
            "signal": "VARCHAR(64) NULL",
            "final_action": "VARCHAR(16) NULL",
            "final_reason": "VARCHAR(255) NULL",
            "final_cap": "DOUBLE NULL",
            "reason": "VARCHAR(255) NULL",
            "risk_tag": "VARCHAR(255) NULL",
            "risk_note": "VARCHAR(255) NULL",
            "stop_ref": "DOUBLE NULL",
            "macd_event": "VARCHAR(32) NULL",
            "chip_score": "DOUBLE NULL",
            "gdhs_delta_pct": "DOUBLE NULL",
            "gdhs_announce_date": "DATE NULL",
            "chip_reason": "VARCHAR(255) NULL",
            "chip_penalty": "DOUBLE NULL",
            "chip_note": "VARCHAR(255) NULL",
            "age_days": "INT NULL",
            "valid_days": "INT NULL",
            "expires_on": "DATE NULL",
            "deadzone_hit": "TINYINT(1) NULL",
            "stale_hit": "TINYINT(1) NULL",
            "fear_score": "DOUBLE NULL",
            "wave_type": "VARCHAR(64) NULL",
            "extra_json": "TEXT NULL",
        }
        if not self._table_exists(table):
            self._create_table(
                table,
                columns,
                primary_key=("sig_date", "code", "strategy_code"),
            )
            return
        self._add_missing_columns(table, columns)
        self._ensure_varchar_length(table, "risk_tag", 255)
        self._ensure_varchar_length(table, "risk_note", 255)
        self._ensure_varchar_length(table, "reason", 255)
        self._ensure_varchar_length(table, "final_reason", 255)
        self._ensure_varchar_length(table, "macd_event", 32)
        self._ensure_varchar_length(table, "wave_type", 64)
        self._ensure_numeric_column(table, "final_cap", "DOUBLE NULL")
        self._ensure_numeric_column(table, "chip_score", "DOUBLE NULL")
        self._ensure_numeric_column(table, "gdhs_delta_pct", "DOUBLE NULL")
        self._ensure_numeric_column(table, "chip_penalty", "DOUBLE NULL")
        self._ensure_numeric_column(table, "fear_score", "DOUBLE NULL")
        self._ensure_numeric_column(table, "age_days", "INT NULL")
        self._ensure_numeric_column(table, "valid_days", "INT NULL")
        self._ensure_numeric_column(table, "deadzone_hit", "TINYINT(1) NULL")
        self._ensure_numeric_column(table, "stale_hit", "TINYINT(1) NULL")
        self._ensure_date_column(table, "gdhs_announce_date", not_null=False)
        self._ensure_date_column(table, "expires_on", not_null=False)
        self._ensure_varchar_length(table, "chip_reason", 255)
        self._ensure_varchar_length(table, "chip_note", 255)
        unique_name = "ux_signal_events_strategy_date_code"
        if not self._index_exists(table, unique_name):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE UNIQUE INDEX `{unique_name}`
                        ON `{table}` (`strategy_code`, `sig_date`, `code`)
                        """
                    )
                )
            self.logger.info("信号事件表 %s 已新增唯一索引 %s。", table, unique_name)

        meta = self._column_meta(table)
        if "expires_on" in meta and "valid_days" in meta:
            stmt = text(
                f"""
                UPDATE `{table}`
                SET `expires_on` = DATE_ADD(`sig_date`, INTERVAL `valid_days` DAY)
                WHERE `expires_on` IS NULL AND `valid_days` IS NOT NULL
                """
            )
            with self.engine.begin() as conn:
                result = conn.execute(stmt)
            updated = int(getattr(result, "rowcount", 0) or 0)
            if updated:
                self.logger.info("信号事件表 %s 已回填 expires_on：%s 条。", table, updated)

    def _ensure_strategy_candidates_table(self) -> None:
        table = TABLE_STRATEGY_CANDIDATES
        columns = {
            "asof_trade_date": "DATE NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "is_liquidity": "TINYINT(1) NOT NULL DEFAULT 0",
            "has_signal": "TINYINT(1) NOT NULL DEFAULT 0",
            "latest_sig_date": "DATE NULL",
            "latest_sig_action": "VARCHAR(16) NULL",
            "latest_sig_strategy_code": "VARCHAR(32) NULL",
            "created_at": "DATETIME(6) NULL",
        }
        if not self._table_exists(table):
            self._create_table(
                table,
                columns,
                primary_key=("asof_trade_date", "code"),
            )
            return
        self._add_missing_columns(table, columns)
        self._ensure_varchar_length(table, "latest_sig_action", 16)
        self._ensure_varchar_length(table, "latest_sig_strategy_code", 32)
        self._ensure_date_column(table, "asof_trade_date", not_null=True)
        self._ensure_date_column(table, "latest_sig_date", not_null=False)
        self._ensure_datetime_column(table, "created_at")

    def build_ready_signals_select(
        self,
        events_table: str,
        indicator_table: str,
        chip_table: str,
    ) -> tuple[str, list[str]]:
        if not events_table:
            return "", []

        ind_join = ""
        indicator_exists = bool(indicator_table and self._table_exists(indicator_table))
        if indicator_exists:
            ind_join = (
                f"""
                LEFT JOIN `{indicator_table}` ind
                  ON e.`sig_date` = ind.`trade_date`
                 AND e.`code` = ind.`code`
                """
            )
        elif indicator_table:
            self.logger.warning("指标表 %s 不存在，ready_signals 将以 NULL 补齐指标列。", indicator_table)

        chip_join = ""
        chip_enabled = False
        if chip_table and self._table_exists(chip_table):
            chip_enabled = True
            chip_join = (
                f"""
                LEFT JOIN `{chip_table}` cf
                  ON e.`sig_date` = cf.`sig_date`
                 AND e.`code` = cf.`code`
                """
            )

        meta = self._column_meta(events_table)

        def _event_expr(name: str) -> str:
            return f"e.`{name}`" if name in meta else "NULL"

        def _ind_expr(name: str) -> str:
            return f"ind.`{name}`" if indicator_exists else "NULL"

        def _coalesce_expr(cols: list[str]) -> str:
            if not cols:
                return "NULL"
            return f"COALESCE({', '.join(cols)})"

        field_exprs: Dict[str, str] = {
            "sig_date": "e.`sig_date`",
            "code": "e.`code`",
            "strategy_code": "e.`strategy_code`",
            "signal": "COALESCE(e.`final_action`, e.`signal`)",
            "final_action": "COALESCE(e.`final_action`, e.`signal`)",
            "final_reason": "COALESCE(e.`final_reason`, e.`reason`)",
            "final_cap": "e.`final_cap`",
            "reason": "COALESCE(e.`final_reason`, e.`reason`)",
            "risk_tag": "e.`risk_tag`",
            "risk_note": "e.`risk_note`",
            "extra_json": "e.`extra_json`",
            "valid_days": "e.`valid_days`",
            "expires_on": "e.`expires_on`",
            "stop_ref": _event_expr("stop_ref"),
            "macd_event": _event_expr("macd_event"),
            "fear_score": _event_expr("fear_score"),
            "wave_type": _event_expr("wave_type"),
            "yearline_state": _ind_expr("yearline_state"),
            "close": _ind_expr("close"),
            "ma5": _ind_expr("ma5"),
            "ma20": _ind_expr("ma20"),
            "ma60": _ind_expr("ma60"),
            "ma250": _ind_expr("ma250"),
            "vol_ratio": _ind_expr("vol_ratio"),
            "macd_hist": _ind_expr("macd_hist"),
            "kdj_k": _ind_expr("kdj_k"),
            "kdj_d": _ind_expr("kdj_d"),
            "atr14": _ind_expr("atr14"),
            "avg_volume_20": _ind_expr("avg_volume_20"),
        }

        gdhs_delta_sources = []
        announce_sources = []
        chip_score_sources = []
        chip_reason_sources = []
        chip_penalty_sources = []
        chip_note_sources = []
        chip_age_sources = []
        chip_deadzone_sources = []
        chip_stale_sources = []

        if "gdhs_delta_pct" in meta:
            gdhs_delta_sources.append("e.`gdhs_delta_pct`")
        if "gdhs_announce_date" in meta:
            announce_sources.append("e.`gdhs_announce_date`")
        if "chip_score" in meta:
            chip_score_sources.append("e.`chip_score`")
        if "chip_reason" in meta:
            chip_reason_sources.append("e.`chip_reason`")
        if "chip_penalty" in meta:
            chip_penalty_sources.append("e.`chip_penalty`")
        if "chip_note" in meta:
            chip_note_sources.append("e.`chip_note`")
        if "age_days" in meta:
            chip_age_sources.append("e.`age_days`")
        if "deadzone_hit" in meta:
            chip_deadzone_sources.append("e.`deadzone_hit`")
        if "stale_hit" in meta:
            chip_stale_sources.append("e.`stale_hit`")

        if chip_enabled:
            gdhs_delta_sources.append("cf.`gdhs_delta_pct`")
            announce_sources.append("cf.`announce_date`")
            chip_score_sources.append("cf.`chip_score`")
            chip_reason_sources.append("cf.`chip_reason`")
            chip_penalty_sources.append("cf.`chip_penalty`")
            chip_note_sources.append("cf.`chip_note`")
            chip_age_sources.append("cf.`age_days`")
            chip_deadzone_sources.append("cf.`deadzone_hit`")
            chip_stale_sources.append("cf.`stale_hit`")

        field_exprs["gdhs_delta_pct"] = _coalesce_expr(gdhs_delta_sources)
        field_exprs["gdhs_announce_date"] = _coalesce_expr(announce_sources)
        field_exprs["chip_score"] = _coalesce_expr(chip_score_sources)
        field_exprs["chip_reason"] = _coalesce_expr(chip_reason_sources)
        field_exprs["chip_penalty"] = _coalesce_expr(chip_penalty_sources)
        field_exprs["chip_note"] = _coalesce_expr(chip_note_sources)
        field_exprs["age_days"] = _coalesce_expr(chip_age_sources)
        field_exprs["deadzone_hit"] = _coalesce_expr(chip_deadzone_sources)
        field_exprs["stale_hit"] = _coalesce_expr(chip_stale_sources)

        industry_join = ""
        industry_fields: Dict[str, str] = {
            "industry": "NULL",
            "industry_classification": "NULL",
        }
        for candidate in ("dim_stock_industry", "a_share_stock_industry"):
            if self._table_exists(candidate):
                industry_join = (
                    f"""
                    LEFT JOIN `{candidate}` ind_dim
                      ON e.`code` = ind_dim.`code`
                    """
                )
                industry_meta = self._column_meta(candidate)
                industry_name_col = None
                for key in ["industry", "industryClassification"]:
                    if key in industry_meta:
                        industry_name_col = key
                        break
                industry_class_col = (
                    "industry_classification"
                    if "industry_classification" in industry_meta
                    else None
                )
                industry_fields = {
                    "industry": f"ind_dim.`{industry_name_col}`" if industry_name_col else "NULL",
                    "industry_classification": (
                        f"ind_dim.`{industry_class_col}`" if industry_class_col else "NULL"
                    ),
                }
                break

        board_join = ""
        board_fields: Dict[str, str] = {
            "board_name": "NULL",
            "board_code": "NULL",
        }
        board_table = "dim_stock_board_industry"
        if self._table_exists(board_table):
            board_join = (
                f"""
                LEFT JOIN `{board_table}` bd
                  ON e.`code` = bd.`code`
                """
            )
            board_meta = self._column_meta(board_table)
            board_name_col = "board_name" if "board_name" in board_meta else None
            board_code_col = "board_code" if "board_code" in board_meta else None
            board_fields = {
                "board_name": f"bd.`{board_name_col}`" if board_name_col else "NULL",
                "board_code": f"bd.`{board_code_col}`" if board_code_col else "NULL",
            }

        field_exprs.update(industry_fields)
        field_exprs.update(board_fields)

        select_clause = ",\n              ".join(
            f"{field_exprs[col]} AS `{col}`" for col in READY_SIGNALS_COLUMNS.keys()
        )
        select_sql = f"""
            SELECT
              {select_clause}
            FROM `{events_table}` e
            {ind_join}
            {chip_join}
            {industry_join}
            {board_join}
            WHERE COALESCE(e.`final_action`, e.`signal`) IN ('BUY','BUY_CONFIRM')
            """
        return select_sql, list(READY_SIGNALS_COLUMNS.keys())

    def _ensure_ready_signals_view(
        self,
        view: str,
        events_table: str,
        indicator_table: str,
        chip_table: str,
    ) -> None:
        if not view or not events_table:
            return

        relation_type = self._relation_type(view)
        if relation_type == "VIEW":
            self._drop_relation(view)

        if not self._table_exists(view):
            self._create_table(
                view,
                READY_SIGNALS_COLUMNS,
                primary_key=("strategy_code", "sig_date", "code"),
            )
        else:
            self._add_missing_columns(view, READY_SIGNALS_COLUMNS)
            self._ensure_primary_key(view, ("strategy_code", "sig_date", "code"))

        for col, length in {
            "code": 20,
            "strategy_code": 32,
            "signal": 64,
            "final_action": 16,
            "final_reason": 255,
            "reason": 255,
            "risk_tag": 255,
            "risk_note": 255,
            "macd_event": 32,
            "wave_type": 64,
            "yearline_state": 50,
            "chip_reason": 255,
            "chip_note": 255,
            "industry": 255,
            "industry_classification": 255,
            "board_name": 255,
            "board_code": 64,
        }.items():
            self._ensure_varchar_length(view, col, length)

        for col, definition in {
            "final_cap": "DOUBLE NULL",
            "stop_ref": "DOUBLE NULL",
            "fear_score": "DOUBLE NULL",
            "close": "DOUBLE NULL",
            "ma5": "DOUBLE NULL",
            "ma20": "DOUBLE NULL",
            "ma60": "DOUBLE NULL",
            "ma250": "DOUBLE NULL",
            "vol_ratio": "DOUBLE NULL",
            "macd_hist": "DOUBLE NULL",
            "kdj_k": "DOUBLE NULL",
            "kdj_d": "DOUBLE NULL",
            "atr14": "DOUBLE NULL",
            "avg_volume_20": "DOUBLE NULL",
            "gdhs_delta_pct": "DOUBLE NULL",
            "chip_score": "DOUBLE NULL",
            "chip_penalty": "DOUBLE NULL",
            "age_days": "INT NULL",
            "deadzone_hit": "TINYINT(1) NULL",
            "stale_hit": "TINYINT(1) NULL",
        }.items():
            self._ensure_numeric_column(view, col, definition)

        self._ensure_date_column(view, "sig_date", not_null=True)
        self._ensure_date_column(view, "expires_on", not_null=False)
        self._ensure_date_column(view, "gdhs_announce_date", not_null=False)

        index_map = {
            "idx_ready_signals_strategy_date": ("strategy_code", "sig_date"),
            "idx_ready_signals_expires_on": ("expires_on",),
            "idx_ready_signals_sig_date": ("sig_date",),
            "idx_ready_signals_code": ("code",),
        }
        for index_name, cols in index_map.items():
            if self._index_exists(view, index_name):
                continue
            cols_clause = ", ".join(f"`{c}`" for c in cols)
            with self.engine.begin() as conn:
                conn.execute(text(f"CREATE INDEX `{index_name}` ON `{view}` ({cols_clause})"))

        self.logger.info("已创建/更新表 %s（准备信号）。", view)

    def _ensure_trade_metrics_table(self) -> None:
        columns = {
            "strategy_code": "VARCHAR(32) NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "entry_date": "DATE NOT NULL",
            "entry_price": "DOUBLE NULL",
            "exit_date": "DATE NULL",
            "exit_price": "DOUBLE NULL",
            "atr_at_entry": "DOUBLE NULL",
            "pnl_pct": "DOUBLE NULL",
            "pnl_atr_ratio": "DOUBLE NULL",
            "holding_days": "INT NULL",
            "exit_reason": "VARCHAR(255) NULL",
        }
        table = TABLE_STRATEGY_TRADE_METRICS
        if not self._table_exists(table):
            self._create_table(table, columns, primary_key=("strategy_code", "code", "entry_date"))
            return
        self._add_missing_columns(table, columns)
        self._ensure_numeric_column(table, "pnl_pct", "DOUBLE NULL")
        self._ensure_numeric_column(table, "pnl_atr_ratio", "DOUBLE NULL")
        self._ensure_numeric_column(table, "holding_days", "INT NULL")
        self._ensure_varchar_length(table, "exit_reason", 255)

    def _ensure_chip_filter_table(self) -> None:
        columns = {
            "sig_date": "DATE NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "announce_date": "DATE NULL",
            "gdhs_delta_pct": "DOUBLE NULL",
            "gdhs_delta_raw": "DOUBLE NULL",
            "chip_score": "DOUBLE NULL",
            "chip_reason": "VARCHAR(255) NULL",
            "chip_penalty": "DOUBLE NULL",
            "chip_note": "VARCHAR(255) NULL",
            "vol_ratio": "DOUBLE NULL",
            "age_days": "INT NULL",
            "deadzone_hit": "TINYINT(1) NULL",
            "stale_hit": "TINYINT(1) NULL",
            "updated_at": "DATETIME(6) NULL",
        }
        table = TABLE_STRATEGY_CHIP_FILTER
        if not self._table_exists(table):
            self._create_table(table, columns, primary_key=("sig_date", "code"))
            return
        self._add_missing_columns(table, columns)
        self._ensure_numeric_column(table, "chip_score", "DOUBLE NULL")
        self._ensure_numeric_column(table, "gdhs_delta_pct", "DOUBLE NULL")
        self._ensure_numeric_column(table, "gdhs_delta_raw", "DOUBLE NULL")
        self._ensure_numeric_column(table, "vol_ratio", "DOUBLE NULL")
        self._ensure_numeric_column(table, "age_days", "INT NULL")
        self._ensure_numeric_column(table, "deadzone_hit", "TINYINT(1) NULL")
        self._ensure_numeric_column(table, "stale_hit", "TINYINT(1) NULL")
        self._ensure_numeric_column(table, "chip_penalty", "DOUBLE NULL")
        self._ensure_varchar_length(table, "chip_reason", 255)
        self._ensure_varchar_length(table, "chip_note", 255)
        self._ensure_datetime_column(table, "updated_at")

    def _ensure_backtest_view(self) -> None:
        table = TABLE_STRATEGY_TRADE_METRICS
        view = VIEW_STRATEGY_BACKTEST
        if not self._table_exists(table):
            self._drop_relation(view)
            return
        stmt = text(
            f"""
            CREATE OR REPLACE VIEW `{view}` AS
            SELECT
              `strategy_code`,
              'WEEK' AS `period_type`,
              CAST(YEARWEEK(`exit_date`, 3) AS CHAR) AS `period_key`,
              COUNT(*) AS `trade_cnt`,
              1.0 * SUM(CASE WHEN `pnl_pct` > 0 THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS `win_rate`,
              AVG(`pnl_atr_ratio`) AS `avg_pnl_atr_ratio`,
              SUM(CASE WHEN `pnl_atr_ratio` > 1.5 THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS `gt_1_5_ratio`
            FROM `{table}`
            WHERE `exit_date` IS NOT NULL
            GROUP BY `strategy_code`, YEARWEEK(`exit_date`, 3)
            UNION ALL
            SELECT
              `strategy_code`,
              'MONTH' AS `period_type`,
              CAST(DATE_FORMAT(`exit_date`, '%Y-%m') AS CHAR) AS `period_key`,
              COUNT(*) AS `trade_cnt`,
              1.0 * SUM(CASE WHEN `pnl_pct` > 0 THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS `win_rate`,
              AVG(`pnl_atr_ratio`) AS `avg_pnl_atr_ratio`,
              SUM(CASE WHEN `pnl_atr_ratio` > 1.5 THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS `gt_1_5_ratio`
            FROM `{table}`
            WHERE `exit_date` IS NOT NULL
            GROUP BY `strategy_code`, DATE_FORMAT(`exit_date`, '%Y-%m')
            """
        )
        with self.engine.begin() as conn:
            conn.execute(stmt)
        self.logger.info("已创建/更新回测视图 %s。", view)

    def _ensure_v_pnl_view(self) -> None:
        table = TABLE_STRATEGY_TRADE_METRICS
        view = VIEW_STRATEGY_PNL
        if not self._table_exists(table):
            self._drop_relation(view)
            return
        stmt = text(
            f"""
            CREATE OR REPLACE VIEW `{view}` AS
            SELECT
              `strategy_code`,
              `code`,
              COUNT(*) AS `trade_cnt`,
              SUM(CASE WHEN `pnl_pct` > 0 THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) AS `win_rate`,
              AVG(`pnl_atr_ratio`) AS `avg_pnl_atr_ratio`,
              AVG(`pnl_pct`) AS `avg_pnl_pct`
            FROM `{table}`
            WHERE `exit_date` IS NOT NULL
            GROUP BY `strategy_code`, `code`
            HAVING `avg_pnl_atr_ratio` > 1.5
            """
        )
        with self.engine.begin() as conn:
            conn.execute(stmt)
        self.logger.info("已创建/更新视图 %s。", view)

    # ---------- Open monitor ----------
    def _ensure_open_monitor_run_table(self, table: str) -> None:
        columns = {
            "run_pk": "BIGINT NOT NULL AUTO_INCREMENT",
            "monitor_date": "DATE NOT NULL",
            "run_id": "VARCHAR(64) NOT NULL",
            "run_stage": "VARCHAR(16) NULL",
            "triggered_at": "DATETIME(6) NULL",
            "checked_at": "DATETIME(6) NULL",
            "status": "VARCHAR(32) NOT NULL DEFAULT 'RUNNING'",
            "error_msg": "TEXT NULL",
            "params_json": "TEXT NULL",
        }
        if not self._table_exists(table):
            self._create_table(table, columns, primary_key=("run_pk",))
        else:
            self._add_missing_columns(table, columns)
            self._drop_columns(table, ["env_index_snapshot_hash"])
        self._ensure_date_column(table, "monitor_date", not_null=True)
        self._ensure_datetime_column(table, "triggered_at")
        self._ensure_datetime_column(table, "checked_at")
        self._ensure_varchar_length(table, "run_id", 64)
        self._ensure_varchar_length(table, "run_stage", 16)
        self._ensure_varchar_length(table, "status", 32)

        if "run_stage" in self._column_meta(table):
            stmt = text(
                f"""
                UPDATE `{table}`
                SET `run_stage` = CASE
                  WHEN `run_id` LIKE 'PREOPEN %' THEN 'PREOPEN'
                  WHEN `run_id` LIKE 'BREAK %' THEN 'BREAK'
                  WHEN `run_id` LIKE 'POSTCLOSE %' THEN 'POSTCLOSE'
                  ELSE 'INTRADAY'
                END
                WHERE `run_stage` IS NULL AND `run_id` IS NOT NULL
                """
            )
            with self.engine.begin() as conn:
                result = conn.execute(stmt)
            updated = int(getattr(result, "rowcount", 0) or 0)
            if updated:
                self.logger.info("开盘监测运行表 %s 已回填 run_stage：%s 条。", table, updated)

        unique_name = "ux_open_monitor_run"
        if not self._index_exists(table, unique_name):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE UNIQUE INDEX `{unique_name}`
                        ON `{table}` (`monitor_date`, `run_id`)
                        """
                    )
                )
            self.logger.info("开盘监测运行表 %s 已新增唯一索引 %s。", table, unique_name)

        stage_index = "idx_open_monitor_run_stage"
        if not self._index_exists(table, stage_index):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE INDEX `{stage_index}`
                        ON `{table}` (`monitor_date`, `run_stage`, `checked_at`)
                        """
                    )
                )
            self.logger.info("开盘监测运行表 %s 已新增索引 %s。", table, stage_index)

    def _ensure_open_monitor_quote_table(self, table: str) -> None:
        columns = {
            "monitor_date": "DATE NOT NULL",
            "run_pk": "BIGINT NOT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "live_trade_date": "DATE NULL",
            "live_open": "DOUBLE NULL",
            "live_high": "DOUBLE NULL",
            "live_low": "DOUBLE NULL",
            "live_latest": "DOUBLE NULL",
            "live_volume": "DOUBLE NULL",
            "live_amount": "DOUBLE NULL",
        }
        if not self._table_exists(table):
            self._create_table(
                table,
                columns,
                primary_key=("run_pk", "code"),
            )
            return
        self._add_missing_columns(table, columns)
        self._drop_columns(table, ["run_id"])
        self._ensure_date_column(table, "monitor_date", not_null=True)
        self._ensure_date_column(table, "live_trade_date", not_null=False)
        self._ensure_numeric_column(table, "run_pk", "BIGINT NOT NULL")
        self._ensure_primary_key(table, ("run_pk", "code"))

        unique_name = "ux_open_monitor_quote_run"
        if not self._index_exists(table, unique_name):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE UNIQUE INDEX `{unique_name}`
                        ON `{table}` (`monitor_date`, `run_pk`, `code`)
                        """
                    )
                )
            self.logger.info("行情快照表 %s 已新增唯一索引 %s。", table, unique_name)

    def _ensure_open_monitor_eval_table(self, table: str) -> None:
        columns = {
            "monitor_date": "DATE NOT NULL",
            "sig_date": "DATE NOT NULL",
            "run_pk": "BIGINT NOT NULL",
            "strategy_code": "VARCHAR(32) NOT NULL",
            "asof_trade_date": "DATE NULL",
            "live_trade_date": "DATE NULL",
            "signal_age": "INT NULL",
            "code": "VARCHAR(20) NOT NULL",
            "avg_volume_20": "DOUBLE NULL",
            "sig_close": "DOUBLE NULL",
            "sig_ma5": "DOUBLE NULL",
            "sig_ma20": "DOUBLE NULL",
            "sig_ma60": "DOUBLE NULL",
            "sig_ma250": "DOUBLE NULL",
            "sig_vol_ratio": "DOUBLE NULL",
            "sig_macd_hist": "DOUBLE NULL",
            "sig_kdj_k": "DOUBLE NULL",
            "sig_kdj_d": "DOUBLE NULL",
            "sig_atr14": "DOUBLE NULL",
            "sig_stop_ref": "DOUBLE NULL",
            "asof_close": "DOUBLE NULL",
            "asof_ma5": "DOUBLE NULL",
            "asof_ma20": "DOUBLE NULL",
            "asof_ma60": "DOUBLE NULL",
            "asof_ma250": "DOUBLE NULL",
            "asof_vol_ratio": "DOUBLE NULL",
            "asof_macd_hist": "DOUBLE NULL",
            "asof_atr14": "DOUBLE NULL",
            "asof_stop_ref": "DOUBLE NULL",
            "live_gap_pct": "DOUBLE NULL",
            "live_pct_change": "DOUBLE NULL",
            "live_intraday_vol_ratio": "DOUBLE NULL",
            "dev_ma5": "DOUBLE NULL",
            "dev_ma20": "DOUBLE NULL",
            "dev_ma5_atr": "DOUBLE NULL",
            "dev_ma20_atr": "DOUBLE NULL",
            "runup_from_sigclose": "DOUBLE NULL",
            "runup_from_sigclose_atr": "DOUBLE NULL",
            "runup_ref_price": "DOUBLE NULL",
            "runup_ref_source": "VARCHAR(32) NULL",
            "entry_exposure_cap": "DOUBLE NULL",
            "signal_strength": "DOUBLE NULL",
            "strength_delta": "DOUBLE NULL",
            "strength_trend": "VARCHAR(16) NULL",
            "strength_note": "VARCHAR(512) NULL",
            "signal_kind": "VARCHAR(16) NULL",
            "sig_signal": "VARCHAR(16) NULL",
            "sig_reason": "VARCHAR(255) NULL",
            "state": "VARCHAR(32) NULL",
            "status_reason": "VARCHAR(255) NULL",
            "action": "VARCHAR(16) NULL",
            "action_reason": "VARCHAR(255) NULL",
            "rule_hits_json": "TEXT NULL",
            "summary_line": "VARCHAR(512) NULL",
            "risk_tag": "VARCHAR(255) NULL",
            "risk_note": "VARCHAR(255) NULL",
            "snapshot_hash": "VARCHAR(64) NULL",
        }
        if not self._table_exists(table):
            self._create_table(
                table,
                columns,
                primary_key=("run_pk", "strategy_code", "sig_date", "code"),
            )
        else:
            self._add_missing_columns(table, columns)
            self._drop_columns(
                table,
                [
                    "run_id",
                    "valid_days",
                    "checked_at",
                    "env_index_score",
                    "env_regime",
                    "env_position_hint",
                    "env_final_gate_action",
                    "env_index_snapshot_hash",
                    "env_weekly_asof_trade_date",
                    "env_weekly_risk_level",
                    "env_weekly_scene",
                    "env_weekly_structure_status",
                    "env_weekly_pattern_status",
                    "env_weekly_gate_action",
                    "env_weekly_gate_policy",
                ],
            )

        for col in ["code", "snapshot_hash"]:
            self._ensure_varchar_length(table, col, 64 if col == "snapshot_hash" else 64)
        self._ensure_varchar_length(table, "strategy_code", 32)
        self._ensure_date_column(table, "monitor_date", not_null=True)
        self._ensure_date_column(table, "sig_date", not_null=True)
        self._ensure_date_column(table, "asof_trade_date", not_null=False)
        self._ensure_date_column(table, "live_trade_date", not_null=False)
        self._ensure_numeric_column(table, "run_pk", "BIGINT NOT NULL")

        for col in [
            "avg_volume_20",
            "sig_close",
            "sig_ma5",
            "sig_ma20",
            "sig_ma60",
            "sig_ma250",
            "sig_vol_ratio",
            "sig_macd_hist",
            "sig_kdj_k",
            "sig_kdj_d",
            "sig_atr14",
            "sig_stop_ref",
            "asof_close",
            "asof_ma5",
            "asof_ma20",
            "asof_ma60",
            "asof_ma250",
            "asof_vol_ratio",
            "asof_macd_hist",
            "asof_atr14",
            "asof_stop_ref",
        ]:
            self._ensure_numeric_column(table, col, "DOUBLE NULL")
        self._ensure_numeric_column(table, "live_intraday_vol_ratio", "DOUBLE NULL")
        self._ensure_numeric_column(table, "signal_strength", "DOUBLE NULL")
        self._ensure_numeric_column(table, "strength_delta", "DOUBLE NULL")
        self._ensure_primary_key(table, ("run_pk", "strategy_code", "sig_date", "code"))

        self._ensure_open_monitor_indexes(table)

    def _ensure_open_monitor_indexes(self, table: str) -> None:
        unique_index = "ux_open_monitor_run"
        if not self._index_exists(table, unique_index):
            with self.engine.begin() as conn:
                try:
                    conn.execute(
                        text(
                            f"""
                            CREATE UNIQUE INDEX `{unique_index}`
                            ON `{table}` (`run_pk`, `strategy_code`, `sig_date`, `code`)
                            """
                        )
                    )
                    self.logger.info("表 %s 已创建唯一索引 %s。", table, unique_index)
                except Exception as exc:  # noqa: BLE001
                    self.logger.warning("创建唯一索引 %s 失败：%s", unique_index, exc)

        index_name = "idx_open_monitor_strength_run"
        if not self._index_exists(table, index_name):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"CREATE INDEX `{index_name}` ON `{table}` (`monitor_date`, `code`, `run_pk`)"
                    )
                )
            self.logger.info("表 %s 已新增索引 %s。", table, index_name)

        code_time_index = "idx_open_monitor_code_run"
        if not self._index_exists(table, code_time_index):
            with self.engine.begin() as conn:
                conn.execute(
                    text(f"CREATE INDEX `{code_time_index}` ON `{table}` (`code`, `run_pk`)")
                )
            self.logger.info("表 %s 已新增索引 %s。", table, code_time_index)

        run_pk_index = "idx_open_monitor_run_pk"
        if not self._index_exists(table, run_pk_index):
            with self.engine.begin() as conn:
                conn.execute(
                    text(f"CREATE INDEX `{run_pk_index}` ON `{table}` (`run_pk`)")
                )
            self.logger.info("表 %s 已新增索引 %s。", table, run_pk_index)

    # ---------- Environment snapshots ----------
    def _ensure_open_monitor_env_table(self, table: str) -> None:
        columns = {
            "run_pk": "BIGINT NOT NULL",
            "monitor_date": "DATE NOT NULL",
            "env_weekly_asof_trade_date": "DATE NULL",
            "env_daily_asof_trade_date": "DATE NULL",
            "env_final_gate_action": "VARCHAR(16) NULL",
            "env_final_cap_pct": "DOUBLE NULL",
            "env_final_reason_json": "TEXT NULL",
            "env_live_override_action": "VARCHAR(16) NULL",
            "env_live_cap_multiplier": "DOUBLE NULL",
            "env_live_event_tags": "VARCHAR(255) NULL",
            "env_live_reason": "VARCHAR(255) NULL",
            "env_index_snapshot_hash": "CHAR(32) NULL",
        }
        if not self._table_exists(table):
            self._create_table(table, columns, primary_key=("run_pk",))
        else:
            self._add_missing_columns(table, columns)
            self._drop_columns(
                table,
                [
                    "run_id",
                    "checked_at",
                    "env_weekly_gate_policy",
                    "env_weekly_gate_action",
                    "env_weekly_risk_level",
                    "env_weekly_scene",
                    "env_weekly_zone_id",
                    "env_daily_zone_id",
                    "env_index_score",
                    "env_regime",
                    "env_position_hint",
                    "env_index_code",
                    "env_index_asof_trade_date",
                    "env_index_live_trade_date",
                    "env_index_asof_close",
                    "env_index_asof_ma20",
                    "env_index_asof_ma60",
                    "env_index_asof_macd_hist",
                    "env_index_asof_atr14",
                    "env_index_live_open",
                    "env_index_live_high",
                    "env_index_live_low",
                    "env_index_live_latest",
                    "env_index_live_pct_change",
                    "env_index_live_volume",
                    "env_index_live_amount",
                    "env_index_dev_ma20_atr",
                    "env_index_gate_action",
                    "env_index_gate_reason",
                    "env_index_position_cap",
                ],
            )
        self._ensure_date_column(table, "monitor_date", not_null=True)
        self._ensure_date_column(table, "env_weekly_asof_trade_date", not_null=False)
        self._ensure_date_column(table, "env_daily_asof_trade_date", not_null=False)
        self._ensure_numeric_column(table, "env_final_cap_pct", "DOUBLE NULL")
        self._ensure_numeric_column(table, "env_live_cap_multiplier", "DOUBLE NULL")
        self._ensure_numeric_column(table, "run_pk", "BIGINT NOT NULL")
        self._ensure_varchar_length(table, "env_live_override_action", 16)
        self._ensure_varchar_length(table, "env_live_event_tags", 255)
        self._ensure_varchar_length(table, "env_live_reason", 255)

        unique_name = "ux_env_snapshot_run"
        if not self._index_exists(table, unique_name):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE UNIQUE INDEX `{unique_name}`
                        ON `{table}` (`monitor_date`, `run_pk`)
                        """
                    )
                )
            self.logger.info("环境快照表 %s 已新增唯一索引 %s。", table, unique_name)

        hash_index = "idx_env_snapshot_hash"
        if not self._index_exists(table, hash_index):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE INDEX `{hash_index}`
                        ON `{table}` (`env_index_snapshot_hash`)
                        """
                    )
                )
            self.logger.info("环境快照表 %s 已新增索引 %s。", table, hash_index)

        run_idx = "idx_env_snapshot_run_pk"
        if not self._index_exists(table, run_idx):
            with self.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE INDEX `{run_idx}`
                        ON `{table}` (`run_pk`)
                        """
                    )
                )
            self.logger.info("环境快照表 %s 已新增索引 %s。", table, run_idx)

    def _ensure_open_monitor_env_view(
        self,
        view: str,
        env_table: str,
        weekly_table: str,
        daily_table: str,
        run_table: str,
        quote_table: str,
    ) -> None:
        if not view:
            return
        if not (env_table and run_table):
            self._drop_relation(view)
            return
        if not self._table_exists(env_table):
            self._drop_relation(view)
            return
        if not weekly_table:
            self._drop_relation(view)
            return
        if not self._table_exists(weekly_table):
            self._drop_relation(view)
            return
        if not daily_table:
            self._drop_relation(view)
            return
        if not self._table_exists(daily_table):
            self._drop_relation(view)
            return
        if not self._table_exists(run_table):
            self._drop_relation(view)
            return
        if not quote_table:
            self._drop_relation(view)
            return
        if not self._table_exists(quote_table):
            self._drop_relation(view)
            return

        history_table = "history_index_daily_kline"
        has_history = self._table_exists(history_table)
        index_asof_close = (
            "idx.`close`" if has_history else "CAST(NULL AS DOUBLE)"
        )
        index_asof_join = ""
        if has_history:
            index_asof_join = f"""
            LEFT JOIN `{history_table}` idx
              ON idx.`date` = env.`env_daily_asof_trade_date`
             AND idx.`code` = '{WEEKLY_MARKET_BENCHMARK_CODE}'
            """

        pct_change_expr = f"""
            CASE
              WHEN q.`live_latest` IS NULL THEN NULL
              WHEN {index_asof_close} IS NULL THEN NULL
              WHEN {index_asof_close} = 0 THEN NULL
              ELSE (q.`live_latest` / {index_asof_close} - 1) * 100
            END
        """
        dev_ma20_atr_expr = """
            CASE
              WHEN q.`live_latest` IS NULL THEN NULL
              WHEN daily.`ma20` IS NULL THEN NULL
              WHEN daily.`atr14` IS NULL THEN NULL
              WHEN daily.`atr14` = 0 THEN NULL
              ELSE (q.`live_latest` - daily.`ma20`) / daily.`atr14`
            END
        """
        effective_regime_expr = """
            CASE
              WHEN UPPER(COALESCE(weekly.`weekly_risk_level`, '')) = 'HIGH'
                AND UPPER(COALESCE(daily.`regime`, '')) = 'RISK_ON'
                THEN 'RISK_OFF'
              ELSE daily.`regime`
            END
        """
        gate_action_expr = f"""
            CASE
              WHEN UPPER(COALESCE({effective_regime_expr}, '')) IN ('BREAKDOWN', 'BEAR_CONFIRMED')
                THEN 'STOP'
              WHEN UPPER(COALESCE({effective_regime_expr}, '')) = 'RISK_OFF'
                THEN 'WAIT'
              WHEN UPPER(COALESCE({effective_regime_expr}, '')) = 'PULLBACK'
                THEN CASE
                  WHEN daily.`position_hint` IS NOT NULL AND daily.`position_hint` <= 0.3
                    THEN 'WAIT'
                  ELSE 'ALLOW'
                END
              WHEN UPPER(COALESCE({effective_regime_expr}, '')) = 'RISK_ON'
                THEN 'ALLOW'
              WHEN daily.`position_hint` IS NOT NULL THEN CASE
                WHEN daily.`position_hint` <= 0 THEN 'STOP'
                WHEN daily.`position_hint` < 0.3 THEN 'WAIT'
                ELSE 'ALLOW'
              END
              ELSE NULL
            END
        """

        stmt = text(
            f"""
            CREATE OR REPLACE VIEW `{view}` AS
            SELECT
              env.`monitor_date`,
              r.`run_id`,
              env.`run_pk`,
              env.`env_index_snapshot_hash`,
              env.`env_final_gate_action`,
              env.`env_final_cap_pct`,
              env.`env_weekly_asof_trade_date`,
              env.`env_daily_asof_trade_date`,
              env.`env_final_reason_json`,
              weekly.`weekly_zone_id` AS `env_weekly_zone_id`,
              daily.`daily_zone_id` AS `env_daily_zone_id`,
              env.`env_live_override_action`,
              env.`env_live_cap_multiplier`,
              env.`env_live_event_tags`,
              env.`env_live_reason`,
              daily.`score` AS `env_index_score`,
              daily.`regime` AS `env_regime_raw`,
              {effective_regime_expr} AS `env_regime`,
              daily.`position_hint` AS `env_position_hint`,
              weekly.`weekly_gate_policy` AS `env_weekly_gate_policy`,
              weekly.`weekly_risk_level` AS `env_weekly_risk_level`,
              weekly.`weekly_scene_code` AS `env_weekly_scene`,
              weekly.`weekly_structure_status` AS `env_weekly_structure_status`,
              weekly.`weekly_pattern_status` AS `env_weekly_pattern_status`,
              weekly.`weekly_plan_a_exposure_cap` AS `env_weekly_plan_a_exposure_cap`,
              weekly.`weekly_key_levels_str` AS `env_weekly_key_levels_str`,
              weekly.`weekly_zone_score` AS `env_weekly_zone_score`,
              weekly.`weekly_exp_return_bucket` AS `env_weekly_exp_return_bucket`,
              weekly.`weekly_money_proxy` AS `env_weekly_money_proxy`,
              weekly.`weekly_tags` AS `env_weekly_tags`,
              weekly.`weekly_note` AS `env_weekly_note`,
              weekly.`weekly_gate_policy` AS `env_weekly_gate_action`,
              daily.`daily_zone_score` AS `env_daily_zone_score`,
              daily.`daily_cap_multiplier` AS `env_daily_cap_multiplier`,
              daily.`daily_zone_reason` AS `env_daily_zone_reason`,
              daily.`bb_pos` AS `env_daily_bb_pos`,
              daily.`bb_width` AS `env_daily_bb_width`,
              COALESCE(
                daily.`benchmark_code`,
                weekly.`benchmark_code`,
                '{WEEKLY_MARKET_BENCHMARK_CODE}'
              ) AS `env_index_code`,
              env.`env_daily_asof_trade_date` AS `env_index_asof_trade_date`,
              q.`live_trade_date` AS `env_index_live_trade_date`,
              {index_asof_close} AS `env_index_asof_close`,
              daily.`ma20` AS `env_index_asof_ma20`,
              daily.`ma60` AS `env_index_asof_ma60`,
              daily.`macd_hist` AS `env_index_asof_macd_hist`,
              daily.`atr14` AS `env_index_asof_atr14`,
              q.`live_open` AS `env_index_live_open`,
              q.`live_high` AS `env_index_live_high`,
              q.`live_low` AS `env_index_live_low`,
              q.`live_latest` AS `env_index_live_latest`,
              {pct_change_expr} AS `env_index_live_pct_change`,
              q.`live_volume` AS `env_index_live_volume`,
              q.`live_amount` AS `env_index_live_amount`,
              {dev_ma20_atr_expr} AS `env_index_dev_ma20_atr`,
              {gate_action_expr} AS `env_index_gate_action`,
              CONCAT(
                'regime=',
                COALESCE({effective_regime_expr}, ''),
                ' pos_hint=',
                COALESCE(daily.`position_hint`, '')
              ) AS `env_index_gate_reason`,
              daily.`position_hint` AS `env_index_position_cap`
            FROM `{env_table}` env
            LEFT JOIN `{run_table}` r
              ON env.`run_pk` = r.`run_pk`
            LEFT JOIN `{weekly_table}` weekly
              ON env.`env_weekly_asof_trade_date` = weekly.`weekly_asof_trade_date`
             AND weekly.`benchmark_code` = '{WEEKLY_MARKET_BENCHMARK_CODE}'
            LEFT JOIN `{daily_table}` daily
              ON env.`env_daily_asof_trade_date` = daily.`asof_trade_date`
             AND daily.`benchmark_code` = '{WEEKLY_MARKET_BENCHMARK_CODE}'
            LEFT JOIN `{quote_table}` q
              ON env.`monitor_date` = q.`monitor_date`
             AND env.`run_pk` = q.`run_pk`
             AND q.`code` = '{WEEKLY_MARKET_BENCHMARK_CODE}'
            {index_asof_join}
            """
        )
        with self.engine.begin() as conn:
            conn.execute(stmt)
        self.logger.info("已创建/更新开盘监测环境视图 %s。", view)

    def _ensure_weekly_indicator_table(self, table: str) -> None:
        columns = {
            "weekly_asof_trade_date": "DATE NOT NULL",
            "benchmark_code": "VARCHAR(16) NOT NULL",
            "weekly_scene_code": "VARCHAR(32) NULL",
            "weekly_phase": "VARCHAR(32) NULL",
            "weekly_structure_status": "VARCHAR(32) NULL",
            "weekly_pattern_status": "VARCHAR(32) NULL",
            "weekly_risk_score": "DOUBLE NULL",
            "weekly_risk_level": "VARCHAR(16) NULL",
            "weekly_gate_policy": "VARCHAR(16) NULL",
            "weekly_plan_a_exposure_cap": "DOUBLE NULL",
            "weekly_key_levels_str": "VARCHAR(255) NULL",
            "weekly_zone_id": "VARCHAR(32) NULL",
            "weekly_zone_score": "INT NULL",
            "weekly_exp_return_bucket": "VARCHAR(16) NULL",
            "weekly_zone_reason": "VARCHAR(255) NULL",
            "weekly_money_proxy": "VARCHAR(255) NULL",
            "weekly_tags": "VARCHAR(255) NULL",
            "weekly_note": "VARCHAR(255) NULL",
            "weekly_plan_json": "TEXT NULL",
        }
        if not self._table_exists(table):
            self._create_table(
                table,
                columns,
                primary_key=("weekly_asof_trade_date", "benchmark_code"),
            )
        else:
            self._add_missing_columns(table, columns)
        self._ensure_date_column(table, "weekly_asof_trade_date", not_null=True)
        self._ensure_varchar_length(table, "benchmark_code", 16)

    def _ensure_daily_market_env_table(self, table: str) -> None:
        columns = {
            "asof_trade_date": "DATE NOT NULL",
            "benchmark_code": "VARCHAR(16) NOT NULL DEFAULT 'sh.000001'",
            "regime": "VARCHAR(32) NULL",
            "score": "DOUBLE NULL",
            "position_hint": "DOUBLE NULL",
            "ma20": "DOUBLE NULL",
            "ma60": "DOUBLE NULL",
            "ma250": "DOUBLE NULL",
            "macd_hist": "DOUBLE NULL",
            "atr14": "DOUBLE NULL",
            "dev_ma20_atr": "DOUBLE NULL",
            "bb_mid": "DOUBLE NULL",
            "bb_upper": "DOUBLE NULL",
            "bb_lower": "DOUBLE NULL",
            "bb_width": "DOUBLE NULL",
            "bb_pos": "DOUBLE NULL",
            "cycle_phase": "VARCHAR(32) NULL",
            "cycle_weekly_asof_trade_date": "DATE NULL",
            "cycle_weekly_scene_code": "VARCHAR(64) NULL",
            "breadth_pct_above_ma20": "DOUBLE NULL",
            "breadth_pct_above_ma60": "DOUBLE NULL",
            "breadth_risk_off_ratio": "DOUBLE NULL",
            "dispersion_score": "DOUBLE NULL",
            "daily_zone_id": "VARCHAR(32) NULL",
            "daily_zone_score": "INT NULL",
            "daily_cap_multiplier": "DOUBLE NULL",
            "daily_zone_reason": "VARCHAR(255) NULL",
            "components_json": "LONGTEXT NULL",
        }
        if not self._table_exists(table):
            self._create_table(
                table,
                columns,
                primary_key=("asof_trade_date", "benchmark_code"),
            )
        else:
            self._add_missing_columns(table, columns)
        self._ensure_date_column(table, "asof_trade_date", not_null=True)
        self._ensure_varchar_length(table, "benchmark_code", 16)

    def _ensure_open_monitor_view(
            self,
            view: str,
            wide_view: str,
            eval_table: str,
            env_view: str,
            quote_table: str,
            run_table: str,
    ) -> None:
        if not (wide_view and eval_table and env_view and run_table):
            return

        dim_stock_exists = self._table_exists("dim_stock_industry")
        board_dim_exists = self._table_exists("dim_stock_board_industry")
        stock_join = (
            "LEFT JOIN `dim_stock_industry` dsi ON e.`code` = dsi.`code`"
            if dim_stock_exists
            else ""
        )
        board_join = (
            "LEFT JOIN `dim_stock_board_industry` dsb ON e.`code` = dsb.`code`"
            if board_dim_exists
            else ""
        )
        name_expr = "dsi.`code_name`" if dim_stock_exists else "NULL"
        industry_expr = "dsi.`industry`" if dim_stock_exists else "NULL"
        board_name_expr = "dsb.`board_name`" if board_dim_exists else "NULL"
        board_code_expr = "dsb.`board_code`" if board_dim_exists else "NULL"
        quote_join = ""
        # 注意：eval 表只存评估/派生字段；盘口行情字段（open/high/low/latest/volume/amount）
        # 通常落在 quote 表（strategy_open_monitor_quote）中。
        live_open_expr = "NULL"
        live_high_expr = "NULL"
        live_low_expr = "NULL"
        live_latest_expr = "NULL"
        live_pct_expr = "e.`live_pct_change`"
        live_volume_expr = "NULL"
        live_amount_expr = "NULL"
        live_gap_expr = "e.`live_gap_pct`"
        live_intraday_expr = "e.`live_intraday_vol_ratio`"
        if quote_table and self._table_exists(quote_table):
            quote_join = (
                f"""
                LEFT JOIN `{quote_table}` q
                  ON e.`monitor_date` = q.`monitor_date`
                 AND e.`run_pk` = q.`run_pk`
                 AND e.`code` = q.`code`
                """
            )
            live_open_expr = "q.`live_open`"
            live_high_expr = "q.`live_high`"
            live_low_expr = "q.`live_low`"
            live_latest_expr = "q.`live_latest`"
            live_volume_expr = "q.`live_volume`"
            live_amount_expr = "q.`live_amount`"

        target_wide_view = wide_view or view
        stmt = text(
            f"""
            CREATE OR REPLACE VIEW `{target_wide_view}` AS
            SELECT
              e.`monitor_date`,
              e.`sig_date`,
              r.`run_id`,
              e.`run_pk`,
              e.`strategy_code`,
              e.`code`,
              {name_expr} AS `name`,
              {industry_expr} AS `industry`,
              {board_name_expr} AS `board_name`,
              {board_code_expr} AS `board_code`,
              e.`asof_trade_date`,
              e.`live_trade_date`,
              e.`signal_age`,
              e.`avg_volume_20`,
              {live_gap_expr} AS `live_gap_pct`,
              {live_pct_expr} AS `live_pct_change`,
              {live_intraday_expr} AS `live_intraday_vol_ratio`,
              e.`sig_close`,
              e.`sig_ma5`,
              e.`sig_ma20`,
              e.`sig_ma60`,
              e.`sig_ma250`,
              e.`sig_vol_ratio`,
              e.`sig_macd_hist`,
              e.`sig_kdj_k`,
              e.`sig_kdj_d`,
              e.`sig_atr14`,
              e.`sig_stop_ref`,
              e.`asof_close`,
              e.`asof_ma5`,
              e.`asof_ma20`,
              e.`asof_ma60`,
              e.`asof_ma250`,
              e.`asof_vol_ratio`,
              e.`asof_macd_hist`,
              e.`asof_atr14`,
              e.`asof_stop_ref`,
              e.`dev_ma5`,
              e.`dev_ma20`,
              e.`dev_ma5_atr`,
              e.`dev_ma20_atr`,
              e.`runup_from_sigclose`,
              e.`runup_from_sigclose_atr`,
              e.`runup_ref_price`,
              e.`runup_ref_source`,
              e.`entry_exposure_cap`,
              env.`env_index_score`,
              env.`env_regime`,
              env.`env_position_hint`,
              env.`env_final_gate_action`,
              env.`env_index_snapshot_hash`,
              e.`signal_strength`,
              e.`strength_delta`,
              e.`strength_trend`,
              e.`strength_note`,
              e.`signal_kind`,
              e.`sig_signal`,
              e.`sig_reason`,
              e.`state`,
              e.`status_reason`,
              e.`action`,
              e.`action_reason`,
              e.`rule_hits_json`,
              e.`summary_line`,
              e.`risk_tag`,
              e.`risk_note`,
              e.`snapshot_hash`,
              r.`checked_at`,
              r.`status`,
              {live_open_expr} AS `live_open`,
              {live_high_expr} AS `live_high`,
              {live_low_expr} AS `live_low`,
              {live_latest_expr} AS `live_latest`,
              {live_volume_expr} AS `live_volume`,
              {live_amount_expr} AS `live_amount`,
              env.`env_weekly_asof_trade_date`,
              env.`env_weekly_risk_level`,
              env.`env_weekly_scene`,
              env.`env_weekly_gate_action`,
              env.`env_weekly_gate_policy`,
              env.`env_final_cap_pct`,
              env.`env_final_reason_json`,
              env.`env_index_code`,
              env.`env_index_asof_trade_date`,
              env.`env_index_live_trade_date`,
              env.`env_index_asof_close`,
              env.`env_index_asof_ma20`,
              env.`env_index_asof_ma60`,
              env.`env_index_asof_macd_hist`,
              env.`env_index_asof_atr14`,
              env.`env_index_live_open`,
              env.`env_index_live_high`,
              env.`env_index_live_low`,
              env.`env_index_live_latest`,
              env.`env_index_live_pct_change`,
              env.`env_index_live_volume`,
              env.`env_index_live_amount`,
              env.`env_index_dev_ma20_atr`,
              env.`env_index_gate_action`,
              env.`env_index_gate_reason`,
              env.`env_index_position_cap`
            FROM `{eval_table}` e
            LEFT JOIN `{env_view}` env
              ON e.`monitor_date` = env.`monitor_date`
             AND e.`run_pk` = env.`run_pk`
            LEFT JOIN `{run_table}` r
              ON e.`run_pk` = r.`run_pk`
            {quote_join}
            {stock_join}
            {board_join}
            """
        )
        with self.engine.begin() as conn:
            conn.execute(stmt)
        self.logger.info("已创建/更新开盘监测宽视图 %s。", target_wide_view)

        if view and view != target_wide_view:
            compact_columns = [
                "monitor_date",
                "sig_date",
                "run_id",
                "run_pk",
                "strategy_code",
                "code",
                "name",
                "industry",
                "board_name",
                "board_code",
                "signal_kind",
                "sig_signal",
                "sig_reason",
                "live_trade_date",
                "live_open",
                "live_latest",
                "live_pct_change",
                "live_gap_pct",
                "live_intraday_vol_ratio",
                "state",
                "status_reason",
                "action",
                "action_reason",
                "entry_exposure_cap",
                "env_final_gate_action",
                "env_index_position_cap",
                "env_position_hint",
                "env_index_live_pct_change",
                "env_index_gate_action",
                "env_index_gate_reason",
                "rule_hits_json",
                "summary_line",
                "checked_at",
                "status",
            ]
            compact_select = ", ".join(f"`{col}`" for col in compact_columns)
            compact_stmt = text(
                f"""
                CREATE OR REPLACE VIEW `{view}` AS
                SELECT {compact_select}
                FROM `{target_wide_view}`
                """
            )
            with self.engine.begin() as conn:
                conn.execute(compact_stmt)
            self.logger.info("已创建/更新开盘监测精简视图 %s。", view)


def ensure_schema() -> None:
    db_config = DatabaseConfig.from_env()
    bootstrap_writer = MySQLWriter(db_config)
    try:
        SchemaManager(bootstrap_writer.engine, db_name=db_config.db_name).ensure_all()
    finally:
        bootstrap_writer.dispose()

================================================================================
FILE: ashare/strategy_candidates.py
================================================================================

"""策略候选池生成与刷新。"""

from __future__ import annotations

import datetime as dt
import logging
from dataclasses import dataclass
from typing import List

import pandas as pd
from sqlalchemy import inspect, text

from .config import get_section
from .db import DatabaseConfig, MySQLWriter
from .schema_manager import SchemaManager, TABLE_STRATEGY_CANDIDATES


@dataclass(frozen=True)
class StrategyCandidatesConfig:
    signal_lookback_days: int = 3

    @classmethod
    def from_config(cls) -> "StrategyCandidatesConfig":
        sec = get_section("open_monitor") or {}
        if not isinstance(sec, dict):
            return cls()

        def _get_int(key: str, default: int) -> int:
            raw = sec.get(key, default)
            try:
                return int(raw)
            except Exception:
                return default

        return cls(
            signal_lookback_days=_get_int(
                "signal_lookback_days",
                cls.signal_lookback_days,
            ),
        )


class StrategyCandidatesService:
    def __init__(self, db_writer: MySQLWriter | None = None, logger=None) -> None:
        self.db_writer = db_writer or MySQLWriter(DatabaseConfig.from_env())
        self.logger = logger or logging.getLogger(__name__)
        self.params = StrategyCandidatesConfig.from_config()
        db_name = getattr(self.db_writer.config, "db_name", None)
        self.table_names = SchemaManager(
            self.db_writer.engine,
            db_name=db_name,
        ).get_table_names()

    def refresh(self, asof_trade_date: dt.date) -> None:
        asof_date = self._normalize_date(asof_trade_date)
        liquidity_codes = self._load_liquidity_codes(asof_date)
        if not liquidity_codes:
            self.logger.error(
                "a_share_top_liquidity 在 %s 无数据，终止刷新 candidates。",
                asof_date,
            )
            raise RuntimeError(f"a_share_top_liquidity 在 {asof_date} 无数据")
        signal_df = self._load_signal_candidates(asof_date)
        merged = self._merge_candidates(asof_date, liquidity_codes, signal_df)
        self._write_candidates(asof_date, merged)

        total = len(merged)
        liquidity_cnt = int(merged["is_liquidity"].sum()) if not merged.empty else 0
        signal_cnt = int(merged["has_signal"].sum()) if not merged.empty else 0
        both_cnt = (
            int(((merged["is_liquidity"] == 1) & (merged["has_signal"] == 1)).sum())
            if not merged.empty
            else 0
        )
        snapshot_only = total - liquidity_cnt
        self.logger.info(
            "strategy_candidates 刷新完成：asof=%s total=%s liquidity=%s signal=%s both=%s snapshot_only=%s",
            asof_date,
            total,
            liquidity_cnt,
            signal_cnt,
            both_cnt,
            snapshot_only,
        )

    def _normalize_date(self, raw: dt.date | str) -> dt.date:
        if isinstance(raw, dt.date):
            return raw
        parsed = pd.to_datetime(raw, errors="coerce")
        if pd.isna(parsed):
            raise ValueError(f"无法解析 asof_trade_date={raw!r}")
        return parsed.date()

    def _table_exists(self, table: str) -> bool:
        if not table:
            return False
        try:
            with self.db_writer.engine.begin() as conn:
                conn.execute(text(f"SELECT 1 FROM `{table}` LIMIT 1"))
            return True
        except Exception:
            return False

    def _column_exists(self, table: str, column: str) -> bool:
        if not table or not column:
            return False
        try:
            inspector = inspect(self.db_writer.engine)
            cols = inspector.get_columns(table)
        except Exception:
            return False
        return any(col.get("name") == column for col in cols if isinstance(col, dict))

    def _load_trade_dates(self, latest_date: dt.date, limit: int) -> List[str]:
        lookback_days = max(int(limit), 1)
        base_table = "history_daily_kline"
        date_col = "date"
        if not self._table_exists(base_table):
            indicator_table = self.table_names.indicator_table
            if self._table_exists(indicator_table):
                base_table = indicator_table
                date_col = "trade_date"
            else:
                base_table = self.table_names.signal_events_table
                date_col = "sig_date"

        stmt = text(
            f"""
            SELECT DISTINCT CAST(`{date_col}` AS CHAR) AS d
            FROM `{base_table}`
            WHERE `{date_col}` <= :base_date
            ORDER BY d DESC
            LIMIT {lookback_days}
            """
        )
        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(stmt, conn, params={"base_date": latest_date.isoformat()})

        if df is None or df.empty:
            return []
        return df["d"].dropna().astype(str).str[:10].tolist()

    def _load_max_valid_days(self, latest_date: dt.date) -> int | None:
        table = self.table_names.signal_events_table
        if not self._table_exists(table):
            return None
        if not self._column_exists(table, "valid_days"):
            return None

        recent_dates = self._load_trade_dates(latest_date, 30)
        if not recent_dates:
            return None
        min_date = recent_dates[-1]

        stmt = text(
            f"""
            SELECT MAX(`valid_days`) AS max_valid_days
            FROM `{table}`
            WHERE `sig_date` >= :min_date
            """
        )
        with self.db_writer.engine.begin() as conn:
            row = conn.execute(stmt, {"min_date": min_date}).mappings().first()

        if not row:
            return None
        raw = row.get("max_valid_days")
        if raw is None or pd.isna(raw):
            return None
        try:
            max_days = int(raw)
        except Exception:
            return None
        return max_days if max_days > 0 else None

    def _resolve_scan_days(self, latest_date: dt.date) -> int:
        lookback = int(self.params.signal_lookback_days)
        max_valid_days = self._load_max_valid_days(latest_date)
        scan_days = lookback
        if max_valid_days is not None:
            scan_days = max(scan_days, max_valid_days + 1)
        return max(int(scan_days), 1)

    def _load_liquidity_codes(self, asof_date: dt.date) -> List[str]:
        stmt = text(
            """
            SELECT `code`
            FROM `a_share_top_liquidity`
            WHERE `date` = :d
            """
        )
        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(stmt, conn, params={"d": asof_date})
        if df.empty or "code" not in df.columns:
            return []
        return df["code"].dropna().astype(str).tolist()

    def _load_signal_candidates(self, asof_date: dt.date) -> pd.DataFrame:
        table = self.table_names.signal_events_table
        if not self._table_exists(table):
            return pd.DataFrame()

        has_valid_days = self._column_exists(table, "valid_days")
        if not has_valid_days:
            self.logger.error("strategy_signal_events 缺少 valid_days，已跳过信号候选读取。")
            return pd.DataFrame()

        scan_days = self._resolve_scan_days(asof_date)
        trade_dates = self._load_trade_dates(asof_date, scan_days)
        if not trade_dates:
            return pd.DataFrame()
        latest = trade_dates[0]
        earliest = trade_dates[-1]
        select_cols = [
            "code",
            "sig_date",
            "strategy_code",
            "UPPER(COALESCE(`final_action`, `signal`)) AS action",
            "valid_days",
        ]

        stmt = text(
            f"""
            SELECT {", ".join(select_cols)}
            FROM `{table}`
            WHERE `sig_date` BETWEEN :earliest AND :latest
              AND UPPER(COALESCE(`final_action`, `signal`)) IN ('BUY','BUY_CONFIRM')
            """
        )
        with self.db_writer.engine.begin() as conn:
            df = pd.read_sql_query(
                stmt,
                conn,
                params={"earliest": earliest, "latest": latest},
            )

        if df.empty:
            return df

        df = df.copy()
        df["sig_date"] = pd.to_datetime(df["sig_date"], errors="coerce")
        df = df.dropna(subset=["sig_date", "code"])
        df["code"] = df["code"].astype(str)
        df["action"] = df["action"].astype(str).str.upper()
        df["valid_days"] = pd.to_numeric(df.get("valid_days"), errors="coerce")
        df["signal_age"] = df["sig_date"].dt.strftime("%Y-%m-%d").map(
            {d: i for i, d in enumerate(trade_dates)}
        )
        df = df.dropna(subset=["valid_days", "signal_age"])
        if df.empty:
            return df
        df["valid_days"] = df["valid_days"].astype(int)
        df["signal_age"] = df["signal_age"].astype(int)
        df = df[df["signal_age"] <= df["valid_days"]]
        if df.empty:
            return df

        action_priority = {"BUY": 1, "BUY_CONFIRM": 2}
        df["action_priority"] = df["action"].map(action_priority).fillna(0).astype(int)
        df = df.sort_values(
            ["code", "sig_date", "action_priority"],
            ascending=[True, False, False],
        )
        latest_df = df.groupby("code", as_index=False).first()
        latest_df = latest_df.rename(
            columns={
                "sig_date": "latest_sig_date",
                "action": "latest_sig_action",
                "strategy_code": "latest_sig_strategy_code",
            }
        )
        return latest_df[
            ["code", "latest_sig_date", "latest_sig_action", "latest_sig_strategy_code"]
        ]

    def _merge_candidates(
        self,
        asof_date: dt.date,
        liquidity_codes: List[str],
        signal_df: pd.DataFrame,
    ) -> pd.DataFrame:
        liquidity_df = pd.DataFrame({"code": liquidity_codes}).drop_duplicates()
        if not liquidity_df.empty:
            liquidity_df["is_liquidity"] = 1
        else:
            liquidity_df = pd.DataFrame(columns=["code", "is_liquidity"])

        signal_base = signal_df.copy()
        if not signal_base.empty:
            signal_base["has_signal"] = 1
        else:
            signal_base = pd.DataFrame(
                columns=[
                    "code",
                    "latest_sig_date",
                    "latest_sig_action",
                    "latest_sig_strategy_code",
                    "has_signal",
                ]
            )

        merged = pd.merge(liquidity_df, signal_base, on="code", how="outer")
        if merged.empty:
            return merged

        merged["is_liquidity"] = (
            pd.to_numeric(merged.get("is_liquidity"), errors="coerce")
            .fillna(0)
            .astype(int)
        )
        merged["has_signal"] = (
            pd.to_numeric(merged.get("has_signal"), errors="coerce")
            .fillna(0)
            .astype(int)
        )
        merged["asof_trade_date"] = asof_date
        merged["latest_sig_date"] = pd.to_datetime(
            merged.get("latest_sig_date"),
            errors="coerce",
        ).dt.date
        merged["latest_sig_action"] = merged.get("latest_sig_action")
        merged["latest_sig_strategy_code"] = merged.get("latest_sig_strategy_code")
        merged["created_at"] = dt.datetime.now()
        return merged[
            [
                "asof_trade_date",
                "code",
                "is_liquidity",
                "has_signal",
                "latest_sig_date",
                "latest_sig_action",
                "latest_sig_strategy_code",
                "created_at",
            ]
        ]

    def _write_candidates(self, asof_date: dt.date, df: pd.DataFrame) -> None:
        delete_stmt = text(
            f"DELETE FROM `{TABLE_STRATEGY_CANDIDATES}` WHERE `asof_trade_date` = :d"
        )
        with self.db_writer.engine.begin() as conn:
            conn.execute(delete_stmt, {"d": asof_date})

        if df.empty:
            self.logger.warning(
                "strategy_candidates asof=%s 无可写入候选（已清空旧数据）。",
                asof_date,
            )
            return

        df = df.copy()
        df["code"] = df["code"].astype(str)
        self.db_writer.write_dataframe(df, TABLE_STRATEGY_CANDIDATES, if_exists="append")

================================================================================
FILE: ashare/universe.py
================================================================================

"""基于 Baostock 日线数据的交易标的筛选工具."""

from __future__ import annotations

from typing import Set

import pandas as pd


class AshareUniverseBuilder:
    """使用 Baostock 数据构建当日交易候选池。"""

    def __init__(
        self,
        top_liquidity_count: int = 100,
        min_listing_days: int = 60,
    ) -> None:
        self.top_liquidity_count = top_liquidity_count
        self.min_listing_days = min_listing_days

    def _infer_st_codes(
        self, stock_df: pd.DataFrame, latest_kline: pd.DataFrame
    ) -> Set[str]:
        if "code" not in stock_df.columns or "code_name" not in stock_df.columns:
            return set()

        names = stock_df["code_name"].astype(str)

        # 使用非捕获分组 (?:...)，避免 pandas 对捕获分组的 warning
        mask_name = (
            names.str.contains(r"^(?:ST|\*ST)", case=False)
            | names.str.contains(r"(?:退|delist)", case=False)
        )

        st_candidates = set(stock_df.loc[mask_name, "code"])

        if latest_kline.empty:
            return st_candidates

        required_cols = {"code", "date", "isST"}
        if not required_cols.issubset(latest_kline.columns):
            return st_candidates
        mask_official = latest_kline["isST"].astype(str) == "1"
        st_candidates.update(set(latest_kline.loc[mask_official, "code"]))

        return st_candidates

    def _infer_stop_codes(self, latest_kline: pd.DataFrame) -> Set[str]:
        cols = latest_kline.columns
        if "code" not in cols:
            return set()

        if "tradestatus" in cols:
            stopped = latest_kline[latest_kline["tradestatus"] != "1"]
            return set(stopped["code"])

        return set()

    def build_universe(
        self,
        stock_df: pd.DataFrame,
        history_df: pd.DataFrame,
        stock_basic_df: pd.DataFrame | None = None,
        industry_df: pd.DataFrame | None = None,
        index_membership: dict[str, set[str]] | None = None,
    ) -> pd.DataFrame:
        if stock_df.empty:
            raise RuntimeError("候选池构建失败：股票列表为空。")
        if history_df.empty:
            raise RuntimeError("候选池构建失败：历史日线数据为空。")

        latest_trade_date = None
        if "date" in history_df.columns:
            latest_trade_date = (
                pd.to_datetime(history_df["date"], errors="coerce")
                .dropna()
                .max()
            )

        filtered_stock_df = stock_df.copy()
        if stock_basic_df is not None and not stock_basic_df.empty:
            filtered_stock_df = filtered_stock_df.merge(
                stock_basic_df,
                on="code",
                how="left",
                suffixes=(None, "_basic"),
            )

            type_col = filtered_stock_df.get("type")
            status_col = filtered_stock_df.get("status")
            if type_col is not None and status_col is not None:
                filtered_stock_df = filtered_stock_df[
                    (pd.to_numeric(type_col, errors="coerce") == 1)
                    & (pd.to_numeric(status_col, errors="coerce") == 1)
                ]

            if latest_trade_date is not None and "ipoDate" in filtered_stock_df.columns:
                ipo_dates = pd.to_datetime(
                    filtered_stock_df["ipoDate"], errors="coerce"
                )
                age_days = (latest_trade_date - ipo_dates).dt.days
                filtered_stock_df = filtered_stock_df[
                    (age_days >= self.min_listing_days) | age_days.isna()
                ]

        # 提取每个标的最新一个交易日的日线数据
        latest_rows = (
            history_df.sort_values("date")
            .groupby("code", as_index=False)
            .tail(1)
            .reset_index(drop=True)
        )

        st_codes = self._infer_st_codes(filtered_stock_df, latest_rows)
        stop_codes = self._infer_stop_codes(latest_rows)
        bad_codes = st_codes | stop_codes

        merged = filtered_stock_df.merge(latest_rows, on="code", how="left")
        filtered = merged[~merged["code"].isin(bad_codes)].copy()

        # --- 新增：解决 MySQL 大小写不敏感导致的重复列名问题 ---
        # stock_df 里有 `tradeStatus`，latest_rows 里有 `tradestatus`
        # 在 MySQL 里会被视为同一个列名，导致 1060 Duplicate column name 错误。
        if "tradeStatus" in filtered.columns and "tradestatus" in filtered.columns:
            # 这里保留日线里的 `tradestatus`，删除股票列表里的 `tradeStatus`
            filtered = filtered.drop(columns=["tradeStatus"])
        # --- 新增结束 ---

        if industry_df is not None and not industry_df.empty:
            filtered = filtered.merge(
                industry_df,
                on="code",
                how="left",
                suffixes=(None, "_industry"),
            )

        if index_membership:
            for index_name, members in index_membership.items():
                flag_col = f"in_{index_name}"
                filtered[flag_col] = filtered["code"].isin(members)

        if "amount" in filtered.columns:
            filtered["amount"] = pd.to_numeric(filtered["amount"], errors="coerce")

        return filtered

    def pick_top_liquidity(self, universe_df: pd.DataFrame) -> pd.DataFrame:
        if universe_df.empty:
            raise RuntimeError("候选池为空，无法筛选流动性。")
        if "amount" not in universe_df.columns:
            raise RuntimeError("候选池缺少成交额字段，无法进行排序。")

        sorted_df = universe_df.sort_values("amount", ascending=False)
        return sorted_df.head(self.top_liquidity_count)

================================================================================
FILE: ashare/utils/__init__.py
================================================================================

"""工具模块."""

from .logger import setup_logger

__all__ = ["setup_logger"]

================================================================================
FILE: ashare/utils/convert.py
================================================================================

"""通用数值转换工具。"""

from __future__ import annotations

import math
from typing import Any


def to_float(value: Any) -> float | None:  # noqa: ANN401
    """尽可能安全地将输入转换为 float。"""

    try:
        if value is None:
            return None
        if isinstance(value, str):
            v = value.strip()
            if v in {"", "-", "--", "None", "nan"}:
                return None
            return float(v.replace(",", ""))
        if isinstance(value, (int, float)):
            if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
                return None
            return float(value)
        return float(value)
    except Exception:
        return None

================================================================================
FILE: ashare/utils/logger.py
================================================================================

"""项目统一日志配置。"""

from __future__ import annotations

import logging
import os
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Optional, Union

from ashare.config import get_section

_DEFAULT_FMT = "%(asctime)s [%(levelname)s] %(message)s"
_DEFAULT_DATEFMT = "%Y-%m-%d %H:%M:%S"


def _parse_level(value: str) -> int:
    value = value.strip().upper()
    if value.isdigit():
        return int(value)
    return getattr(logging, value, logging.INFO)


def _get_level_from_config() -> Optional[int]:
    section = get_section("logging")
    level_value = section.get("level")
    if level_value is None:
        return None
    if isinstance(level_value, int):
        return level_value
    if isinstance(level_value, str):
        return _parse_level(level_value)
    return _parse_level(str(level_value))


def setup_logger(
    log_dir: Optional[Union[str, Path]] = None,
    level: int = logging.INFO,
    *,
    console_level: Optional[int] = None,
    file_level: Optional[int] = None,
    max_bytes: int = 20 * 1024 * 1024,
    backup_count: int = 5,
) -> logging.Logger:
    """配置并返回项目 logger（幂等，可重复调用）。

    - 默认写入 {log_dir}/ashare.log（log_dir 为空则写到项目根目录）。
    - 控制台 + 文件同时输出；文件使用滚动切分，避免日志无限增大。
    - 若设置了环境变量 ASHARE_LOG_LEVEL / ASHARE_LOG_DIR，或 config.yaml.logging.level，会覆盖默认值。
    """

    env_dir = os.getenv("ASHARE_LOG_DIR")
    if env_dir:
        log_dir = env_dir

    config_level = _get_level_from_config()
    if config_level is not None:
        level = config_level

    env_level = os.getenv("ASHARE_LOG_LEVEL")
    if env_level:
        level = _parse_level(env_level)

    if log_dir is None:
        log_dir_path = Path(__file__).resolve().parents[2]
    else:
        log_dir_path = Path(log_dir)

    log_dir_path.mkdir(parents=True, exist_ok=True)
    log_file = log_dir_path / "ashare.log"

    logger = logging.getLogger("ashare")

    if getattr(logger, "_ashare_configured", False):
        return logger

    logger.setLevel(logging.DEBUG)
    logger.propagate = False

    for handler in list(logger.handlers):
        logger.removeHandler(handler)
        try:
            handler.close()
        except Exception:
            pass

    fmt = logging.Formatter(_DEFAULT_FMT, datefmt=_DEFAULT_DATEFMT)

    if console_level is None:
        console_level = level
    if file_level is None:
        file_level = level

    console_handler = logging.StreamHandler()
    console_handler.setLevel(console_level)
    console_handler.setFormatter(fmt)

    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=max_bytes,
        backupCount=backup_count,
        encoding="utf-8",
    )
    file_handler.setLevel(file_level)
    file_handler.setFormatter(fmt)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    logging.getLogger().setLevel(logging.WARNING)
    for noisy in ("urllib3", "sqlalchemy.engine"):
        logging.getLogger(noisy).setLevel(logging.WARNING)

    logger._ashare_configured = True  # type: ignore[attr-defined]
    logger.debug("日志已初始化：log_file=%s level=%s", log_file, level)
    return logger

================================================================================
FILE: ashare/weekly_channel_regime.py
================================================================================

from __future__ import annotations

"""周线下跌/上升通道（线性回归通道）+ 30/60 周均线的情景分类。

设计目标：
- 尽量把“手动画通道”的主观部分，替换成可重复的计算方法；
- 输出“情景（state）+ 仓位提示（position_hint）+ 通道位置百分比（chan_pos）+ 关键价位
  （upper/lower/MA30/MA60）”，便于在 open_monitor / 报告里展示或做过滤。

注意：
- 这里不做艾略特波浪计数（主观性强），只保留可客观落地的条件判断；
- 通道用线性回归中轴 + 残差标准差*dev 的上下轨。
"""

from dataclasses import dataclass
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd


def _to_weekly_ohlcv(df_daily: pd.DataFrame) -> pd.DataFrame:
    """把日线 OHLCV 按 code 聚合成周线（week_end=当周最后交易日）。

    期望列：code,date,open,high,low,close,volume,(optional amount)
    返回列：code,week_end,open,high,low,close,volume,(optional amount)
    """

    if df_daily is None or df_daily.empty:
        return pd.DataFrame(columns=["code", "week_end", "open", "high", "low", "close", "volume"])

    work = df_daily.copy()
    work["date_dt"] = pd.to_datetime(work["date"], errors="coerce")
    for col in ["open", "high", "low", "close", "volume", "amount"]:
        if col in work.columns:
            work[col] = pd.to_numeric(work[col], errors="coerce")

    keep = [
        c
        for c in ["code", "date", "date_dt", "open", "high", "low", "close", "volume", "amount"]
        if c in work.columns
    ]
    work = work[keep].dropna(subset=["code", "date_dt"]).copy()
    if work.empty:
        return pd.DataFrame(columns=["code", "week_end", "open", "high", "low", "close", "volume"])

    out_parts = []
    for code, grp in work.groupby("code", sort=False):
        grp = grp.sort_values("date_dt").copy()
        grp["week_key"] = grp["date_dt"].dt.to_period("W-FRI")

        def _agg_week(g: pd.DataFrame) -> pd.Series:
            g = g.sort_values("date_dt")
            week_end_dt = g["date_dt"].iloc[-1]
            week_end = week_end_dt.date().isoformat() if pd.notna(week_end_dt) else None
            data = {
                "open": g["open"].iloc[0],
                "high": g["high"].max(),
                "low": g["low"].min(),
                "close": g["close"].iloc[-1],
                "volume": g["volume"].sum(),
                "week_end": week_end,
            }
            if "amount" in g.columns:
                data["amount"] = g["amount"].sum()
            return pd.Series(data)

        grouped = grp.groupby("week_key", sort=False)
        try:
            wk = (
                grouped.apply(_agg_week, include_groups=False)
                .dropna(subset=["open", "high", "low", "close"])
                .reset_index(drop=True)
            )
        except TypeError:
            wk = (
                grouped.apply(_agg_week)
                .dropna(subset=["open", "high", "low", "close"])
                .reset_index(drop=True)
            )
        if wk.empty:
            continue
        wk["code"] = str(code)
        out_parts.append(wk)

    if not out_parts:
        return pd.DataFrame(columns=["code", "week_end", "open", "high", "low", "close", "volume"])

    out = pd.concat(out_parts, ignore_index=True)
    out = out.sort_values(["code", "week_end"]).reset_index(drop=True)

    cols = ["code", "week_end", "open", "high", "low", "close", "volume"]
    if "amount" in out.columns:
        cols.append("amount")
    out = out[[c for c in cols if c in out.columns]]
    return out


def _sma(s: pd.Series, n: int) -> pd.Series:
    n = max(int(n), 1)
    return s.rolling(n, min_periods=n).mean()


def _linear_regression_channel(
    close: pd.Series,
    *,
    length: int = 52,
    dev: float = 2.0,
) -> pd.DataFrame:
    """线性回归通道：center + dev*std(residual) 的上下轨。

    - length: 回归窗口（周）
    - dev: 标准差倍数
    """

    close = pd.to_numeric(close, errors="coerce").astype(float)
    n = len(close)
    center = np.full(n, np.nan, dtype=float)
    upper = np.full(n, np.nan, dtype=float)
    lower = np.full(n, np.nan, dtype=float)
    slope = np.full(n, np.nan, dtype=float)

    length = max(int(length), 2)
    x = np.arange(length, dtype=float)

    for i in range(length - 1, n):
        y = close.iloc[i - length + 1 : i + 1].to_numpy(dtype=float)
        if np.any(np.isnan(y)):
            continue
        a, b = np.polyfit(x, y, 1)  # y = a*x + b
        y_hat = a * x + b
        resid = y - y_hat
        sigma = float(np.std(resid, ddof=1)) if length > 2 else 0.0

        center[i] = y_hat[-1]
        upper[i] = y_hat[-1] + float(dev) * sigma
        lower[i] = y_hat[-1] - float(dev) * sigma
        slope[i] = a

    return pd.DataFrame(
        {
            "lrc_center": center,
            "lrc_upper": upper,
            "lrc_lower": lower,
            "lrc_slope": slope,
        },
        index=close.index,
    )


@dataclass
class WeeklyChannelResult:
    state: Optional[str]
    position_hint: Optional[float]
    detail: Dict[str, Dict[str, Any]]
    context: Dict[str, Any]

    def to_payload(self) -> Dict[str, Any]:
        payload = {
            "state": self.state,
            "position_hint": self.position_hint,
            "detail": self.detail,
        }
        payload.update(self.context)
        return payload


class WeeklyChannelClassifier:
    """周线通道情景分类器（指数/个股均可）。"""

    def __init__(
        self,
        *,
        lrc_length: int = 52,
        lrc_dev: float = 2.0,
        touch_chan_eps: float = 0.005,
        near_lower_eps: float = 0.02,
        near_ma_eps: float = 0.01,
        ma_fast: int = 30,
        ma_slow: int = 60,
        primary_code: str = "sh.000001",
    ) -> None:
        self.lrc_length = max(int(lrc_length), 2)
        self.lrc_dev = float(lrc_dev)
        self.touch_chan_eps = float(touch_chan_eps)
        self.near_lower_eps = float(near_lower_eps)
        self.near_ma_eps = float(near_ma_eps)
        self.ma_fast = max(int(ma_fast), 1)
        self.ma_slow = max(int(ma_slow), 1)
        self.primary_code = str(primary_code or "").strip() or "sh.000001"

    @staticmethod
    def _near(x: float | None, y: float | None, eps: float) -> bool:
        if x is None or y is None:
            return False
        if pd.isna(x) or pd.isna(y) or y == 0:
            return False
        return abs(float(x) - float(y)) / abs(float(y)) <= float(eps)

    def _classify_one(self, wk: pd.DataFrame) -> Dict[str, Any]:
        # 分组后的 wk 可能继承了全表的原始 index（不是从 0 开始的连续 RangeIndex）
        # 若直接 concat(lrc.reset_index(drop=True)) 会发生 index 对齐错位，导致出现
        # close/week_end 为 None 但通道列有值的“幽灵行”。
        wk = wk.copy().reset_index(drop=True)
        wk["ma30"] = _sma(wk["close"], self.ma_fast)
        wk["ma60"] = _sma(wk["close"], self.ma_slow)
        wk["vol_ma20"] = _sma(wk["volume"], 20)
        lrc = _linear_regression_channel(wk["close"], length=self.lrc_length, dev=self.lrc_dev)
        # wk 已 reset_index(drop=True)，lrc 的 index 与 wk 完全一致，直接 concat 即可
        wk = pd.concat([wk, lrc], axis=1)

        last = wk.iloc[-1]
        close = float(last.get("close")) if pd.notna(last.get("close")) else None
        high = float(last.get("high")) if pd.notna(last.get("high")) else None
        low = float(last.get("low")) if pd.notna(last.get("low")) else None
        upper = float(last.get("lrc_upper")) if pd.notna(last.get("lrc_upper")) else None
        lower = float(last.get("lrc_lower")) if pd.notna(last.get("lrc_lower")) else None
        slope = float(last.get("lrc_slope")) if pd.notna(last.get("lrc_slope")) else None
        ma30 = float(last.get("ma30")) if pd.notna(last.get("ma30")) else None
        ma60 = float(last.get("ma60")) if pd.notna(last.get("ma60")) else None
        volume = float(last.get("volume")) if pd.notna(last.get("volume")) else None
        amount = float(last.get("amount")) if pd.notna(last.get("amount")) else None
        vol_ma20 = float(last.get("vol_ma20")) if pd.notna(last.get("vol_ma20")) else None

        wk_vol_ratio_20 = None
        if volume is not None and vol_ma20 not in (None, 0):
            wk_vol_ratio_20 = volume / vol_ma20 if vol_ma20 else None

        prev_high = None
        if len(wk) >= 2:
            prev_row = wk.iloc[-2]
            prev_high = float(prev_row.get("high")) if pd.notna(prev_row.get("high")) else None

        slope_change_4w = None
        slope_shift = wk["lrc_slope"].shift(4)
        if not slope_shift.empty:
            last_slope = wk["lrc_slope"].iloc[-1]
            prev_slope = slope_shift.iloc[-1]
            if pd.notna(last_slope) and pd.notna(prev_slope):
                slope_change_4w = float(last_slope - prev_slope)

        state = "INSIDE_CHANNEL"
        note = "仍在通道内运行"

        chan_pos = None
        chan_pos_clamped = None
        if close is not None and upper is not None and lower is not None and upper > lower:
            chan_pos = (close - lower) / (upper - lower)
            chan_pos_clamped = min(max(chan_pos, 0.0), 1.0)

        near_lower = (
            lower is not None
            and (
                (close is not None and self._near(close, lower, self.near_lower_eps))
                or (low is not None and low <= lower * (1.0 + self.touch_chan_eps))
            )
        )
        near_ma30 = self._near(close, ma30, self.near_ma_eps) or (
            ma30 is not None and low is not None and low <= ma30 * (1.0 + self.near_ma_eps)
        )

        if upper is not None and close is not None and close > upper:
            state = "CHANNEL_BREAKOUT_UP"
            note = "周收盘价上破通道上轨"
        elif lower is not None and close is not None and close < lower:
            state = "CHANNEL_BREAKDOWN"
            note = "周收盘价跌破通道下轨"
        elif near_lower and near_ma30:
            state = "LOWER_RAIL_NEAR_MA30_ZONE"
            note = "触及/接近下轨且在30周线附近（反弹观察区）"
        elif near_lower:
            state = "NEAR_LOWER_RAIL"
            note = "靠近通道下轨（下轨反弹观察区）"
        elif (
            self._near(close, ma60, self.near_ma_eps)
            or (ma60 is not None and low is not None and low <= ma60 * (1.0 + self.near_ma_eps))
        ):
            state = "EXTREME_NEAR_MA60_ZONE"
            note = "回踩到60周线附近（更深调整观察区）"

        position_hint_map = {
            "CHANNEL_BREAKOUT_UP": 0.8,
            "INSIDE_CHANNEL": 0.6,
            "NEAR_LOWER_RAIL": 0.5,
            "LOWER_RAIL_NEAR_MA30_ZONE": 0.4,
            "EXTREME_NEAR_MA60_ZONE": 0.2,
            "CHANNEL_BREAKDOWN": 0.0,
        }

        position_hint = chan_pos_clamped
        if state == "CHANNEL_BREAKOUT_UP":
            base = position_hint_map.get(state)
            position_hint = base if position_hint is None else max(position_hint, base)
        elif state == "CHANNEL_BREAKDOWN":
            position_hint = 0.0
        elif state == "EXTREME_NEAR_MA60_ZONE":
            position_hint = 0.3 if position_hint is None else min(position_hint, 0.3)
        elif state == "LOWER_RAIL_NEAR_MA30_ZONE":
            position_hint = 0.4 if position_hint is None else min(position_hint, 0.4)
        elif state == "NEAR_LOWER_RAIL":
            position_hint = 0.5 if position_hint is None else min(position_hint, 0.5)
        elif position_hint is None:
            position_hint = position_hint_map.get(state)

        week_end = last.get("week_end")
        week_end_str = None
        if pd.notna(week_end):
            try:
                week_end_str = pd.to_datetime(week_end).date().isoformat()
            except Exception:
                week_end_str = str(week_end)[:10]

        return {
            "week_end": week_end_str,
            "close": close,
            "ma30": ma30,
            "ma60": ma60,
            "chan_upper": upper,
            "chan_lower": lower,
            "chan_slope": slope,
            "chan_pos": chan_pos,
            "high": high,
            "low": low,
            "wk_volume": volume,
            "wk_amount": amount,
            "wk_vol_ma20": vol_ma20,
            "wk_vol_ratio_20": wk_vol_ratio_20,
            "slope_change_4w": slope_change_4w,
            "prev_high": prev_high,
            "channel_dir": (
                "DOWN" if (slope is not None and slope < 0) else "UP" if (slope is not None and slope > 0) else None
            ),
            "state": state,
            "position_hint": position_hint,
            "note": note,
        }

    def classify(self, df_daily: pd.DataFrame) -> WeeklyChannelResult:
        if df_daily is None or df_daily.empty:
            return WeeklyChannelResult(state=None, position_hint=None, detail={}, context={})

        wk = _to_weekly_ohlcv(df_daily)
        if wk.empty:
            return WeeklyChannelResult(state=None, position_hint=None, detail={}, context={})

        detail: Dict[str, Dict[str, Any]] = {}
        weekly_bars_by_code: Dict[str, list[dict[str, Any]]] = {}
        for code, grp in wk.groupby("code", sort=False):
            grp = grp.sort_values("week_end").reset_index(drop=True)
            if len(grp) < max(self.ma_slow, self.lrc_length):
                # 数据不足时仍输出最新值，但 state/通道可能为 None
                payload = self._classify_one(grp)
            else:
                payload = self._classify_one(grp)
            detail[str(code)] = payload

            code_weekly_bars: list[dict[str, Any]] = []
            if not grp.empty:
                for _, row in grp.tail(120).iterrows():
                    try:
                        week_end = pd.to_datetime(row.get("week_end")).date().isoformat()
                    except Exception:
                        week_end = str(row.get("week_end"))[:10]
                    code_weekly_bars.append(
                        {
                            "week_end": week_end,
                            "open": float(row.get("open")) if pd.notna(row.get("open")) else None,
                            "high": float(row.get("high")) if pd.notna(row.get("high")) else None,
                            "low": float(row.get("low")) if pd.notna(row.get("low")) else None,
                            "close": float(row.get("close")) if pd.notna(row.get("close")) else None,
                            "volume": float(row.get("volume")) if pd.notna(row.get("volume")) else None,
                            "amount": float(row.get("amount")) if pd.notna(row.get("amount")) else None,
                        }
                    )
            weekly_bars_by_code[str(code)] = code_weekly_bars

        # 选一个“主参考指数”输出整体 state（便于 open_monitor 直接引用）
        primary = self.primary_code
        if primary not in detail and detail:
            primary = sorted(detail.keys())[0]
        primary_payload = detail.get(primary, {}) if detail else {}

        primary_weekly_bars: list[dict[str, Any]] = []
        if not wk.empty and primary:
            primary_df = wk[wk["code"] == primary].sort_values("week_end").tail(120)
            if not primary_df.empty:
                for _, row in primary_df.iterrows():
                    try:
                        week_end = pd.to_datetime(row.get("week_end")).date().isoformat()
                    except Exception:
                        week_end = str(row.get("week_end"))[:10]
                    primary_weekly_bars.append(
                        {
                            "week_end": week_end,
                            "open": float(row.get("open")) if pd.notna(row.get("open")) else None,
                            "high": float(row.get("high")) if pd.notna(row.get("high")) else None,
                            "low": float(row.get("low")) if pd.notna(row.get("low")) else None,
                            "close": float(row.get("close")) if pd.notna(row.get("close")) else None,
                            "volume": float(row.get("volume")) if pd.notna(row.get("volume")) else None,
                            "amount": float(row.get("amount")) if pd.notna(row.get("amount")) else None,
                        }
                    )

        return WeeklyChannelResult(
            state=primary_payload.get("state"),
            position_hint=primary_payload.get("position_hint"),
            detail=detail,
            context={
                "primary_code": primary if detail else None,
                "week_end": primary_payload.get("week_end"),
                "chan_pos": primary_payload.get("chan_pos"),
                "note": primary_payload.get("note"),
                "wk_vol_ratio_20": primary_payload.get("wk_vol_ratio_20"),
                "slope_change_4w": primary_payload.get("slope_change_4w"),
                "primary_weekly_bars": primary_weekly_bars,
                "weekly_bars_by_code": weekly_bars_by_code,
            },
        )

================================================================================
FILE: ashare/weekly_env_builder.py
================================================================================

"""周线环境构建器。"""

from __future__ import annotations

import datetime as dt
import json
import logging
from typing import Any, Dict

import pandas as pd
from sqlalchemy import bindparam, text

from .baostock_core import BaostockDataFetcher
from .baostock_session import BaostockSession
from .market_regime import MarketRegimeClassifier
from .schema_manager import WEEKLY_MARKET_BENCHMARK_CODE
from .utils.convert import to_float
from .weekly_channel_regime import WeeklyChannelClassifier
from .weekly_pattern_system import WeeklyPlanSystem


class WeeklyEnvironmentBuilder:
    """负责构建周线环境上下文的组件。"""

    def __init__(
        self,
        *,
        db_writer,
        logger: logging.Logger,
        index_codes: list[str],
        board_env_enabled: bool,
        board_spot_enabled: bool,
        env_index_score_threshold: float,
        weekly_soft_gate_strength_threshold: float,
    ) -> None:
        self.db_writer = db_writer
        self.logger = logger
        self.index_codes = index_codes
        self.board_env_enabled = board_env_enabled
        self.board_spot_enabled = board_spot_enabled
        self.env_index_score_threshold = env_index_score_threshold
        self.weekly_soft_gate_strength_threshold = weekly_soft_gate_strength_threshold

        self.market_regime = MarketRegimeClassifier()
        self.benchmark_code = WEEKLY_MARKET_BENCHMARK_CODE
        self.weekly_channel = WeeklyChannelClassifier(primary_code=self.benchmark_code)
        self.weekly_plan_system = WeeklyPlanSystem()

        self._calendar_cache: set[str] = set()
        self._calendar_range: tuple[dt.date, dt.date] | None = None
        self._baostock_client: BaostockDataFetcher | None = None

    def _get_baostock_client(self) -> BaostockDataFetcher:
        if self._baostock_client is None:
            self._baostock_client = BaostockDataFetcher(BaostockSession())
        return self._baostock_client

    def load_trading_calendar(self, start: dt.date, end: dt.date) -> bool:
        """加载并缓存交易日历，避免节假日误判。"""

        if (
            self._calendar_range
            and start >= self._calendar_range[0]
            and end <= self._calendar_range[1]
        ):
            return True

        current_start = start
        current_end = end
        if self._calendar_range:
            current_start = min(self._calendar_range[0], start)
            current_end = max(self._calendar_range[1], end)

        try:
            client = self._get_baostock_client()
            calendar_df = client.get_trade_calendar(
                current_start.isoformat(), current_end.isoformat()
            )
        except Exception as exc:  # noqa: BLE001
            self.logger.warning("加载交易日历失败，将回退工作日判断：%s", exc)
            return False

        if calendar_df.empty or "calendar_date" not in calendar_df.columns:
            return False

        dates = (
            pd.to_datetime(calendar_df["calendar_date"], errors="coerce")
            .dt.date.dropna()
            .tolist()
        )
        self._calendar_cache.update({d.isoformat() for d in dates})
        self._calendar_range = (current_start, current_end)
        return True

    def resolve_latest_closed_week_end(self, latest_trade_date: str) -> tuple[str, bool]:
        """确定最近一个已收盘的周末交易日（周线确认）。"""

        def _parse_date(val: str) -> dt.date | None:
            try:
                return dt.datetime.strptime(val, "%Y-%m-%d").date()
            except Exception:  # noqa: BLE001
                return None

        trade_date = _parse_date(latest_trade_date)
        if trade_date is None:
            return latest_trade_date, False

        week_start = trade_date - dt.timedelta(days=trade_date.weekday())
        week_end = week_start + dt.timedelta(days=6)
        calendar_loaded = self.load_trading_calendar(
            week_start - dt.timedelta(days=21), week_end
        )

        def _in_cache(date_val: dt.date) -> bool:
            return date_val.isoformat() in self._calendar_cache

        if calendar_loaded:
            last_trade_day_in_week: dt.date | None = None
            for i in range(7):
                candidate = week_end - dt.timedelta(days=i)
                if _in_cache(candidate):
                    last_trade_day_in_week = candidate
                    break

            if last_trade_day_in_week:
                prev_week_last: dt.date | None = None
                prev_candidate = week_start - dt.timedelta(days=1)
                for _ in range(30):
                    if _in_cache(prev_candidate):
                        prev_week_last = prev_candidate
                        break
                    prev_candidate -= dt.timedelta(days=1)

                week_end_asof = (
                    trade_date
                    if trade_date == last_trade_day_in_week
                    else prev_week_last or last_trade_day_in_week
                )
                return week_end_asof.isoformat(), trade_date == week_end_asof

        fallback_friday = week_start + dt.timedelta(days=4)
        if trade_date >= fallback_friday:
            week_end_asof = fallback_friday
            return week_end_asof.isoformat(), trade_date == week_end_asof

        prev_friday = fallback_friday - dt.timedelta(days=7)
        return prev_friday.isoformat(), trade_date == prev_friday

    def load_index_trend(self, latest_trade_date: str) -> Dict[str, Any]:
        """加载指数趋势情景。"""

        if not self.index_codes or not self._table_exists("history_index_daily_kline"):
            return {"score": None, "detail": {}, "regime": None, "position_hint": None}

        end_date = dt.datetime.strptime(latest_trade_date, "%Y-%m-%d").date()
        start_date = (end_date - dt.timedelta(days=600)).isoformat()

        stmt = text(
            """
            SELECT `code`, `date`, `open`, `high`, `low`, `close`, `volume`, `amount`
            FROM history_index_daily_kline
            WHERE `code` IN :codes AND `date` >= :start_date AND `date` <= :end_date
            ORDER BY `code`, `date`
            """
        ).bindparams(bindparam("codes", expanding=True))

        try:
            with self.db_writer.engine.begin() as conn:
                df = pd.read_sql_query(
                    stmt,
                    conn,
                    params={
                        "codes": self.index_codes,
                        "start_date": start_date,
                        "end_date": latest_trade_date,
                    },
                )
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取指数日线失败：%s", exc)
            return {"score": None, "detail": {}, "regime": None, "position_hint": None}

        if df.empty:
            return {"score": None, "detail": {}, "regime": None, "position_hint": None}

        regime_result = self.market_regime.classify(df)
        payload = regime_result.to_payload()
        return payload

    def _table_exists(self, table: str) -> bool:
        try:
            with self.db_writer.engine.begin() as conn:
                conn.execute(text(f"SELECT 1 FROM `{table}` LIMIT 1"))
            return True
        except Exception:
            return False

    def load_board_spot_strength_from_db(
        self, latest_trade_date: str | None, checked_at: dt.datetime | None
    ) -> pd.DataFrame:
        """优先从数据库读取板块强弱快照。"""

        if not self.board_env_enabled or not self.board_spot_enabled:
            return pd.DataFrame()

        table = "board_industry_spot"
        if not self._table_exists(table):
            return pd.DataFrame()

        target_ts = checked_at or dt.datetime.now()
        latest_ts = None
        stmt_latest = text(
            f"""
            SELECT MAX(`ts`) AS ts
            FROM `{table}`
            WHERE `ts` <= :ts
            """
        )
        try:
            with self.db_writer.engine.begin() as conn:
                latest_df = pd.read_sql_query(stmt_latest, conn, params={"ts": target_ts})
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取板块快照时间失败：%s", exc)
            latest_df = pd.DataFrame()

        if not latest_df.empty:
            latest_ts = latest_df.iloc[0].get("ts")

        if latest_ts is None and latest_trade_date:
            try:
                trade_date = dt.datetime.strptime(latest_trade_date, "%Y-%m-%d").date()
            except Exception:
                trade_date = None
            if trade_date is not None:
                stmt_trade_date = text(
                    f"""
                    SELECT MAX(`ts`) AS ts
                    FROM `{table}`
                    WHERE DATE(`ts`) = :trade_date
                    """
                )
                try:
                    with self.db_writer.engine.begin() as conn:
                        trade_df = pd.read_sql_query(
                            stmt_trade_date, conn, params={"trade_date": trade_date}
                        )
                    if not trade_df.empty:
                        latest_ts = trade_df.iloc[0].get("ts")
                except Exception as exc:  # noqa: BLE001
                    self.logger.debug("按交易日读取板块快照失败：%s", exc)

        if latest_ts is None:
            return pd.DataFrame()

        stmt_fetch = text(
            f"""
            SELECT * FROM `{table}`
            WHERE `ts` = :ts
            """
        )
        try:
            with self.db_writer.engine.begin() as conn:
                df = pd.read_sql_query(stmt_fetch, conn, params={"ts": latest_ts})
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取板块快照明细失败：%s", exc)
            return pd.DataFrame()

        if df.empty:
            return pd.DataFrame()

        rename_map = {}
        for col in df.columns:
            if "代码" in col and "board_code" not in rename_map:
                rename_map[col] = "board_code"
            if "名称" in col and "board_name" not in rename_map and "板块" in col:
                rename_map[col] = "board_name"
            if col in {"涨跌幅", "涨跌幅(%)", "chg_pct", "change_rate", "pct_chg"}:
                rename_map[col] = "chg_pct"
            if "排名" in col and "rank" not in rename_map:
                rename_map[col] = "rank"

        df = df.rename(columns=rename_map)
        if "board_code" in df.columns:
            df["board_code"] = df["board_code"].astype(str)
        if "chg_pct" in df.columns:
            if df["chg_pct"].dtype == object:
                df["chg_pct"] = (
                    df["chg_pct"]
                    .astype(str)
                    .str.replace("%", "", regex=False)
                    .str.replace("％", "", regex=False)
                    .str.strip()
                )
            df["chg_pct"] = pd.to_numeric(df["chg_pct"], errors="coerce")
        if "rank" in df.columns:
            df["rank"] = pd.to_numeric(df["rank"], errors="coerce")

        if "rank" not in df.columns or df["rank"].isna().all():
            if "chg_pct" in df.columns:
                df = df.sort_values(by="chg_pct", ascending=False)
            df["rank"] = range(1, len(df) + 1)
        return df

    def load_board_spot_strength(
        self, latest_trade_date: str | None = None, checked_at: dt.datetime | None = None
    ) -> pd.DataFrame:
        """读取板块强弱，优先数据库失败再实时。"""

        db_df = self.load_board_spot_strength_from_db(latest_trade_date, checked_at)
        if not db_df.empty:
            return db_df

        if not self.board_env_enabled or not self.board_spot_enabled:
            return pd.DataFrame()

        try:
            import akshare as ak  # type: ignore
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("AkShare 不可用，无法获取板块强弱：%s", exc)
            return pd.DataFrame()

        try:
            board_df = ak.stock_board_industry_spot_em()
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("获取板块强弱失败：%s", exc)
            return pd.DataFrame()

        if board_df is None or getattr(board_df, "empty", True):
            return pd.DataFrame()

        rename_map = {"板块名称": "board_name", "板块代码": "board_code", "涨跌幅": "chg_pct"}
        for key in list(rename_map.keys()):
            if key not in board_df.columns:
                rename_map.pop(key, None)

        board_df = board_df.rename(columns=rename_map)
        if "chg_pct" in board_df.columns:
            board_df = board_df.sort_values(by="chg_pct", ascending=False)
            board_df["rank"] = range(1, len(board_df) + 1)
        return board_df

    def load_index_weekly_channel(self, latest_trade_date: str) -> dict[str, Any]:
        """加载指数周线通道情景（从指数日线聚合为周线计算）。"""

        if not self.index_codes or not self._table_exists("history_index_daily_kline"):
            return {"state": None, "position_hint": None, "detail": {}, "primary_code": None}

        week_end_asof, current_week_closed = self.resolve_latest_closed_week_end(
            latest_trade_date
        )

        start_date = None
        try:
            end_dt = dt.datetime.strptime(week_end_asof, "%Y-%m-%d").date()
            start_date = (end_dt - dt.timedelta(days=900)).isoformat()
        except Exception:  # noqa: BLE001
            start_date = None

        stmt = text(
            f"""
            SELECT `code`, `date`, `open`, `high`, `low`, `close`, `volume`, `amount`
            FROM history_index_daily_kline
            WHERE `code` IN :codes AND `date` <= :d
            {'AND `date` >= :start_date' if start_date is not None else ''}
            ORDER BY `code`, `date`
            """
        ).bindparams(bindparam("codes", expanding=True))
        try:
            with self.db_writer.engine.begin() as conn:
                params = {"codes": self.index_codes, "d": week_end_asof}
                if start_date is not None:
                    params["start_date"] = start_date
                df = pd.read_sql_query(stmt, conn, params=params)
        except Exception as exc:  # noqa: BLE001
            self.logger.debug("读取指数日线用于周线通道失败：%s", exc)
            return {"state": None, "position_hint": None, "detail": {}, "primary_code": None}

        if df.empty:
            return {"state": None, "position_hint": None, "detail": {}, "primary_code": None}

        result = self.weekly_channel.classify(df)
        payload = result.to_payload()
        payload["weekly_asof_trade_date"] = week_end_asof
        payload["weekly_current_week_closed"] = current_week_closed
        payload["weekly_asof_week_closed"] = True

        weekly_bars_by_code = {}
        if isinstance(result.context, dict):
            raw_weekly_bars = result.context.get("weekly_bars_by_code")
            if isinstance(raw_weekly_bars, dict):
                weekly_bars_by_code = {
                    str(code): bars
                    for code, bars in raw_weekly_bars.items()
                    if isinstance(bars, list)
                }

        merged_weekly_windows: list[dict[str, Any]] = []
        for code, bars in weekly_bars_by_code.items():
            for item in bars:
                if isinstance(item, dict):
                    merged_weekly_windows.append({"code": code, **item})

        payload["weekly_windows_by_code"] = weekly_bars_by_code
        payload["weekly_windows_merged"] = merged_weekly_windows
        return payload

    @staticmethod
    def _clip(text_val: str | None, limit: int = 255) -> str | None:
        if text_val is None:
            return None
        normalized = " ".join(str(text_val).split())
        return normalized[:limit]

    def build_weekly_scenario(
        self, weekly_payload: dict[str, Any], index_trend: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        scenario: dict[str, Any] = {
            "weekly_asof_trade_date": None,
            "weekly_week_closed": False,
            "weekly_current_week_closed": False,
            "weekly_gating_enabled": False,
            "weekly_structure_tags": [],
            "weekly_confirm_tags": [],
            "weekly_money_tags": [],
            "weekly_risk_score": None,
            "weekly_risk_level": "UNKNOWN",
            "weekly_confirm": False,
            "weekly_direction_confirmed": False,
            "weekly_key_levels": {},
            "weekly_money_proxy": {},
            "weekly_phase": None,
            "weekly_plan_a": None,
            "weekly_plan_b": None,
            "weekly_scene_code": None,
            "weekly_bias": "NEUTRAL",
            "weekly_status": "FORMING",
            "weekly_structure_status": "FORMING",
            "weekly_pattern_status": None,
            "weekly_key_levels_str": None,
            "weekly_plan_a_if": None,
            "weekly_plan_a_then": None,
            "weekly_plan_a_confirm": None,
            "weekly_plan_a_exposure_cap": None,
            "weekly_plan_b_if": None,
            "weekly_plan_b_then": None,
            "weekly_plan_b_recover_if": None,
            "weekly_plan_json": None,
            "weekly_note": None,
        }

        if not isinstance(weekly_payload, dict):
            scenario["weekly_plan_a"] = "周线数据缺失，轻仓观望"
            scenario["weekly_plan_b"] = "周线数据缺失，轻仓观望"
            return scenario

        plan = self.weekly_plan_system.build(weekly_payload, index_trend or {})

        scenario.update(plan)
        scenario["weekly_asof_trade_date"] = plan.get("weekly_asof_trade_date")
        scenario["weekly_week_closed"] = plan.get("weekly_week_closed", False)
        scenario["weekly_current_week_closed"] = bool(
            plan.get("weekly_current_week_closed", False)
        )
        scenario["weekly_gating_enabled"] = bool(plan.get("weekly_gating_enabled", False))
        scenario["weekly_risk_score"] = to_float(plan.get("weekly_risk_score"))
        scenario["weekly_risk_level"] = plan.get("weekly_risk_level") or "UNKNOWN"
        scenario["weekly_confirm"] = plan.get("weekly_confirm")
        scenario["weekly_direction_confirmed"] = bool(
            plan.get("weekly_direction_confirmed", False)
        )
        scenario["weekly_phase"] = plan.get("weekly_phase")
        scenario["weekly_key_levels"] = plan.get("weekly_key_levels", {})
        scenario["weekly_key_levels_str"] = self._clip(plan.get("weekly_key_levels_str"), 255)
        scenario["weekly_plan_a"] = self._clip(plan.get("weekly_plan_a"), 255)
        scenario["weekly_plan_b"] = self._clip(plan.get("weekly_plan_b"), 255)
        scenario["weekly_plan_a_if"] = self._clip(plan.get("weekly_plan_a_if"), 255)
        scenario["weekly_plan_a_then"] = self._clip(plan.get("weekly_plan_a_then"), 64)
        scenario["weekly_plan_a_confirm"] = self._clip(plan.get("weekly_plan_a_confirm"), 128)
        scenario["weekly_plan_a_exposure_cap"] = to_float(plan.get("weekly_plan_a_exposure_cap"))
        scenario["weekly_plan_b_if"] = self._clip(plan.get("weekly_plan_b_if"), 255)
        scenario["weekly_plan_b_then"] = self._clip(plan.get("weekly_plan_b_then"), 64)
        scenario["weekly_plan_b_recover_if"] = self._clip(plan.get("weekly_plan_b_recover_if"), 128)
        scenario["weekly_plan_json"] = self._clip(plan.get("weekly_plan_json"), 2000)
        scenario["weekly_structure_status"] = (
            plan.get("weekly_structure_status") or plan.get("weekly_status")
        )
        scenario["weekly_pattern_status"] = plan.get("weekly_pattern_status")
        if not scenario.get("weekly_week_closed", True):
            scenario["weekly_note"] = "本周未收盘，等待区间破位/突破（周收盘有效）"

        tags: list[str] = []
        for key in ["weekly_structure_tags", "weekly_confirm_tags"]:
            vals = plan.get(key)
            if isinstance(vals, list):
                tags.extend([str(v) for v in vals if str(v)])
        if plan.get("weekly_bias"):
            tags.append(f"BIAS_{plan['weekly_bias']}")
        if plan.get("weekly_structure_status"):
            tags.append(f"STATUS_{plan['weekly_structure_status']}")
        scenario["weekly_structure_tags"] = plan.get("weekly_structure_tags", [])
        scenario["weekly_confirm_tags"] = plan.get("weekly_confirm_tags", [])
        scenario["weekly_tags"] = ";".join(tags)[:255] if tags else None

        return scenario

    def build_environment_context(
        self, latest_trade_date: str, *, checked_at: dt.datetime | None = None
    ) -> dict[str, Any]:
        index_trend = self.load_index_trend(latest_trade_date)
        weekly_channel = self.load_index_weekly_channel(latest_trade_date)
        weekly_scenario = self.build_weekly_scenario(weekly_channel, index_trend)
        board_strength = self.load_board_spot_strength(latest_trade_date, checked_at)
        board_map: dict[str, Any] = {}
        if not board_strength.empty and "board_name" in board_strength.columns:
            total = len(board_strength)
            for _, row in board_strength.iterrows():
                name = str(row.get("board_name") or "").strip()
                code = str(row.get("board_code") or "").strip()
                rank = row.get("rank")
                pct = row.get("chg_pct")
                status = "neutral"
                if total > 0 and rank:
                    if rank <= max(1, int(total * 0.2)):
                        status = "strong"
                    elif rank >= max(1, int(total * 0.8)):
                        status = "weak"
                payload = {"rank": rank, "chg_pct": pct, "status": status}
                for key in [name, code]:
                    key_norm = str(key).strip()
                    if key_norm:
                        board_map[key_norm] = payload

        position_hint_raw = None
        weekly_cap = None
        if isinstance(index_trend, dict):
            position_hint_raw = to_float(index_trend.get("position_hint"))
        if isinstance(weekly_scenario, dict):
            weekly_cap = to_float(weekly_scenario.get("weekly_plan_a_exposure_cap"))
        gating_enabled = bool(weekly_scenario.get("weekly_gating_enabled", False))

        effective_position_hint = position_hint_raw
        if gating_enabled and weekly_cap is not None:
            effective_position_hint = (
                weekly_cap
                if position_hint_raw is None
                else min(position_hint_raw, weekly_cap)
            )

        weekly_note = None
        if isinstance(weekly_scenario, dict):
            weekly_note = weekly_scenario.get("weekly_note")
        if weekly_note is None and isinstance(weekly_channel, dict):
            weekly_note = weekly_channel.get("note")

        env_context = {
            "index": index_trend,
            "weekly": weekly_channel,
            "weekly_windows": weekly_channel.get("weekly_windows_merged"),
            "weekly_windows_by_code": weekly_channel.get("weekly_windows_by_code"),
            "boards": board_map,
            "regime": index_trend.get("regime"),
            "index_score": to_float(index_trend.get("score")),
            "position_hint": effective_position_hint,
            "position_hint_raw": position_hint_raw,
            "effective_position_hint": effective_position_hint,
            "weekly_state": weekly_channel.get("state") if isinstance(weekly_channel, dict) else None,
            "weekly_position_hint": weekly_channel.get("position_hint") if isinstance(weekly_channel, dict) else None,
            "weekly_note": weekly_note,
            "weekly_scenario": weekly_scenario,
            "weekly_asof_trade_date": weekly_scenario.get("weekly_asof_trade_date"),
            "weekly_week_closed": weekly_scenario.get("weekly_week_closed"),
            "weekly_current_week_closed": weekly_scenario.get("weekly_current_week_closed"),
            "weekly_risk_score": weekly_scenario.get("weekly_risk_score"),
            "weekly_risk_level": weekly_scenario.get("weekly_risk_level"),
            "weekly_confirm": weekly_scenario.get("weekly_confirm"),
            "weekly_gating_enabled": gating_enabled,
            "weekly_plan_a": weekly_scenario.get("weekly_plan_a"),
            "weekly_plan_b": weekly_scenario.get("weekly_plan_b"),
            "weekly_scene_code": weekly_scenario.get("weekly_scene_code"),
            "weekly_phase": weekly_scenario.get("weekly_phase"),
            "weekly_key_levels_str": weekly_scenario.get("weekly_key_levels_str"),
            "weekly_plan_a_if": weekly_scenario.get("weekly_plan_a_if"),
            "weekly_plan_a_then": weekly_scenario.get("weekly_plan_a_then"),
            "weekly_plan_a_confirm": weekly_scenario.get("weekly_plan_a_confirm"),
            "weekly_plan_a_exposure_cap": weekly_scenario.get("weekly_plan_a_exposure_cap"),
            "weekly_plan_b_if": weekly_scenario.get("weekly_plan_b_if"),
            "weekly_plan_b_then": weekly_scenario.get("weekly_plan_b_then"),
            "weekly_plan_b_recover_if": weekly_scenario.get("weekly_plan_b_recover_if"),
            "weekly_plan_json": weekly_scenario.get("weekly_plan_json"),
            "weekly_bias": weekly_scenario.get("weekly_bias"),
            "weekly_status": weekly_scenario.get("weekly_structure_status")
            or weekly_scenario.get("weekly_status"),
            "weekly_structure_status": weekly_scenario.get("weekly_structure_status"),
            "weekly_pattern_status": weekly_scenario.get("weekly_pattern_status"),
            "weekly_direction_confirmed": weekly_scenario.get(
                "weekly_direction_confirmed"
            ),
            "weekly_money_tags": weekly_scenario.get("weekly_money_tags"),
        }

        money_proxy = weekly_scenario.get("weekly_money_proxy") if isinstance(weekly_scenario, dict) else {}
        proxy_parts: list[str] = []
        if isinstance(money_proxy, dict):
            vol_ratio = money_proxy.get("vol_ratio_20")
            slope_delta = money_proxy.get("slope_change_4w")
            obv_slope = money_proxy.get("obv_slope_13")
            if vol_ratio is not None:
                proxy_parts.append(f"vol_ratio_20={vol_ratio:.2f}")
            if slope_delta is not None:
                proxy_parts.append(f"slope_chg_4w={slope_delta:.4f}")
            if obv_slope is not None:
                proxy_parts.append(f"obv_slope_13={obv_slope:.2f}")
        env_context["weekly_money_proxy"] = ";".join(proxy_parts)[:255] if proxy_parts else None

        scenario_tags: list[str] = []
        if isinstance(weekly_scenario, dict):
            for key in ["weekly_structure_tags", "weekly_confirm_tags"]:
                tags = weekly_scenario.get(key)
                if isinstance(tags, list):
                    scenario_tags.extend([str(t) for t in tags if str(t)])
            if weekly_scenario.get("weekly_bias"):
                scenario_tags.append(f"BIAS_{weekly_scenario['weekly_bias']}")
            if weekly_scenario.get("weekly_structure_status"):
                scenario_tags.append(f"STATUS_{weekly_scenario['weekly_structure_status']}")
            if weekly_scenario.get("weekly_tags") and not scenario_tags:
                scenario_tags.extend(str(weekly_scenario.get("weekly_tags")).split(";"))
        env_context["weekly_tags"] = ";".join(scenario_tags)[:255] if scenario_tags else None

        for key in [
            "below_ma250_streak",
            "break_confirmed",
            "reclaim_confirmed",
            "effective_breakdown_days",
            "effective_reclaim_days",
            "yearline_state",
            "regime_note",
        ]:
            if isinstance(index_trend, dict) and key in index_trend:
                env_context[key] = index_trend[key]

        weekly_gate_policy = self.resolve_env_weekly_gate_policy(env_context)
        env_context["weekly_gate_policy"] = weekly_gate_policy
        weekly_zone = self._resolve_weekly_zone(
            weekly_scenario,
            weekly_gate_policy,
            env_context.get("weekly_tags"),
        )
        weekly_scenario.update(weekly_zone)
        env_context.update(weekly_zone)
        self._finalize_env_directives(env_context, weekly_gate_policy=weekly_gate_policy)

        return env_context

    @staticmethod
    def resolve_env_weekly_gate_policy(env_context: dict[str, Any] | None) -> str | None:
        if not env_context:
            return None

        weekly_scenario = (
            env_context.get("weekly_scenario") if isinstance(env_context, dict) else {}
        )
        if not isinstance(weekly_scenario, dict):
            weekly_scenario = {}

        existing_policy = None
        if isinstance(env_context, dict):
            existing_policy = env_context.get("weekly_gate_policy")
            if existing_policy:
                return str(existing_policy)

        def _get_env(key: str) -> Any:  # noqa: ANN401
            if isinstance(env_context, dict):
                value = env_context.get(key, None)
                if value not in (None, "", [], {}):
                    return value
            return weekly_scenario.get(key)

        gating_enabled = bool(_get_env("weekly_gating_enabled"))
        risk_level = str(_get_env("weekly_risk_level") or "").upper()
        status = str(_get_env("weekly_status") or "").upper()
        structure_status = str(_get_env("weekly_structure_status") or status).upper()
        weekly_phase = str(_get_env("weekly_phase") or "").upper()
        weekly_tags = str(_get_env("weekly_tags") or "")
        weekly_risk_score = to_float(_get_env("weekly_risk_score"))

        if not gating_enabled:
            return "ALLOW"

        baseline_gate = "ALLOW"
        if weekly_phase == "BREAKDOWN_RISK":
            baseline_gate = "WAIT"
        elif risk_level == "HIGH":
            if weekly_risk_score is not None and weekly_risk_score >= 85:
                baseline_gate = "WAIT"
            elif (
                weekly_risk_score is not None
                and 70 <= weekly_risk_score < 85
                and weekly_phase == "BULL_TREND"
                and structure_status == "FORMING"
            ):
                baseline_gate = "ALLOW_SMALL"
            else:
                baseline_gate = "WAIT"
        elif risk_level == "MEDIUM" and structure_status == "FORMING":
            # feat: MEDIUM 风险周线不一票 WAIT，改为 ALLOW_SMALL 以降仓放行
            baseline_gate = "ALLOW_SMALL"

        def _tighten(current: str, target: str) -> str:
            severity = {"ALLOW": 0, "ALLOW_SMALL": 1, "WAIT": 2, "STOP": 3}
            if severity.get(target, 0) > severity.get(current, 0):
                return target
            return current

        if weekly_phase == "BEAR_TREND" and risk_level in {"MEDIUM", "HIGH"}:
            baseline_gate = _tighten(baseline_gate, "ALLOW_SMALL")

        if "VOL_WEAK" in weekly_tags and risk_level != "LOW":
            baseline_gate = _tighten(baseline_gate, "ALLOW_SMALL")

        return baseline_gate

    @staticmethod
    def _merge_gate_actions(*actions: str | None) -> str | None:
        severity = {"STOP": 3, "WAIT": 2, "ALLOW_SMALL": 1, "ALLOW": 0, None: -1}
        normalized = []
        for action in actions:
            if action is None:
                normalized.append((severity[None], None))
                continue
            action_norm = str(action).strip().upper()
            if action_norm == "GO":
                action_norm = "ALLOW"
            normalized.append((severity.get(action_norm, 0), action_norm))
        if not normalized:
            return None
        normalized.sort(key=lambda x: x[0], reverse=True)
        return normalized[0][1]

    @staticmethod
    def _derive_gate_action(regime: str | None, position_hint: float | None) -> str | None:
        regime_norm = str(regime or "").strip().upper() or None
        pos_hint_val = to_float(position_hint)
        if regime_norm in {"BREAKDOWN", "BEAR_CONFIRMED"}:
            return "STOP"
        if regime_norm == "RISK_OFF":
            return "WAIT"
        if regime_norm == "PULLBACK":
            if pos_hint_val is not None and pos_hint_val <= 0.3:
                return "WAIT"
            return "ALLOW"
        if regime_norm == "RISK_ON":
            return "ALLOW"
        if pos_hint_val is not None:
            if pos_hint_val <= 0:
                return "STOP"
            if pos_hint_val < 0.3:
                return "WAIT"
            return "ALLOW"
        return None

    @staticmethod
    def _resolve_risk_emotion_policy(env_context: dict[str, Any]) -> dict[str, Any]:
        risk_level = str(env_context.get("weekly_risk_level") or "").upper()
        regime = str(env_context.get("regime") or "").upper()

        # Risk (weekly) x Emotion (daily regime) matrix.
        matrix = {
            "HIGH": {
                "RISK_ON": ("WAIT", 0.15, "HIGH_RISK_RISK_ON"),
                "PULLBACK": ("WAIT", 0.15, "HIGH_RISK_PULLBACK"),
                "RISK_OFF": ("STOP", 0.0, "HIGH_RISK_RISK_OFF"),
                "BREAKDOWN": ("STOP", 0.0, "HIGH_RISK_BREAKDOWN"),
                "BEAR_CONFIRMED": ("STOP", 0.0, "HIGH_RISK_BEAR"),
            },
            "MEDIUM": {
                "RISK_ON": ("ALLOW_SMALL", 0.3, "MEDIUM_RISK_RISK_ON"),
                "PULLBACK": ("ALLOW_SMALL", 0.25, "MEDIUM_RISK_PULLBACK"),
                "RISK_OFF": ("WAIT", 0.15, "MEDIUM_RISK_RISK_OFF"),
                "BREAKDOWN": ("WAIT", 0.15, "MEDIUM_RISK_BREAKDOWN"),
            },
            "LOW": {
                "RISK_ON": ("ALLOW", 1.0, "LOW_RISK_RISK_ON"),
                "PULLBACK": ("ALLOW_SMALL", 0.5, "LOW_RISK_PULLBACK"),
                "RISK_OFF": ("WAIT", 0.25, "LOW_RISK_RISK_OFF"),
            },
        }

        if not risk_level or not regime:
            return {}

        row = matrix.get(risk_level, {})
        if regime not in row:
            return {}

        gate_action, cap_limit, tag = row[regime]
        return {"gate_action": gate_action, "cap_limit": cap_limit, "matrix_tag": tag}

    @staticmethod
    def _resolve_weekly_zone(
        weekly_scenario: dict[str, Any] | None,
        weekly_gate_policy: str | None,
        weekly_tags: str | None,
    ) -> dict[str, Any]:
        scenario = weekly_scenario or {}
        risk_level = str(scenario.get("weekly_risk_level") or "").upper()
        gate_policy = str(weekly_gate_policy or scenario.get("weekly_gate_policy") or "").upper()
        plan_cap = to_float(scenario.get("weekly_plan_a_exposure_cap"))
        direction_confirmed = bool(scenario.get("weekly_direction_confirmed", False))
        tags = str(weekly_tags or scenario.get("weekly_tags") or "")

        zone_id = "WZ2_NEUTRAL"
        reason_parts: list[str] = []

        if gate_policy in {"STOP", "WAIT"} or risk_level == "HIGH":
            zone_id = "WZ0_RISK_OFF"
        elif "OVERHEAT" in tags or "EUPHORIA" in tags:
            zone_id = "WZ4_EUPHORIA"
        elif risk_level == "MEDIUM" and plan_cap is not None and plan_cap <= 0.25:
            zone_id = "WZ1_DEFENSIVE"
        elif risk_level == "LOW" and direction_confirmed:
            zone_id = "WZ3_ATTACK"

        zone_score_map = {
            "WZ0_RISK_OFF": 10,
            "WZ1_DEFENSIVE": 30,
            "WZ2_NEUTRAL": 50,
            "WZ3_ATTACK": 70,
            "WZ4_EUPHORIA": 85,
        }
        exp_return_map = {
            "WZ0_RISK_OFF": "LOW",
            "WZ1_DEFENSIVE": "LOW",
            "WZ2_NEUTRAL": "MID",
            "WZ3_ATTACK": "HIGH",
            "WZ4_EUPHORIA": "HIGH",
        }

        if gate_policy:
            reason_parts.append(f"gate={gate_policy}")
        if risk_level:
            reason_parts.append(f"risk={risk_level}")
        if plan_cap is not None:
            reason_parts.append(f"cap={plan_cap:.2f}")
        if direction_confirmed:
            reason_parts.append("direction_confirmed")
        if tags:
            reason_parts.append(f"tags={tags}")

        return {
            "weekly_zone_id": zone_id,
            "weekly_zone_score": zone_score_map.get(zone_id, 50),
            "weekly_exp_return_bucket": exp_return_map.get(zone_id, "MID"),
            "weekly_zone_reason": ";".join(reason_parts)[:255] if reason_parts else None,
        }

    def _finalize_env_directives(
        self,
        env_context: dict[str, Any] | None,
        *,
        weekly_gate_policy: str | None = None,
    ) -> None:
        if not isinstance(env_context, dict):
            return

        risk_emotion = self._resolve_risk_emotion_policy(env_context)

        gate_candidates: list[str | None] = []
        reason_parts: dict[str, Any] = {}
        gate_norm = None
        if weekly_gate_policy:
            gate_norm = str(weekly_gate_policy).strip().upper()
            reason_parts["weekly_gate_policy"] = gate_norm

        index_gate_norm = None
        index_snapshot = env_context.get("index_intraday")
        if isinstance(index_snapshot, dict):
            index_gate = index_snapshot.get("env_index_gate_action")
            if index_gate is not None:
                index_gate_norm = str(index_gate).strip().upper()
                reason_parts["index_gate_action"] = index_gate_norm

        daily_zone_id = env_context.get("daily_zone_id")
        daily_gate_hint = None
        if str(daily_zone_id or "").upper() == "DZ_BREAKDOWN":
            daily_gate_hint = "WAIT"
        if daily_gate_hint:
            gate_candidates.append(daily_gate_hint)
            reason_parts["daily_gate_hint"] = daily_gate_hint

        live_override_action = str(env_context.get("env_live_override_action") or "").upper()
        live_gate_hint = None
        if live_override_action == "PAUSE":
            live_gate_hint = "WAIT"
        elif live_override_action == "EXIT":
            live_gate_hint = "STOP"

        regime_gate = self._derive_gate_action(
            env_context.get("regime"), env_context.get("position_hint_raw") or env_context.get("position_hint")
        )
        if regime_gate:
            reason_parts["regime_gate_action"] = regime_gate

        if risk_emotion:
            matrix_gate = risk_emotion.get("gate_action")
            if matrix_gate:
                gate_candidates.append(matrix_gate)
                reason_parts["risk_emotion_gate_action"] = matrix_gate
            matrix_cap = risk_emotion.get("cap_limit")
            if matrix_cap is not None:
                reason_parts["risk_emotion_cap_limit"] = f"{matrix_cap:.2f}"
            matrix_tag = risk_emotion.get("matrix_tag")
            if matrix_tag:
                reason_parts["risk_emotion_matrix"] = matrix_tag

        effective_weekly_gate = gate_norm
        live_unlock_gate = str(env_context.get("env_live_unlock_gate") or "").strip().upper()
        if (
            gate_norm == "WAIT"
            and live_unlock_gate == "ALLOW_SMALL"
            and live_override_action not in {"PAUSE", "EXIT"}
            and index_gate_norm not in {"WAIT", "STOP"}
            and regime_gate not in {"WAIT", "STOP"}
        ):
            effective_weekly_gate = "ALLOW_SMALL"
            reason_parts["weekly_gate_effective"] = effective_weekly_gate
            reason_parts["env_live_unlock_gate"] = live_unlock_gate

        if effective_weekly_gate:
            gate_candidates.append(effective_weekly_gate)
        if index_gate_norm:
            gate_candidates.append(index_gate_norm)
        if live_gate_hint:
            gate_candidates.append(live_gate_hint)
            reason_parts["live_gate_hint"] = live_gate_hint
        if regime_gate:
            gate_candidates.append(regime_gate)

        for key in [
            "weekly_asof_trade_date",
            "weekly_risk_score",
            "weekly_scene_code",
            "weekly_structure_status",
            "weekly_pattern_status",
            "daily_asof_trade_date",
            "weekly_zone_id",
            "daily_zone_id",
            "env_live_override_action",
            "env_live_cap_multiplier",
            "env_live_event_tags",
            "env_live_reason",
            "env_live_unlock_gate",
        ]:
            value = env_context.get(key)
            if value not in (None, "", [], {}, ()):  # noqa: PLC1901
                reason_parts[key] = value

        final_gate = self._merge_gate_actions(*gate_candidates) or "ALLOW"

        weekly_cap = to_float(env_context.get("weekly_plan_a_exposure_cap"))
        daily_pos_hint = to_float(env_context.get("position_hint"))
        daily_cap_multiplier = to_float(env_context.get("daily_cap_multiplier")) or 1.0
        live_cap_multiplier = to_float(env_context.get("env_live_cap_multiplier")) or 1.0
        breadth_pct = to_float(env_context.get("breadth_pct_above_ma20"))
        breadth_factor = (
            0.6 + 0.4 * breadth_pct if breadth_pct is not None else 1.0
        )
        if breadth_pct is not None:
            if breadth_pct >= 0.95:
                breadth_factor = min(breadth_factor, 0.85)
                reason_parts["breadth_saturation"] = "RISK_ON_PEAK"
            elif breadth_pct <= 0.05:
                breadth_factor = min(breadth_factor, 0.75)
                reason_parts["breadth_saturation"] = "RISK_OFF_WASHOUT"

        cap_candidates = [weekly_cap, daily_pos_hint]
        filtered_caps = [c for c in cap_candidates if c is not None]
        base_cap = min(filtered_caps) if filtered_caps else 1.0
        final_cap = base_cap * daily_cap_multiplier * breadth_factor * live_cap_multiplier
        final_cap = min(max(final_cap, 0.0), 1.0)
        small_cap_limit = 0.25
        if risk_emotion and risk_emotion.get("cap_limit") is not None:
            cap_limit = float(risk_emotion["cap_limit"])
            if final_cap > cap_limit:
                final_cap = cap_limit
                reason_parts["gate_cap_limit"] = f"RISK_EMOTION_{cap_limit:.2f}"
        weekly_risk_level = str(env_context.get("weekly_risk_level") or "").strip().upper()
        weekly_scene = str(
            env_context.get("weekly_scene_code") or env_context.get("weekly_scene") or ""
        ).strip().upper()
        breadth_saturation = reason_parts.get("breadth_saturation")
        tier_cap = None
        if weekly_risk_level == "HIGH":
            if breadth_saturation == "RISK_ON_PEAK":
                tier_cap = 0.05
            elif weekly_scene.startswith("WEDGE"):
                tier_cap = 0.08
        if tier_cap is not None and final_cap > tier_cap:
            final_cap = tier_cap
            reason_parts["risk_tier_cap"] = f"{weekly_risk_level}_{tier_cap:.2f}"
        if effective_weekly_gate == "ALLOW_SMALL" and gate_norm == "WAIT":
            small_cap_limit = 0.15
            reason_parts["unlock_cap_limit"] = f"{small_cap_limit:.2f}"
        if final_gate in {"STOP", "WAIT"}:
            final_cap = 0.0
            reason_parts["gate_cap_limit"] = f"{final_gate}_0"
        elif final_gate == "ALLOW_SMALL":
            if final_cap > small_cap_limit:
                final_cap = small_cap_limit
                reason_parts["gate_cap_limit"] = f"ALLOW_SMALL_{small_cap_limit:.2f}"
        env_context["env_final_gate_action"] = final_gate
        env_context["env_final_cap_pct"] = final_cap
        env_context["env_final_reason_json"] = self._clip(
            json.dumps(reason_parts, ensure_ascii=False, default=self._json_default),
            2000,
        )

    @property
    def calendar_cache(self) -> set[str]:
        return self._calendar_cache

    @property
    def calendar_range(self) -> tuple[dt.date, dt.date] | None:
        return self._calendar_range

    @staticmethod
    def _json_default(obj: Any) -> str:
        if isinstance(obj, (dt.date, dt.datetime)):
            return obj.isoformat()
        return str(obj)

================================================================================
FILE: ashare/weekly_pattern_system.py
================================================================================

"""周线形态识别与 Plan 生成系统。

基于周线 OHLCV 自动识别箱体/三角/楔形/旗形/双顶/头肩顶等形态，
输出统一的结构化计划字段，供 open_monitor 的周线 gating 使用。

实现原则：
- 仅依赖本地计算，遵循 ChartSchool 等常见规则的简化版本；
- 形态识别靠枢轴点+趋势线回归，避免主观手动画线；
- Plan 输出为机器可读 tokens + 简短中文，便于后续扩展。
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd


def _sma(series: pd.Series, n: int) -> pd.Series:
    n = max(int(n), 1)
    return series.rolling(n, min_periods=n).mean()


def _atr(high: pd.Series, low: pd.Series, close: pd.Series, n: int = 14) -> pd.Series:
    high = pd.to_numeric(high, errors="coerce")
    low = pd.to_numeric(low, errors="coerce")
    close = pd.to_numeric(close, errors="coerce")
    prev_close = close.shift(1)
    tr1 = high - low
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    return tr.rolling(n, min_periods=n).mean()


def _obv(close: pd.Series, volume: pd.Series) -> pd.Series:
    close = pd.to_numeric(close, errors="coerce")
    volume = pd.to_numeric(volume, errors="coerce")
    direction = close.diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)
    obv = (direction * volume).fillna(0).cumsum()
    return obv


def _fit_regression(points: List[Tuple[int, float]]) -> Tuple[float | None, float | None]:
    if len(points) < 2:
        return None, None
    x = np.array([p[0] for p in points], dtype=float)
    y = np.array([p[1] for p in points], dtype=float)
    if np.any(np.isnan(y)):
        return None, None
    slope, intercept = np.polyfit(x, y, 1)
    return float(slope), float(intercept)


def _line_value(slope: float | None, intercept: float | None, x: int) -> float | None:
    if slope is None or intercept is None:
        return None
    return slope * x + intercept


def _clip_text(text: str | None, limit: int) -> str | None:
    if text is None:
        return None
    normalized = " ".join(str(text).split())
    return normalized[:limit]


@dataclass
class PatternCandidate:
    scene: str
    bias: str
    status: str
    score: float
    key_levels: Dict[str, float]
    structure_tags: List[str]
    confirm_tags: List[str]
    money_tags: List[str]
    description: str


class WeeklyPlanSystem:
    """周线形态识别系统。

    输入：weekly_payload.context.primary_weekly_bars（list[dict]）。
    输出：统一 schema 的计划字段，用于周线 gating。
    """

    def __init__(self) -> None:
        self.ma_fast = 30
        self.ma_slow = 60
        self.fractal_left = 2
        self.fractal_right = 2
        self.max_pivots = 8
        self.break_eps = 0.01
        self.confirm_vol_ratio_threshold = 1.05
        self.retest_tolerance = 0.01

    def _near_by_atr(
        self,
        p1: float,
        p2: float,
        *,
        atr: float | None,
        pct_floor: float,
        atr_mult: float,
    ) -> bool:
        if p1 is None or p2 is None:
            return False
        base = max(float(p1), float(p2))
        if base <= 0:
            return False
        tol_abs = base * float(pct_floor)
        if atr is not None and not pd.isna(atr):
            tol_abs = max(tol_abs, float(atr) * float(atr_mult))
        return abs(float(p1) - float(p2)) <= tol_abs

    def _prepare_df(self, bars: List[Dict[str, Any]]) -> pd.DataFrame:
        df = pd.DataFrame(bars)
        if df.empty:
            return df
        for col in ["open", "high", "low", "close", "volume", "amount"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")
        df["week_end"] = pd.to_datetime(df.get("week_end"), errors="coerce")
        df = df.sort_values("week_end").reset_index(drop=True)
        df["ma_fast"] = _sma(df["close"], self.ma_fast)
        df["ma_slow"] = _sma(df["close"], self.ma_slow)
        df["atr14"] = _atr(df["high"], df["low"], df["close"], 14)
        df["vol_ma20"] = _sma(df["volume"], 20)
        df["wk_vol_ratio_20"] = df.apply(
            lambda row: row["volume"] / row["vol_ma20"] if row.get("vol_ma20") else np.nan,
            axis=1,
        )
        df["ret_4w"] = df["close"].pct_change(4)
        df["ret_8w"] = df["close"].pct_change(8)
        df["range_width_pct"] = (df["high"] - df["low"]) / df["close"]
        df["volatility_4w"] = df["close"].pct_change().rolling(4, min_periods=2).std()
        df["obv"] = _obv(df["close"], df["volume"])
        df["obv_slope_13"] = df["obv"].diff(13)
        return df

    def _extract_pivots(self, df: pd.DataFrame) -> Tuple[List[Tuple[int, float]], List[Tuple[int, float]]]:
        highs: List[Tuple[int, float]] = []
        lows: List[Tuple[int, float]] = []
        if df.empty:
            return highs, lows
        values_high = df["high"].to_numpy()
        values_low = df["low"].to_numpy()
        n = len(df)
        for i in range(n):
            left = max(i - self.fractal_left, 0)
            right = min(i + self.fractal_right, n - 1)
            window_high = values_high[left : right + 1]
            window_low = values_low[left : right + 1]
            if np.isnan(values_high[i]) or np.isnan(values_low[i]):
                continue
            if values_high[i] >= np.nanmax(window_high) and i not in {0, n - 1}:
                highs.append((i, float(values_high[i])))
            if values_low[i] <= np.nanmin(window_low) and i not in {0, n - 1}:
                lows.append((i, float(values_low[i])))
        highs = highs[-self.max_pivots :]
        lows = lows[-self.max_pivots :]
        return highs, lows

    def _convergence(self, upper_slope: float | None, lower_slope: float | None, width_start: float | None, width_end: float | None) -> float | None:
        if upper_slope is None or lower_slope is None:
            return None
        if width_start is None or width_start <= 0 or width_end is None or width_end <= 0:
            return None
        return width_end / width_start

    def _detect_flag(self, df: pd.DataFrame) -> PatternCandidate | None:
        if len(df) < 12:
            return None
        recent_close = df["close"].iloc[-1]
        base_idx = max(len(df) - 8, 0)
        base_close = df["close"].iloc[base_idx]
        if base_close == 0 or pd.isna(recent_close) or pd.isna(base_close):
            return None
        impulse = (recent_close - base_close) / base_close
        if abs(impulse) < 0.12:
            return None
        recent_vol = df["volatility_4w"].iloc[-1]
        prev_vol = df["volatility_4w"].iloc[-5] if len(df) >= 6 else np.nan
        consolidating = (pd.notna(recent_vol) and pd.notna(prev_vol) and recent_vol < prev_vol)
        width_mean = df["range_width_pct"].iloc[-4:].mean()
        width_prev = df["range_width_pct"].iloc[-8:-4].mean() if len(df) >= 8 else np.nan
        if pd.notna(width_mean) and pd.notna(width_prev) and width_mean < width_prev * 0.9:
            consolidating = consolidating or True
        if not consolidating:
            return None
        bias = "BULLISH" if impulse > 0 else "BEARISH"
        scene = "FLAG_PENNANT_BULL" if impulse > 0 else "FLAG_PENNANT_BEAR"
        status = "FORMING"
        score = 65.0 + min(abs(impulse) * 100, 15)
        tags = [scene, "CONSOLIDATION"]
        return PatternCandidate(
            scene=scene,
            bias=bias,
            status=status,
            score=score,
            key_levels={},
            structure_tags=tags,
            confirm_tags=[],
            money_tags=[],
            description="冲击后横盘整理，留意突破",  # 简短描述
        )

    def _detect_double(self, highs: List[Tuple[int, float]], lows: List[Tuple[int, float]], atr_last: float | None = None) -> PatternCandidate | None:
        if len(highs) >= 2:
            last_two = highs[-2:]
            if last_two[0][1] and last_two[1][1]:
                if self._near_by_atr(last_two[0][1], last_two[1][1], atr=atr_last, pct_floor=0.02, atr_mult=0.5):
                    neckline = None
                    if lows:
                        neckline = float(np.mean([p[1] for p in lows[-2:]]))
                    return PatternCandidate(
                        scene="DOUBLE_TOP",
                        bias="BEARISH",
                        status="FORMING",
                        score=70.0,
                        key_levels={"neckline": neckline} if neckline else {},
                        structure_tags=["DOUBLE_TOP"],
                        confirm_tags=[],
                        money_tags=[],
                        description="双顶观察，关注颈线跌破",
                    )
        if len(lows) >= 2:
            last_two = lows[-2:]
            if last_two[0][1] and last_two[1][1]:
                if self._near_by_atr(last_two[0][1], last_two[1][1], atr=atr_last, pct_floor=0.02, atr_mult=0.5):
                    neckline = None
                    if highs:
                        neckline = float(np.mean([p[1] for p in highs[-2:]]))
                    return PatternCandidate(
                        scene="DOUBLE_BOTTOM",
                        bias="BULLISH",
                        status="FORMING",
                        score=70.0,
                        key_levels={"neckline": neckline} if neckline else {},
                        structure_tags=["DOUBLE_BOTTOM"],
                        confirm_tags=[],
                        money_tags=[],
                        description="双底观察，关注颈线突破",
                    )
        return None

    def _detect_hs(self, highs: List[Tuple[int, float]], lows: List[Tuple[int, float]], atr_last: float | None = None) -> PatternCandidate | None:
        if len(highs) < 3 or len(lows) < 2:
            return None
        last_highs = highs[-3:]
        shoulder1, head, shoulder2 = last_highs[0][1], last_highs[1][1], last_highs[2][1]
        if head <= max(shoulder1, shoulder2):
            return None
        shoulders_close = self._near_by_atr(shoulder1, shoulder2, atr=atr_last, pct_floor=0.05, atr_mult=0.8)
        if not shoulders_close:
            return None
        neckline = float(np.mean([p[1] for p in lows[-2:]]))
        return PatternCandidate(
            scene="HS_TOP",
            bias="BEARISH",
            status="FORMING",
            score=78.0,
            key_levels={"neckline": neckline},
            structure_tags=["HS_TOP"],
            confirm_tags=[],
            money_tags=[],
            description="头肩顶雏形，关注颈线跌破",
        )

    def _detect_channel_triangle_wedge(
        self,
        df: pd.DataFrame,
        highs: List[Tuple[int, float]],
        lows: List[Tuple[int, float]],
    ) -> PatternCandidate:
        idx_last = len(df) - 1
        log_highs = [(i, float(np.log(p))) for i, p in highs if p and p > 0]
        log_lows = [(i, float(np.log(p))) for i, p in lows if p and p > 0]
        upper_slope, upper_intercept = _fit_regression(log_highs)
        lower_slope, lower_intercept = _fit_regression(log_lows)
        upper_last_log = _line_value(upper_slope, upper_intercept, idx_last)
        lower_last_log = _line_value(lower_slope, lower_intercept, idx_last)
        upper_last = float(np.exp(upper_last_log)) if upper_last_log is not None else None
        lower_last = float(np.exp(lower_last_log)) if lower_last_log is not None else None
        width_end = upper_last - lower_last if upper_last is not None and lower_last is not None else None
        width_start = None
        if log_highs and log_lows:
            min_idx = min(log_highs[0][0], log_lows[0][0])
            upper_start_log = _line_value(upper_slope, upper_intercept, min_idx)
            lower_start_log = _line_value(lower_slope, lower_intercept, min_idx)
            if upper_start_log is not None and lower_start_log is not None:
                upper_start = float(np.exp(upper_start_log))
                lower_start = float(np.exp(lower_start_log))
                width_start = upper_start - lower_start
        convergence = self._convergence(upper_slope, lower_slope, width_start, width_end)

        slope_diff = None
        if upper_slope is not None and lower_slope is not None:
            slope_diff = abs(upper_slope - lower_slope)

        scene = "CHANNEL"
        bias = "NEUTRAL"
        status = "FORMING"
        tags: List[str] = []
        score = 55.0

        flat_threshold_pct = 0.0005
        parallel_threshold_pct = 0.0005
        near_parallel = slope_diff is not None and slope_diff < parallel_threshold_pct
        upper_flat = upper_slope is not None and abs(upper_slope) < flat_threshold_pct
        lower_flat = lower_slope is not None and abs(lower_slope) < flat_threshold_pct
        converging = convergence is not None and convergence < 1.0

        if near_parallel:
            if upper_slope is not None and upper_slope > 0 and lower_slope is not None and lower_slope > 0:
                scene = "CHANNEL_UP"
                bias = "BULLISH"
                tags.append("PARALLEL")
                score = 60.0
            elif upper_slope is not None and upper_slope < 0 and lower_slope is not None and lower_slope < 0:
                scene = "CHANNEL_DOWN"
                bias = "BEARISH"
                tags.append("PARALLEL")
                score = 60.0
        if upper_flat and lower_flat:
            scene = "RECTANGLE"
            bias = "NEUTRAL"
            score = 65.0
            tags.append("RANGE")
        elif upper_flat and lower_slope is not None and lower_slope > 0 and converging:
            scene = "TRIANGLE_ASC"
            bias = "NEUTRAL"
            score = 70.0
            tags.append("ASC_TRIANGLE")
        elif lower_flat and upper_slope is not None and upper_slope < 0 and converging:
            scene = "TRIANGLE_DESC"
            bias = "NEUTRAL"
            score = 70.0
            tags.append("DESC_TRIANGLE")
        elif (
            upper_slope is not None
            and lower_slope is not None
            and upper_slope < 0
            and lower_slope > 0
            and converging
        ):
            scene = "TRIANGLE_SYMM"
            bias = "NEUTRAL"
            score = 72.0
            tags.append("SYMM_TRIANGLE")
        elif (
            upper_slope is not None
            and lower_slope is not None
            and ((upper_slope > 0 and lower_slope > 0) or (upper_slope < 0 and lower_slope < 0))
            and converging
        ):
            scene = "WEDGE_RISING" if upper_slope > 0 else "WEDGE_FALLING"
            bias = "BEARISH" if scene == "WEDGE_RISING" else "BULLISH"
            score = 75.0
            tags.append("WEDGE")

        key_levels = {}
        if upper_last is not None:
            key_levels["upper"] = upper_last
        if lower_last is not None:
            key_levels["lower"] = lower_last
        return PatternCandidate(
            scene=scene,
            bias=bias,
            status=status,
            score=score,
            key_levels=key_levels,
            structure_tags=[scene] + tags,
            confirm_tags=[],
            money_tags=[],
            description="形态识别完成，等待突破",
        )

    def _detect_pattern(
        self, df: pd.DataFrame, weekly_closed: bool, slope_change: float | None = None
    ) -> PatternCandidate:
        highs, lows = self._extract_pivots(df)
        atr_last = None
        if "atr14" in df.columns and not df.empty:
            val = df["atr14"].iloc[-1]
            if pd.notna(val):
                atr_last = float(val)
        base_candidate = self._detect_channel_triangle_wedge(df, highs, lows)
        candidates: List[PatternCandidate] = [base_candidate]
        flag_candidate = self._detect_flag(df)
        if flag_candidate:
            candidates.append(flag_candidate)
        double_candidate = self._detect_double(highs, lows, atr_last)
        if double_candidate:
            candidates.append(double_candidate)
        hs_candidate = self._detect_hs(highs, lows, atr_last)
        if hs_candidate:
            candidates.append(hs_candidate)
        candidates = sorted(candidates, key=lambda c: c.score, reverse=True)
        top = candidates[0]
        return self._apply_breakout_confirm(df, top, weekly_closed, slope_change)

    def _apply_breakout_confirm(
        self,
        df: pd.DataFrame,
        candidate: PatternCandidate,
        weekly_closed: bool,
        slope_change: float | None = None,
    ) -> PatternCandidate:
        last_idx = len(df) - 1
        last_close = float(df["close"].iloc[-1]) if not df.empty else None
        wk_vol_ratio = df["wk_vol_ratio_20"].iloc[-1] if not df.empty else np.nan
        confirm_tags = list(candidate.confirm_tags)
        money_tags = list(candidate.money_tags)
        structure_tags = list(candidate.structure_tags)
        status = candidate.status
        scene = candidate.scene
        key_levels = dict(candidate.key_levels)

        upper = key_levels.get("upper")
        lower = key_levels.get("lower")
        neckline = key_levels.get("neckline")

        breakout_up = False
        breakout_down = False
        if last_close is not None:
            if upper is not None and last_close > upper * (1 + self.break_eps):
                breakout_up = True
            if lower is not None and last_close < lower * (1 - self.break_eps):
                breakout_down = True
            if neckline is not None:
                if scene in {"DOUBLE_BOTTOM", "HS_BOTTOM"} and last_close > neckline * (1 + self.break_eps):
                    breakout_up = True
                if scene in {"DOUBLE_TOP", "HS_TOP"} and last_close < neckline * (1 - self.break_eps):
                    breakout_down = True

        bias = candidate.bias
        if breakout_up:
            status = "BREAKOUT_UP"
            bias = "BULLISH"
            structure_tags.append("BREAKOUT_UP")
        elif breakout_down:
            status = "BREAKOUT_DOWN"
            bias = "BEARISH"
            structure_tags.append("BREAKDOWN_DOWN")

        confirmed = False
        if pd.notna(wk_vol_ratio) and wk_vol_ratio >= 1.1:
            confirmed = True
            confirm_tags.append("VOL_CONFIRM")
        elif pd.notna(wk_vol_ratio) and wk_vol_ratio < 0.9:
            confirm_tags.append("VOL_WEAK")

        confirm_signals, money_signals = self._collect_confirm_signals(
            df, key_levels, wk_vol_ratio, slope_change
        )
        confirm_tags.extend(confirm_signals)
        money_tags.extend(money_signals)
        confirmed = confirmed or bool(confirm_signals)

        if status in {"BREAKOUT_UP", "BREAKOUT_DOWN"} and confirmed:
            status = "CONFIRMED"
            structure_tags.append("CONFIRMED")
        if not weekly_closed:
            confirm_tags.append("IF_CURRENT_WEEK_UNCLOSED")

        confirm_tags = list(dict.fromkeys(confirm_tags))
        money_tags = list(dict.fromkeys(money_tags))

        score = candidate.score
        if status == "CONFIRMED":
            score += 10
        elif status.startswith("BREAKOUT"):
            score += 5

        return PatternCandidate(
            scene=scene,
            bias=bias,
            status=status,
            score=score,
            key_levels=key_levels,
            structure_tags=structure_tags,
            confirm_tags=confirm_tags,
            money_tags=money_tags,
            description=candidate.description,
        )

    def _collect_confirm_signals(
        self,
        df: pd.DataFrame,
        key_levels: Dict[str, float],
        wk_vol_ratio: float | None,
        slope_change: float | None = None,
    ) -> Tuple[List[str], List[str]]:
        confirm_tags: List[str] = []
        money_tags: List[str] = []
        vol_threshold = self.confirm_vol_ratio_threshold
        if pd.notna(wk_vol_ratio) and wk_vol_ratio >= vol_threshold:
            confirm_tags.append("VOL_CONFIRM")

        obv_slope = float(df["obv_slope_13"].iloc[-1]) if not df.empty else None
        vol_not_weak = pd.isna(wk_vol_ratio) or wk_vol_ratio >= 0.9
        if obv_slope is not None and not pd.isna(obv_slope) and obv_slope > 0:
            money_tags.append("OBV_SLOPE_UP")
        elif slope_change is not None and not pd.isna(slope_change) and slope_change > 0 and vol_not_weak:
            money_tags.append("MONEY_FLOW_TURNING_UP")

        upper = key_levels.get("upper")
        if upper is not None and len(df) >= 2:
            recent_low = df["low"].iloc[-2:].min()
            last_close = df["close"].iloc[-1]
            if pd.notna(recent_low) and pd.notna(last_close):
                hold_threshold = upper * (1 - self.retest_tolerance)
                if last_close > upper * (1 + self.break_eps * 0.5) and recent_low >= hold_threshold:
                    confirm_tags.append("RETEST_HELD")

        return confirm_tags, money_tags

    def _risk(
        self,
        bias: str,
        status: str,
        confirmed: bool,
        *,
        chan_pos: float | None,
        width_pct: float | None,
        atr_pct: float | None,
        close_below_ma_slow: bool,
        close_below_lower: bool,
        close_below_neckline: bool,
    ) -> Tuple[float, str]:
        score = 50.0
        if bias == "BEARISH":
            score += 10.0
        elif bias == "BULLISH":
            score -= 5.0

        if status == "CONFIRMED" and bias == "BEARISH":
            score += 15.0
        elif status == "CONFIRMED" and bias == "BULLISH":
            score -= 10.0
        elif status == "BREAKOUT_DOWN":
            score += 12.0
        elif status == "BREAKOUT_UP":
            score -= 6.0

        if chan_pos is not None:
            edge_risk = abs(chan_pos - 0.5) * 40.0
            if chan_pos >= 0.8:
                edge_risk += 5.0
            if chan_pos <= 0.2:
                edge_risk += 10.0
            score += edge_risk

        vol_pct = None
        for val in [width_pct, atr_pct]:
            if val is not None and not pd.isna(val):
                vol_pct = max(vol_pct or 0.0, float(val))
        if vol_pct is not None:
            score += min(vol_pct * 100.0, 20.0)

        if close_below_ma_slow:
            score += 12.0
        if close_below_lower:
            score += 18.0
        if close_below_neckline:
            score += 18.0

        score = max(0.0, min(100.0, float(score)))

        force_high = close_below_lower or close_below_neckline
        if force_high:
            score = max(score, 80.0)

        if score >= 70.0 or force_high:
            level = "HIGH"
        elif score <= 40.0:
            level = "LOW"
        else:
            level = "MEDIUM"
        return score, level

    @staticmethod
    def _derive_weekly_phase(
        *,
        last_close: float | None,
        ma_fast: float | None,
        ma_slow: float | None,
        upper: float | None,
        lower: float | None,
        status: str,
        bias: str,
    ) -> str:
        if last_close is None or pd.isna(last_close):
            return "RANGE"
        breakdown_trigger = False
        if lower is not None and last_close < lower:
            breakdown_trigger = True
        if status == "BREAKOUT_DOWN":
            breakdown_trigger = True
        if status == "CONFIRMED" and bias == "BEARISH":
            breakdown_trigger = True
        if breakdown_trigger:
            return "BREAKDOWN_RISK"

        if ma_slow is not None and not pd.isna(ma_slow) and last_close < ma_slow:
            return "BEAR_TREND"

        if (
            ma_fast is not None
            and ma_slow is not None
            and not pd.isna(ma_fast)
            and not pd.isna(ma_slow)
            and last_close > ma_fast
            and ma_fast >= ma_slow
        ):
            return "BULL_TREND"

        return "RANGE"

    def _plan_texts(
        self,
        scene_code: str,
        bias: str,
        status: str,
        direction_confirmed: bool,
        key_levels: Dict[str, float],
        asof_week_closed: bool,
        gate_policy: str | None = None,
    ) -> Tuple[str, str, str | None, str, str, float | None, str | None, str, str | None]:
        plan_a_if: List[str] = []
        plan_b_if: List[str] = []
        plan_b_recover: List[str] = []
        plan_a_confirm = "VOL_CONFIRM_REQUIRED" if not direction_confirmed else "CONFIRMED"
        plan_a_then = "THEN_FOLLOW_BREAKOUT_UP" if bias == "BULLISH" else "THEN_DEFEND"
        plan_b_then = "THEN_DOWNGRADE_WAIT"
        exposure_cap = None

        gate_policy_norm = str(gate_policy or "").upper() or None

        if status in {"FORMING"}:
            plan_a_if.append("IF_WAIT_RANGE_BREAK")
        if status == "BREAKOUT_UP":
            plan_a_if.append("IF_BREAKOUT_UP")
        if status == "BREAKOUT_DOWN":
            plan_a_if.append("IF_BREAKDOWN")
        if gate_policy_norm == "WAIT" and "IF_WAIT_RANGE_BREAK" not in plan_a_if:
            plan_a_if.append("IF_WAIT_RANGE_BREAK")
        if not asof_week_closed:
            plan_a_if.append("IF_CURRENT_WEEK_UNCLOSED")
        if direction_confirmed:
            plan_a_if.append("IF_CONFIRMED")
        if "VOL_CONFIRM" in plan_a_if:
            plan_a_confirm = "VOL_CONFIRM_REQUIRED"

        if bias == "BEARISH":
            plan_b_if.append("IF_RISK_HIGH")
        if not direction_confirmed:
            plan_b_if.append("IF_CONFIRM_MISSING")

        if "ma_fast" in key_levels:
            plan_b_recover.append("RECOVER_RECLAIM_MA_FAST")
        if "upper" in key_levels:
            plan_b_recover.append("RECOVER_BACK_INSIDE")

        key_upper = key_levels.get("upper")
        key_lower = key_levels.get("lower")
        key_neckline = key_levels.get("neckline")
        plan_a = ""
        plan_b = ""
        if bias == "BULLISH":
            if status in {"BREAKOUT_UP", "CONFIRMED"}:
                plan_a = "突破上沿，跟随放量上攻"
                plan_b = "若跌回关键位，减仓等待回收"
            else:
                plan_a = "形态整理中，轻仓等待向上确认"
                plan_b = "跌破下沿或颈线则观望/止损"
        elif bias == "BEARISH":
            if status in {"BREAKOUT_DOWN", "CONFIRMED"}:
                plan_a = "跌破关键位，防守为主"
                plan_b = "若收回关键位且量能恢复再观察"
            else:
                plan_a = "偏空形态酝酿，谨慎试仓或等待"
                plan_b = "放量跌破下沿则离场"
        else:
            plan_a = "中性整理，等待突破方向"
            plan_b = "突破失败或假突破时观望"

        key_hint_parts = []
        if key_upper is not None:
            key_hint_parts.append(f"上沿{key_upper:.2f}")
        if key_lower is not None:
            key_hint_parts.append(f"下沿{key_lower:.2f}")
        if key_neckline is not None:
            key_hint_parts.append(f"颈线{key_neckline:.2f}")
        if key_hint_parts:
            plan_a = f"{plan_a}（{';'.join(key_hint_parts)}）"

        plan_a_if_str = _clip_text(";".join(plan_a_if), 255)
        plan_b_if_str = _clip_text(";".join(plan_b_if), 255)
        plan_b_recover_str = _clip_text(";".join(plan_b_recover), 128)
        return (
            _clip_text(plan_a, 255) or "",
            _clip_text(plan_b, 255) or "",
            plan_a_if_str,
            plan_a_then,
            plan_a_confirm,
            exposure_cap,
            plan_b_if_str,
            plan_b_then,
            plan_b_recover_str,
        )

    def build(self, weekly_payload: Dict[str, Any], index_trend: Dict[str, Any] | None = None) -> Dict[str, Any]:
        plan: Dict[str, Any] = {
            "weekly_scene_code": None,
            "weekly_bias": "NEUTRAL",
            "weekly_status": "FORMING",
            "weekly_structure_status": None,
            "weekly_pattern_status": None,
            "weekly_key_levels": {},
            "weekly_key_levels_str": None,
            "weekly_structure_tags": [],
            "weekly_confirm_tags": [],
            "weekly_money_tags": [],
            "weekly_money_proxy": {},
            "weekly_phase": None,
            "weekly_risk_score": None,
            "weekly_risk_level": "UNKNOWN",
            "weekly_confirm": False,
            "weekly_direction_confirmed": False,
            "weekly_gating_enabled": False,
            "weekly_plan_a": None,
            "weekly_plan_b": None,
            "weekly_plan_a_if": None,
            "weekly_plan_a_then": None,
            "weekly_plan_a_confirm": None,
            "weekly_plan_a_exposure_cap": None,
            "weekly_plan_b_if": None,
            "weekly_plan_b_then": None,
            "weekly_plan_b_recover_if": None,
            "weekly_plan_json": None,
        }

        if not isinstance(weekly_payload, dict):
            return plan
        weekly_bars = weekly_payload.get("primary_weekly_bars")
        if not isinstance(weekly_bars, list) or not weekly_bars:
            return plan

        df = self._prepare_df(weekly_bars)
        if df.empty:
            return plan

        asof_raw = weekly_payload.get("weekly_asof_trade_date")
        asof_ts = pd.to_datetime(asof_raw, errors="coerce") if asof_raw is not None else None
        asof_str = asof_ts.date().isoformat() if asof_ts is not None and pd.notna(asof_ts) else None
        asof_ts = pd.to_datetime(asof_str).normalize() if asof_str is not None else None
        if asof_ts is not None:
            df = df[df["week_end"] <= asof_ts].copy().reset_index(drop=True)
            if df.empty:
                return plan

        week_end = df["week_end"].iloc[-1]
        week_end_str = week_end.date().isoformat() if pd.notna(week_end) else None
        if asof_str is None:
            asof_str = week_end_str
        current_week_closed = bool(weekly_payload.get("weekly_current_week_closed", False))
        weekly_closed = weekly_payload.get("weekly_asof_week_closed")
        if weekly_closed is None:
            weekly_closed = bool(current_week_closed and asof_str == week_end_str)

        asof_week_closed = bool(weekly_closed)

        slope_delta = weekly_payload.get("slope_change_4w")
        if slope_delta is None and isinstance(weekly_payload.get("context", {}), dict):
            slope_delta = weekly_payload.get("context", {}).get("slope_change_4w")

        candidate = self._detect_pattern(df, weekly_closed, slope_delta)
        gate_policy = weekly_payload.get("weekly_gate_policy")
        if gate_policy is None and isinstance(index_trend, dict):
            gate_policy = index_trend.get("weekly_gate_policy")

        direction_confirmed = candidate.status == "CONFIRMED"
        structure_status = "CONFIRMED" if direction_confirmed else "FORMING"
        pattern_status = candidate.status
        key_levels = dict(candidate.key_levels)
        if not key_levels.get("ma_fast"):
            ma_fast_val = float(df["ma_fast"].iloc[-1]) if pd.notna(df["ma_fast"].iloc[-1]) else None
            if ma_fast_val:
                key_levels["ma_fast"] = ma_fast_val
        if not key_levels.get("ma_slow"):
            ma_slow_val = float(df["ma_slow"].iloc[-1]) if pd.notna(df["ma_slow"].iloc[-1]) else None
            if ma_slow_val:
                key_levels["ma_slow"] = ma_slow_val
        if not key_levels.get("atr14"):
            atr_val = float(df["atr14"].iloc[-1]) if pd.notna(df["atr14"].iloc[-1]) else None
            if atr_val:
                key_levels["atr14"] = atr_val

        last_close = float(df["close"].iloc[-1]) if pd.notna(df["close"].iloc[-1]) else None
        ma_fast_val = key_levels.get("ma_fast")
        ma_slow_val = key_levels.get("ma_slow")
        upper_level = key_levels.get("upper")
        lower_level = key_levels.get("lower")
        neckline_level = key_levels.get("neckline")

        weekly_phase = self._derive_weekly_phase(
            last_close=last_close,
            ma_fast=ma_fast_val,
            ma_slow=ma_slow_val,
            upper=upper_level,
            lower=lower_level,
            status=candidate.status,
            bias=candidate.bias,
        )

        range_width_pct = (
            float(df["range_width_pct"].iloc[-1])
            if pd.notna(df["range_width_pct"].iloc[-1])
            else None
        )
        atr_pct = None
        atr_val = key_levels.get("atr14")
        if atr_val is not None and last_close:
            atr_pct = float(atr_val) / float(last_close)

        chan_pos = None
        if (
            upper_level is not None
            and lower_level is not None
            and last_close is not None
            and upper_level != lower_level
        ):
            chan_pos = (float(last_close) - float(lower_level)) / (
                float(upper_level) - float(lower_level)
            )

        close_below_ma_slow = (
            last_close is not None
            and ma_slow_val is not None
            and last_close < ma_slow_val
        )
        close_below_lower = (
            last_close is not None and lower_level is not None and last_close < lower_level
        )
        close_below_neckline = (
            last_close is not None
            and neckline_level is not None
            and last_close < neckline_level
        )

        risk_score, risk_level = self._risk(
            candidate.bias,
            candidate.status,
            direction_confirmed,
            chan_pos=chan_pos,
            width_pct=range_width_pct,
            atr_pct=atr_pct,
            close_below_ma_slow=close_below_ma_slow,
            close_below_lower=close_below_lower,
            close_below_neckline=close_below_neckline,
        )

        confidence = float(candidate.score) if candidate.score is not None else None
        force_high = bool(close_below_lower or close_below_neckline)
        if not force_high and confidence is not None and confidence < 75.0 and risk_level == "HIGH":
            risk_level = "MEDIUM"
            risk_score = min(float(risk_score), 69.9)

        key_parts = []
        for label in ["upper", "lower", "neckline", "ma_fast", "ma_slow"]:
            val = key_levels.get(label)
            if val is not None:
                key_parts.append(f"{label}={val:.2f}")
        key_levels_str = ";".join(key_parts)[:255] if key_parts else None

        (
            plan_a,
            plan_b,
            plan_a_if,
            plan_a_then,
            plan_a_confirm,
            exposure_cap,
            plan_b_if,
            plan_b_then,
            plan_b_recover,
        ) = self._plan_texts(
            f"{candidate.scene}_{candidate.status}",
            candidate.bias,
            candidate.status,
            direction_confirmed,
            key_levels,
            asof_week_closed,
            gate_policy,
        )

        if exposure_cap is None:
            base_cap_map = {
                "BULL_TREND": 0.7,
                "RANGE": 0.4,
                "BEAR_TREND": 0.2,
                "BREAKDOWN_RISK": 0.1,
            }
            exposure_cap = base_cap_map.get(weekly_phase, 0.4)

            modifier = 1.0
            confirm_tags = set(candidate.confirm_tags)
            if "VOL_CONFIRM" in confirm_tags:
                modifier += 0.1
            wk_vol_ratio_20 = None
            if "wk_vol_ratio_20" in df.columns and not df.empty:
                wk_vol_ratio_20 = df["wk_vol_ratio_20"].iloc[-1]
            if wk_vol_ratio_20 is not None and not pd.isna(wk_vol_ratio_20) and float(wk_vol_ratio_20) < 0.9:
                modifier -= 0.1
            if chan_pos is not None:
                if chan_pos >= 0.85:
                    modifier -= 0.1
                elif chan_pos >= 0.75:
                    modifier -= 0.05
                if chan_pos <= 0.15:
                    modifier -= 0.1
                elif chan_pos <= 0.25:
                    modifier -= 0.05
            if range_width_pct is not None:
                if range_width_pct >= 0.08:
                    modifier -= 0.1
                elif range_width_pct >= 0.05:
                    modifier -= 0.05
            if atr_pct is not None and atr_pct >= 0.05:
                modifier -= 0.05

            exposure_cap = float(exposure_cap) * modifier
            if not asof_week_closed:
                exposure_cap *= 0.8
            exposure_cap = min(max(exposure_cap, 0.05), 0.95)

        money_proxy = {
            "vol_ratio_20": float(df["wk_vol_ratio_20"].iloc[-1])
            if pd.notna(df["wk_vol_ratio_20"].iloc[-1])
            else None,
            "obv_slope_13": float(df["obv_slope_13"].iloc[-1])
            if pd.notna(df["obv_slope_13"].iloc[-1])
            else None,
        }
        if slope_delta is not None:
            money_proxy["slope_change_4w"] = slope_delta

        plan.update(
            {
                "weekly_scene_code": f"{candidate.scene}_{candidate.status}",
                "weekly_bias": candidate.bias,
                "weekly_status": structure_status,
                "weekly_structure_status": structure_status,
                "weekly_pattern_status": pattern_status,
                "weekly_key_levels": key_levels,
                "weekly_key_levels_str": key_levels_str,
                "weekly_structure_tags": candidate.structure_tags,
                "weekly_confirm_tags": candidate.confirm_tags,
                "weekly_money_tags": candidate.money_tags,
                "weekly_money_proxy": money_proxy,
                "weekly_phase": weekly_phase,
                "weekly_risk_score": risk_score,
                "weekly_risk_level": risk_level,
                "weekly_confirm": direction_confirmed,
                "weekly_direction_confirmed": direction_confirmed,
                "weekly_gating_enabled": True,
                "weekly_plan_a": plan_a,
                "weekly_plan_b": plan_b,
                "weekly_plan_a_if": plan_a_if,
                "weekly_plan_a_then": plan_a_then,
                "weekly_plan_a_confirm": plan_a_confirm,
                "weekly_plan_a_exposure_cap": exposure_cap,
                "weekly_plan_b_if": plan_b_if,
                "weekly_plan_b_then": plan_b_then,
                "weekly_plan_b_recover_if": plan_b_recover,
                "weekly_asof_trade_date": asof_str,
                "weekly_week_closed": bool(weekly_closed),
                "weekly_current_week_closed": current_week_closed,
            }
        )
        plan_payload = {k: v for k, v in plan.items() if k != "weekly_plan_json"}
        assert "weekly_plan_json" not in plan_payload
        plan["weekly_plan_json"] = _clip_text(
            pd.Series(plan_payload).to_json(force_ascii=False), 2000
        )
        return plan

================================================================================
FILE: config.yaml
================================================================================


logging:
  # 日志级别（DEBUG/INFO/WARNING/ERROR 或数字），默认 INFO；
  # 也可通过环境变量 ASHARE_LOG_LEVEL 覆盖。
  level: INFO

app:
  # 输出目录（AshareApp.output_dir）
  output_dir: output
  # 是否拉取/增量更新日线K线数据（history_daily_kline）
  # true：会调用 Baostock K 线接口进行冷启动/增量更新
  # false：不请求 Baostock K 线接口，只从数据库表切片读取
  #       （要求 history_daily_kline 表已存在且非空）
  fetch_daily_kline: true

  # 是否导出股票元数据（股票列表/证券基本资料/行业分类/指数成分股）
  # true：会调用 Baostock 元数据接口更新 a_share_stock_list/a_share_stock_basic/a_share_stock_industry/index_*_members
  # false：不请求 Baostock 元数据接口，只从数据库读取（要求相关表已存在，或至少存在 history_daily_kline 用于兜底）
  fetch_stock_meta: true

  # 最近 N 个交易日窗口
  history_days: 365

  # 近期自然日视图窗口（仅用于你手动查询近期数据方便）
  # - 会创建/刷新视图：history_recent_{history_view_days}_days
  # - 与 history_days 解耦；history_days 仍用于程序内部计算窗口
  # - 设为 0 则不创建视图
  history_view_days: 45
  # 流动性排序时保留的股票数量
  top_liquidity_count: 100
  # 最少上市天数过滤阈值
  min_listing_days: 60
  # 是否在每次运行时刷新 Baostock 季频财务数据（非常耗时）
  # true：每次运行都会打 Baostock 接口，更新 fundamentals_quarter_* 表
  # false：只使用数据库中已有的财务表，直接拼接 fundamentals_latest_wide
  refresh_fundamentals: false

  # 是否拉取常用指数日线数据（用于大盘趋势过滤）
  fetch_index_kline: true
  index_codes:
    - sh.000001
    - sh.000300
    - sh.000905
    - sz.399001
    - sz.399006
  index_history_days: 900

  # 是否创建/更新股票-行业维表（基于 query_stock_industry）
  build_stock_industry_dim: true

database:
  host: 127.0.0.1
  port: 3306
  user: root
  password: ""
  db_name: ashare
  # SQLAlchemy 连接池配置，避免频繁创建连接
  pool_size: 5
  max_overflow: 10
  pool_recycle: 1800
  pool_timeout: 30

proxy:
  http: http://127.0.0.1:7890
  https: http://127.0.0.1:7890

baostock:
  # Baostock 登录失败时的重试次数与间隔（秒）
  retry: 3
  retry_sleep: 3.0
  # Socket 超时（秒），防止底层网络调用无限等待
  socket_timeout: 30
  # 登录状态保活探测间隔（秒），避免高频心跳导致频繁登录
  keepalive_interval: 60
  # 单只股票拉取的子进程超时时间（秒）
  per_code_timeout: 30
  # 单只股票最大尝试次数
  max_retries: 2
  # 断点续跑判定：每只股票满足最少行数后视为已完成
  resume_min_rows_per_code: 200
  # 并发控制：默认/最小/最大子进程数以及网络带宽上限
  worker_processes: 1
  min_worker_processes: 1
  max_worker_processes: 8
  network_worker_cap: 1
  # 进度日志与批量写入配置
  progress_log_every: 100
  history_flush_batch: 5
  history_write_chunksize: 500
  # 清理策略：window=仅清理当前批次区间，skip=直接追加；history_retention_days>0 时会按窗口定期清理
  history_cleanup_mode: window
  history_retention_days: 0

akshare:
  enabled: true
  lhb:
    enabled: true
  margin:
    enabled: true
    exchanges:
      - sse
      - szse
  gdhs:
    enabled: true
    period: ""
    detail_enabled: true
    detail_top_n: 100

  board_industry:
    enabled: true
    spot_enabled: true
    hist_enabled: true
    history_days: 300
    adjust: hfq
    build_stock_board_dim: true

# MA5-MA20 顺势趋势波段系统
# - enabled: true 时会在 start.py 数据采集后自动运行，默认写入 signals 表并创建 candidates 视图
strategy_ma5_ma20_trend:
  enabled: true
  # 选股池来源：top_liquidity（推荐，低频）/ universe / all
  universe_source: top_liquidity
  # 与 app.history_days 对齐（默认 365）
  lookback_days: 300

  volume_ratio_threshold: 1.5
  volume_ma_window: 5

  pullback_band: 0.01
  kdj_low_threshold: 30

  indicator_table: strategy_indicator_daily
  signal_events_table: strategy_signal_events

  # signals 写入范围：latest=仅最新交易日（默认），window=回填本次计算窗口内全部交易日
  signals_write_scope: window
  # 信号有效期（交易日口径）
  valid_days: 3

# 开盘监测：检查“前一交易日收盘 BUY 信号”是否可在今日开盘执行
open_monitor:
  enabled: true
  # 以下两项仅供 SchemaManager 生成 ready_signals_view 时使用（open_monitor 运行时不直接读取这些表）
  # signal_events_table 默认沿用 strategy_ma5_ma20_trend.signal_events_table，可显式覆盖
  # indicator_table 默认沿用 strategy_ma5_ma20_trend.indicator_table（用于补齐 close/ma*/atr 等指标列）
  # signal_events_table: strategy_signal_events
  # indicator_table: strategy_indicator_daily
  quote_table: strategy_open_monitor_quote
  output_table: strategy_open_monitor_eval
  # 指数代码与回看窗口
  index_code: sh.000001
  index_hist_lookback_days: 250

  # 回看近 N 个交易日的 BUY 信号作为候选
  signal_lookback_days: 5

  # 行情来源：akshare / eastmoney（路线A：建议固定 eastmoney，避免 AkShare/代理导致噪音）
  quote_source: eastmoney

  # 调度间隔（分钟）：默认与 run_open_monitor_scheduler 对齐，同时作为盘中去重桶大小
  interval_minutes: 5
  # 去重时间桶分辨率（分钟）：缺省跟 interval_minutes 一致，可单独覆盖
  run_id_minutes: 5

  # 过滤阈值（按你的风格可自行调小/调大）
  max_gap_up_pct: 0.05        # 今开高开超过 5%：不追
  max_gap_up_atr_mult: 1.5    # 动态阈值：gap_up > min(max_gap_up_pct, ATR*mult/昨收) 也会不追（更能挡住跳空追高）
  max_gap_down_pct: -0.03     # 今开低开超过 -3%：不接
  min_open_vs_ma20_pct: 0.0   # 今开必须站上 MA20（0.0=不允许跌破）
  limit_up_trigger_pct: 9.7   # 涨幅接近/达到涨停：跳过

  # 追高过滤：入场价相对 MA5 乖离过大（例如 0.08=超过 MA5 8% 就不追）
  max_entry_vs_ma5_pct: 0.08
  # 乖离/拉升阈值：用于可解释化派生字段
  runup_atr_max: 1.2
  dev_ma5_atr_max: 2.0
  dev_ma20_atr_max: 2.5

  # 指数关键位突破阈值（用于盘中解锁/解释）
  live_breakout_high_eps: 0.002    # 冲过上沿：high >= upper * (1 + eps)
  live_breakout_latest_eps: 0.001  # 站稳上方：latest >= upper * (1 + eps)
  live_retest_pct: 0.003           # 回踩容忍：upper * pct 与 ATR 容忍二选大
  live_retest_atr_mult: 0.5        # 回踩容忍：daily_atr14 * mult

  # 风控：开盘以“实际入场价”为基准计算止损（stop_ref=entry-stop_atr_mult*ATR）
  stop_atr_mult: 2.0

  # 情绪过滤：信号日如果已经接近涨停的大阳线，次日默认不追
  signal_day_limit_up_pct: 0.095

  # 大盘环境分数阈值（指数均线评分低于此值时默认 WAIT）
  env_index_score_threshold: 1

  # 输出
  write_to_db: true
  # 增量写入：true=每次运行 append（保留历史快照，便于对比）；false=按 monitor_date+code 去重，只保留当天最新一份
  incremental_write: true
  # 增量导出：true=文件名带 checked_at 时间戳，避免同一天多次导出覆盖
  incremental_export_timestamp: true
  export_csv: false
  export_top_n: 100
  output_subdir: open_monitor

================================================================================
FILE: QWEN.md
================================================================================

# A股量化策略分析系统

## 项目概述

这是一个基于 Baostock 和 AkShare 的 A 股量化策略分析系统，提供数据采集、策略分析、开盘监测和风险控制等功能。项目采用 Python 开发，使用 MySQL 作为数据存储，实现了完整的量化投资分析流程。

## 核心功能

### 1. 数据采集与预处理
- **数据源**：从 Baostock 获取股票基础数据、日线数据、指数数据等
- **数据管理**：支持历史数据批量拉取和增量更新
- **流动性筛选**：提供流动性排序和高流动性标的筛选
- **行为数据**：支持龙虎榜、两融、股东户数等行为数据采集

### 2. 策略分析模块
- **MA5-MA20 趋势策略**：基于均线交叉的顺势交易策略
  - 多头排列趋势过滤
  - MA5/MA20 金叉死叉信号
  - 放量确认和 MACD 过滤
  - KDJ 低位金叉增强信号
- **筹码筛选策略**：基于股东户数变化的筹码集中度分析
- **周线通道策略**：基于周线级别的趋势通道分析

### 3. 开盘监测系统
- **信号读取**：读取前一交易日收盘信号（BUY）
- **实时过滤**：结合实时行情进行二次过滤
- **风险评估**：提供追高风险、破位风险、涨停风险等多维度评估
- **执行建议**：输出可执行/不可执行清单

### 4. 风险控制机制
- **基本面风险**：净利润、同比增长等财务指标评估
- **技术面风险**：ATR止损、均线破位等技术指标控制
- **市场环境**：大盘趋势、情绪指标等市场环境过滤
- **个股风险**：ST标签、妖股识别等个股特殊风险

## 项目结构

```
AShare/
├── ashare/                 # 核心模块
│   ├── app.py             # 数据采集主入口
│   ├── ma5_ma20_trend_strategy.py  # MA5-MA20 策略
│   ├── chip_filter.py     # 筹码筛选
│   ├── open_monitor.py    # 开盘监测
│   ├── schema_manager.py  # 数据库表结构管理
│   └── ...               # 其他功能模块
├── config.yaml            # 配置文件
├── start.py              # 项目启动脚本
├── run_*.py              # 各功能模块运行脚本
├── requirements.txt       # 依赖包
└── README.md             # 项目说明
```

## 依赖包

项目主要依赖以下 Python 包：
- `akshare==1.17.94` - 金融数据接口
- `baostock==0.8.9` - 股票数据接口
- `pandas==2.3.3` - 数据处理
- `numpy==2.3.5` - 数值计算
- `PyMySQL==1.1.2` - MySQL 数据库连接
- `SQLAlchemy==2.0.45` - ORM 框架
- `PyYAML==6.0.3` - 配置文件解析

## 配置说明

### 数据库配置
```yaml
database:
  host: 127.0.0.1
  port: 3306
  user: root
  password: ""
  db_name: ashare
```

### 策略参数配置
```yaml
strategy_ma5_ma20_trend:
  enabled: true
  lookback_days: 300
  volume_ratio_threshold: 1.5
  pullback_band: 0.01
  kdj_low_threshold: 30
  signals_write_scope: window
  valid_days: 3
```

### 开盘监测配置
```yaml
open_monitor:
  enabled: true
  signal_lookback_days: 5
  quote_source: eastmoney
  max_gap_up_pct: 0.05
  max_gap_down_pct: -0.03
  min_open_vs_ma20_pct: 0.0
  limit_up_trigger_pct: 9.7
  write_to_db: true
  incremental_write: true
```

## 运行方式

### 完整流程运行
```bash
# 运行完整流程（数据采集 + 策略分析 + 开盘监测）
python start.py
```

### 单独运行各模块
```bash
# 数据采集
python -c "from ashare.app import AshareApp; AshareApp().run()"

# MA5-MA20 策略
python run_ma5_ma20_trend_strategy.py

# 筹码筛选
python run_chip_filter.py

# 开盘监测
python run_open_monitor.py

# 定时运行开盘监测
python run_open_monitor_scheduler.py --interval 5
```

## 数据库表结构

系统自动管理以下核心数据表：
- `a_share_stock_list`: 股票列表
- `history_daily_kline`: 历史日线数据
- `a_share_universe`: 股票池（已过滤ST、退市等）
- `strategy_indicator_daily`: 策略指标数据
- `strategy_signal_events`: 策略信号事件
- `strategy_ready_signals`: 策略准备就绪信号
- `strategy_chip_filter`: 筹码筛选数据
- `strategy_open_monitor_eval`: 开盘监测评估结果
- `strategy_open_monitor_quote`: 开盘监测行情数据

## 策略逻辑

### MA5-MA20 趋势策略
1. **趋势过滤**：多头排列（close > MA60 > MA250，MA20 > MA60 > MA250）
2. **买入信号**：
   - MA5 上穿 MA20（金叉）+ 放量 + MACD 确认
   - 趋势回踩 MA20 + MA5 向上 + MACD 确认
   - MACD 柱翻红确认
   - W 底突破确认
3. **卖出信号**：
   - MA5 下穿 MA20（死叉）
   - 跌破 MA20 且放量
   - MACD 柱翻绿

### 开盘监测逻辑
1. 读取前一交易日的 BUY 信号
2. 获取实时开盘行情
3. 多维度过滤：
   - 高开过多（追高风险）
   - 低开破位（跌破 MA20）
   - 涨停（买不到）
4. 输出执行建议（EXECUTE/WAIT/STOP）

## 开发约定

- **编码风格**：遵循 PEP 8 Python 编码规范
- **日志记录**：使用 logging 模块记录运行状态
- **异常处理**：统一异常处理机制，避免程序崩溃
- **数据库操作**：使用 SQLAlchemy ORM 进行数据库操作
- **配置管理**：使用 config.yaml 进行配置管理

## 注意事项

1. **网络环境**：项目依赖 Baostock 和 AkShare 接口，需要稳定的网络连接
2. **数据更新**：建议在交易日结束后运行数据采集，确保数据完整性
3. **策略参数**：可根据市场环境和个人偏好调整策略参数
4. **风险提示**：所有策略仅供研究参考，不构成投资建议

## 扩展功能

项目设计具有良好的扩展性，可以方便地添加：
- 新的量化策略
- 其他数据源
- 更多风险控制指标
- 自定义信号评估逻辑
================================================================================
FILE: README.md
================================================================================

# A股量化策略分析系统

基于 Baostock 和 AkShare 的 A 股量化策略分析系统，提供数据采集、策略分析、开盘监测和风险控制等功能。

## 项目概述

本项目是一个综合性的 A 股量化分析平台，主要功能包括：

- **数据采集**：从 Baostock 获取股票基础数据、日线数据、指数数据等
- **策略分析**：实现 MA5-MA20 趋势策略、筹码筛选策略等
- **开盘监测**：实时监测前一交易日信号在开盘时的执行可行性
- **风险控制**：多维度风险评估和过滤机制
- **数据库管理**：自动创建和维护策略相关的数据表结构

## 核心功能

### 1. 数据采集与预处理
- 自动登录 Baostock 获取股票列表和日线数据
- 支持历史数据批量拉取和增量更新
- 提供流动性筛选和高流动性标的排序
- 支持龙虎榜、两融、股东户数等行为数据采集

### 2. 策略分析模块
- **MA5-MA20 趋势策略**：基于均线交叉的顺势交易策略
  - 多头排列趋势过滤
  - MA5/MA20 金叉死叉信号
  - 放量确认和 MACD 过滤
  - KDJ 低位金叉增强信号
- **筹码筛选策略**：基于股东户数变化的筹码集中度分析
- **周线通道策略**：基于周线级别的趋势通道分析

### 3. 开盘监测系统
- 读取前一交易日收盘信号（BUY）
- 结合实时行情进行二次过滤
- 提供追高风险、破位风险、涨停风险等多维度评估
- 输出可执行/不可执行清单

### 4. 风险控制机制
- 基本面风险评估（净利润、同比增长等）
- 技术面风险控制（ATR止损、均线破位等）
- 市场环境过滤（大盘趋势、情绪指标等）
- 个股特殊风险（ST标签、妖股识别等）

## 项目结构

```
AShare/
├── ashare/                 # 核心模块
│   ├── app.py             # 数据采集主入口
│   ├── ma5_ma20_trend_strategy.py  # MA5-MA20 策略
│   ├── chip_filter.py     # 筹码筛选
│   ├── open_monitor.py    # 开盘监测
│   ├── schema_manager.py  # 数据库表结构管理
│   └── ...               # 其他功能模块
├── config.yaml            # 配置文件
├── start.py              # 项目启动脚本
├── run_*.py              # 各功能模块运行脚本
├── requirements.txt       # 依赖包
└── README.md             # 项目说明
```

## 安装与配置

### 1. 环境准备
```bash
# 安装依赖
pip install -r requirements.txt
```

### 2. 数据库配置
项目默认使用 MySQL 数据库，可在 `config.yaml` 中配置：

```yaml
database:
  host: 127.0.0.1
  port: 3306
  user: root
  password: ""
  db_name: ashare
```

### 3. 代理配置（可选）
如果需要通过代理访问网络，可在 `config.yaml` 中配置：

```yaml
proxy:
  http: http://127.0.0.1:7890
  https: http://127.0.0.1:7890
```

## 使用方法

### 1. 完整流程运行
```bash
# 运行完整流程（数据采集 + 策略分析 + 开盘监测）
python start.py
```

### 2. 单独运行各模块

#### 数据采集
```bash
# 仅运行数据采集（股票列表、日线数据等）
python -c "from ashare.app import AshareApp; AshareApp().run()"
```

#### MA5-MA20 策略
```bash
# 运行 MA5-MA20 趋势策略
python run_ma5_ma20_trend_strategy.py
```

#### 筹码筛选
```bash
# 运行筹码筛选策略
python run_chip_filter.py
```

#### 开盘监测
```bash
# 运行开盘监测
python run_open_monitor.py

# 定时运行开盘监测（每5分钟一次）
python run_open_monitor_scheduler.py --interval 5
```

#### 周线市场指标
```bash
# 运行周线市场指标分析
python run_index_weekly_channel.py
```

#### 日线市场指标
```bash
# 运行日线市场指标分析
python run_daily_market_indicator.py
```

#### 预开盘漏斗
```bash
# 运行预开盘漏斗分析
python run_premarket_funnel.py
```

## 配置说明

### 策略参数配置
在 `config.yaml` 中可以配置各种策略参数：

#### MA5-MA20 策略配置
```yaml
strategy_ma5_ma20_trend:
  enabled: true
  lookback_days: 300
  volume_ratio_threshold: 1.5
  pullback_band: 0.01
  kdj_low_threshold: 30
  signals_write_scope: window
  valid_days: 3
```

#### 开盘监测配置
```yaml
open_monitor:
  enabled: true
  signal_lookback_days: 5
  quote_source: eastmoney
  max_gap_up_pct: 0.05
  max_gap_down_pct: -0.03
  min_open_vs_ma20_pct: 0.0
  limit_up_trigger_pct: 9.7
  write_to_db: true
  incremental_write: true
```

### 数据库表结构
系统自动管理以下核心数据表：

- `a_share_stock_list`: 股票列表
- `history_daily_kline`: 历史日线数据
- `a_share_universe`: 股票池（已过滤ST、退市等）
- `strategy_indicator_daily`: 策略指标数据
- `strategy_signal_events`: 策略信号事件
- `strategy_ready_signals`: 策略准备就绪信号
- `strategy_chip_filter`: 筹码筛选数据
- `strategy_open_monitor_eval`: 开盘监测评估结果
- `strategy_open_monitor_quote`: 开盘监测行情数据

## 策略逻辑

### MA5-MA20 趋势策略
1. **趋势过滤**：多头排列（close > MA60 > MA250，MA20 > MA60 > MA250）
2. **买入信号**：
   - MA5 上穿 MA20（金叉）+ 放量 + MACD 确认
   - 趋势回踩 MA20 + MA5 向上 + MACD 确认
   - MACD 柱翻红确认
   - W 底突破确认
3. **卖出信号**：
   - MA5 下穿 MA20（死叉）
   - 跌破 MA20 且放量
   - MACD 柱翻绿

### 开盘监测逻辑
1. 读取前一交易日的 BUY 信号
2. 获取实时开盘行情
3. 多维度过滤：
   - 高开过多（追高风险）
   - 低开破位（跌破 MA20）
   - 涨停（买不到）
4. 输出执行建议（EXECUTE/WAIT/STOP）

## 注意事项

1. **网络环境**：项目依赖 Baostock 和 AkShare 接口，需要稳定的网络连接
2. **数据更新**：建议在交易日结束后运行数据采集，确保数据完整性
3. **策略参数**：可根据市场环境和个人偏好调整策略参数
4. **风险提示**：所有策略仅供研究参考，不构成投资建议

## 扩展功能

项目设计具有良好的扩展性，可以方便地添加：
- 新的量化策略
- 其他数据源
- 更多风险控制指标
- 自定义信号评估逻辑

## 许可证

本项目仅供学习和研究使用。
================================================================================
FILE: requirements.txt
================================================================================

# [SKIP] 非 UTF-8 文本文件，未导出内容。

================================================================================
FILE: run_chip_filter.py
================================================================================

"""运行筹码过滤计算。"""

from __future__ import annotations

import pandas as pd

from ashare.chip_filter import ChipFilter
from ashare.db import DatabaseConfig, MySQLWriter
from ashare.open_monitor import OpenMonitorParams
from ashare.open_monitor_repo import OpenMonitorRepository
from ashare.schema_manager import ensure_schema
from ashare.utils.logger import setup_logger


def _build_chip_inputs(signals: pd.DataFrame) -> pd.DataFrame:
    if signals.empty:
        return pd.DataFrame()

    inputs = pd.DataFrame()
    inputs["sig_date"] = pd.to_datetime(signals.get("sig_date"), errors="coerce")
    inputs["date"] = inputs["sig_date"]
    if "code" not in signals.columns:
        return pd.DataFrame()
    inputs["code"] = signals["code"].astype(str)

    mapping = {
        "sig_vol_ratio": "vol_ratio",
        "sig_close": "close",
        "sig_ma20": "ma20",
        "sig_fear_score": "fear_score",
        "sig_macd_hist": "macd_hist",
    }
    for src, dest in mapping.items():
        if src in signals.columns:
            inputs[dest] = signals[src]

    inputs = inputs.dropna(subset=["sig_date", "code"])
    return inputs


def main() -> int:
    ensure_schema()
    logger = setup_logger()
    params = OpenMonitorParams.from_config()
    db_writer = MySQLWriter(DatabaseConfig.from_env())
    repo = OpenMonitorRepository(db_writer.engine, logger, params)

    latest_trade_date, signal_dates, signals = repo.load_recent_buy_signals()
    if not latest_trade_date or signals.empty:
        logger.warning("未获取到可用信号，已跳过筹码计算。")
        return 0

    logger.info("筹码过滤输入信号数=%s（信号日=%s）", len(signals), signal_dates)
    inputs = _build_chip_inputs(signals)
    if inputs.empty:
        logger.warning("筹码过滤输入为空，已跳过。")
        return 0

    chip_df = ChipFilter().apply(inputs)
    rowcount = 0 if chip_df is None else len(chip_df)
    logger.info("筹码过滤写入完成，行数=%s。", rowcount)
    return rowcount


if __name__ == "__main__":
    main()

================================================================================
FILE: run_daily_market_indicator.py
================================================================================

"""回填日线市场指标。"""

from __future__ import annotations

from ashare.market_indicator_builder import MarketIndicatorBuilder
from ashare.market_indicator_runner import MarketIndicatorRunner
from ashare.open_monitor import MA5MA20OpenMonitorRunner
from ashare.schema_manager import ensure_schema


def run_daily_market_indicator(
    *, start_date: str | None = None, end_date: str | None = None, mode: str = "incremental"
) -> dict:
    ensure_schema()
    runner = MA5MA20OpenMonitorRunner()
    builder = MarketIndicatorBuilder(env_builder=runner.env_builder, logger=runner.logger)
    indicator_runner = MarketIndicatorRunner(
        repo=runner.repo,
        builder=builder,
        logger=runner.logger,
    )
    return indicator_runner.run_daily_indicator(
        start_date=start_date,
        end_date=end_date,
        mode=mode,
    )


def main() -> None:
    run_daily_market_indicator()


if __name__ == "__main__":
    main()

================================================================================
FILE: run_index_weekly_channel.py
================================================================================

"""回填周线市场指标。"""

from __future__ import annotations

from ashare.market_indicator_builder import MarketIndicatorBuilder
from ashare.market_indicator_runner import MarketIndicatorRunner
from ashare.open_monitor import MA5MA20OpenMonitorRunner
from ashare.schema_manager import ensure_schema


def run_weekly_market_indicator(
    *, start_date: str | None = None, end_date: str | None = None, mode: str = "incremental"
) -> dict:
    """回填周线指标（增量/全量由 mode 控制）。"""
    ensure_schema()
    runner = MA5MA20OpenMonitorRunner()
    builder = MarketIndicatorBuilder(env_builder=runner.env_builder, logger=runner.logger)
    indicator_runner = MarketIndicatorRunner(
        repo=runner.repo,
        builder=builder,
        logger=runner.logger,
    )
    return indicator_runner.run_weekly_indicator(
        start_date=start_date,
        end_date=end_date,
        mode=mode,
    )


def main() -> None:
    run_weekly_market_indicator()


if __name__ == "__main__":
    main()

================================================================================
FILE: run_ma5_ma20_trend_strategy.py
================================================================================

"""运行 MA5-MA20 顺势趋势波段系统。

用法：
  python run_ma5_ma20_trend_strategy.py

说明：
  - 读取 config.yaml 的 strategy_ma5_ma20_trend 配置。
  - 需要你已先运行 python start.py，把 history_recent_{N}_days 等表准备好。
"""

from ashare.ma5_ma20_trend_strategy import MA5MA20StrategyRunner
from ashare.schema_manager import ensure_schema


def main() -> None:
    # 独立执行脚本：不受 config.yaml 的 enabled 总开关限制
    ensure_schema()
    MA5MA20StrategyRunner().run(force=True)


if __name__ == "__main__":
    main()

================================================================================
FILE: run_open_monitor.py
================================================================================

from __future__ import annotations

from ashare.open_monitor import MA5MA20OpenMonitorRunner
from ashare.schema_manager import ensure_schema


def main() -> None:
    ensure_schema()
    MA5MA20OpenMonitorRunner().run(force=True)


if __name__ == "__main__":
    main()

================================================================================
FILE: run_open_monitor_scheduler.py
================================================================================

"""定时调度执行开盘监测（每整 N 分钟触发一次，自动跳过非交易日）。"""

from __future__ import annotations

import datetime as dt
from datetime import timedelta
import time

from ashare.config import get_section
from ashare.open_monitor import MA5MA20OpenMonitorRunner
from ashare.env_snapshot_utils import load_trading_calendar
from ashare.schema_manager import ensure_schema


def _next_run_at(now: dt.datetime, interval_min: int) -> dt.datetime:
    """计算下一个“整 interval_min 分钟”的触发时间（秒=0）。"""

    if interval_min <= 0:
        raise ValueError("interval_min must be positive")

    # 如果正好落在边界（例如 09:30:00），就返回 now（便于立即执行）
    if now.second == 0 and now.microsecond == 0 and (now.minute % interval_min == 0):
        return now

    minute_block = (now.minute // interval_min) * interval_min
    next_minute = minute_block + interval_min

    base = now.replace(second=0, microsecond=0)
    if next_minute < 60:
        return base.replace(minute=next_minute)
    # 进位到下一小时
    return base.replace(minute=0) + dt.timedelta(hours=1)


def _default_interval_from_config() -> int:
    cfg = get_section("open_monitor") or {}
    if not isinstance(cfg, dict):
        return 5
    try:
        interval = int(cfg.get("interval_minutes", 5))
        return interval if interval > 0 else 5
    except Exception:  # noqa: BLE001
        return 5


TRADING_WINDOWS = [
    (dt.time(hour=9, minute=20), dt.time(hour=11, minute=35)),
    (dt.time(hour=12, minute=50), dt.time(hour=15, minute=10)),
]


def _in_trading_window(ts: dt.datetime) -> bool:
    t = ts.time()
    for start, end in TRADING_WINDOWS:
        if start <= t <= end:
            return True
    return False


def _is_trading_day(runner: MA5MA20OpenMonitorRunner, d: dt.date) -> bool:
    """判断是否交易日：优先用 baostock 交易日历，失败则回退工作日。"""
    try:
        # 覆盖一小段范围，便于缓存复用（模块级缓存）。
        start = d - timedelta(days=30)
        end = d + timedelta(days=30)
        calendar = load_trading_calendar(start, end)
        if calendar:
            return d.isoformat() in calendar
    except Exception:
        pass
    return d.weekday() < 5

def _next_trading_day(d: dt.date, runner: MA5MA20OpenMonitorRunner) -> dt.date:
    """返回 >=d 的下一个交易日。"""
    cur = d
    for _ in range(370):  # 最多兜底一年，避免死循环
        if _is_trading_day(runner, cur):
            return cur
        cur = cur + timedelta(days=1)
    return d


def _next_trading_start(ts: dt.datetime, runner: MA5MA20OpenMonitorRunner) -> dt.datetime:
    today = ts.date()
    t = ts.time()

    today = _next_trading_day(today, runner)
    for start, end in TRADING_WINDOWS:
        if t < start:
            return dt.datetime.combine(today, start)
        if start <= t <= end:
            return ts

    nxt = _next_trading_day(today + dt.timedelta(days=1), runner)
    return dt.datetime.combine(nxt, TRADING_WINDOWS[0][0])


def main(*, interval_minutes: int | None = None, once: bool = False) -> None:
    ensure_schema()
    runner = MA5MA20OpenMonitorRunner()
    logger = runner.logger
    interval_min = int(interval_minutes or _default_interval_from_config())
    if interval_min <= 0:
        raise ValueError("interval must be positive")

    logger.info("开盘监测调度器启动：interval=%s 分钟（整点对齐）", interval_min)

    try:
        while True:
            now = dt.datetime.now()
            run_at = _next_run_at(now, interval_min)

            if not _in_trading_window(run_at):
                next_start = _next_trading_start(run_at, runner)
                if next_start > now:
                    logger.info(
                        "当前不在交易时段，下一交易窗口：%s",
                        next_start.strftime("%Y-%m-%d %H:%M:%S"),
                    )
                run_at = _next_run_at(next_start, interval_min)

            while not _in_trading_window(run_at):
                next_start = _next_trading_start(run_at + dt.timedelta(minutes=interval_min), runner)
                run_at = _next_run_at(next_start, interval_min)

            sleep_s = (run_at - dt.datetime.now()).total_seconds()
            if sleep_s > 0:
                logger.info("下一次触发：%s（%.1fs 后）", run_at.strftime("%Y-%m-%d %H:%M:%S"), sleep_s)
                time.sleep(sleep_s)

            trigger_at = run_at
            logger.info(
                "触发开盘监测：%s",
                trigger_at.strftime("%Y-%m-%d %H:%M:%S"),
            )
            try:
                runner.run(force=True, checked_at=trigger_at)
            except Exception as exc:  # noqa: BLE001
                logger.exception("开盘监测执行异常（将等待下一轮）：%s", exc)

            if once:
                logger.info("调度器已按 once 执行完成，退出。")
                return

            # 防止“刚好运行很快又落在同一秒边界”导致重复触发
            time.sleep(0.2)

    except KeyboardInterrupt:
        logger.info("收到 Ctrl+C，调度器退出。")


if __name__ == "__main__":
    main()

================================================================================
FILE: run_premarket_funnel.py
================================================================================

"""盘前漏斗总控脚本：依次跑基础信号/筹码/日线/周线。"""

from __future__ import annotations

from ashare.utils.logger import setup_logger
from run_chip_filter import main as run_chip_filter
from run_daily_market_indicator import main as run_daily_market_indicator
from run_index_weekly_channel import main as run_index_weekly_channel
from run_ma5_ma20_trend_strategy import main as run_ma5_ma20_trend_strategy


def _run_step(name: str, func):
    logger = setup_logger()
    logger.info("[premarket] start: %s", name)
    result = func()
    logger.info("[premarket] done: %s", name)
    return result


def main() -> None:
    logger = setup_logger()
    try:
        _run_step("ma5_ma20_trend", run_ma5_ma20_trend_strategy)
        chip_rows = _run_step("chip_filter", run_chip_filter)
        logger.info("[premarket] chip_filter rows=%s", chip_rows)
        _run_step("daily_market_indicator", run_daily_market_indicator)
        _run_step("index_weekly_channel", run_index_weekly_channel)
    except Exception as exc:  # noqa: BLE001
        logger.exception("[premarket] step failed, aborting: %s", exc)
        raise


if __name__ == "__main__":
    main()

================================================================================
FILE: start.py
================================================================================

"""项目启动脚本入口, 直接执行即可运行示例."""

import datetime as dt
import json
import logging
from pathlib import Path

from sqlalchemy import text

from ashare.app import AshareApp
from ashare.ma5_ma20_trend_strategy import MA5MA20StrategyRunner
from ashare.open_monitor import MA5MA20OpenMonitorRunner
from ashare.schema_manager import ensure_schema
from run_index_weekly_channel import run_weekly_market_indicator


def _parse_asof_date(raw: str | None) -> str | None:
    if not raw:
        return None
    try:
        return dt.date.fromisoformat(str(raw)).isoformat()
    except Exception:  # noqa: BLE001
        logging.warning(f"[WARN] --asof-date 无法解析：{raw}")
        return None


def _resolve_latest_sig_date(
    repo, table: str, strategy_code: str
) -> str | None:
    if not table or not repo._table_exists(table):  # noqa: SLF001
        return None
    if "sig_date" not in repo._get_table_columns(table):  # noqa: SLF001
        return None
    clause = "WHERE `strategy_code` = :strategy_code" if strategy_code else ""
    stmt = text(
        f"SELECT MAX(`sig_date`) AS latest_sig_date FROM `{table}` {clause}"
    )
    with repo.engine.begin() as conn:
        row = conn.execute(stmt, {"strategy_code": strategy_code}).mappings().first()
    if not row:
        return None
    v = row.get("latest_sig_date")
    if isinstance(v, dt.datetime):
        return v.date().isoformat()
    if isinstance(v, dt.date):
        return v.isoformat()
    if v:
        try:
            return dt.datetime.fromisoformat(str(v)).date().isoformat()
        except Exception:  # noqa: BLE001
            return None
    return None


def _load_sig_date_breakdown(
    repo, table: str, strategy_code: str, limit: int = 7
) -> list[dict]:
    if not table or not repo._table_exists(table):  # noqa: SLF001
        return []
    cols = set(repo._get_table_columns(table))  # noqa: SLF001
    if "sig_date" not in cols:
        return []

    clauses = []
    params: dict = {"limit": limit}
    if strategy_code and "strategy_code" in cols:
        clauses.append("`strategy_code` = :strategy_code")
        params["strategy_code"] = strategy_code
    if "signal" in cols:
        clauses.append("`signal` = 'BUY'")

    where = f"WHERE {' AND '.join(clauses)}" if clauses else ""
    stmt = text(
        f"""
        SELECT `sig_date`, COUNT(*) AS cnt
        FROM `{table}`
        {where}
        GROUP BY `sig_date`
        ORDER BY `sig_date` DESC
        LIMIT :limit
        """
    )

    with repo.engine.begin() as conn:
        rows = conn.execute(stmt, params).fetchall()

    breakdown = []
    for row in rows:
        sig_date = row[0]
        cnt = row[1]
        if sig_date:
            breakdown.append({"sig_date": str(sig_date), "count": int(cnt or 0)})
    return breakdown


def _self_check(
    repo,
    *,
    ready_view: str | None,
    total_buy_cnt: int,
    latest_sig_date_cnt: int,
    weekly_status: dict,
    skipped_weekly: bool,
) -> None:
    if ready_view and repo._table_exists(ready_view):  # noqa: SLF001
        logging.info(f"[CHECK] ready_signals_view OK: {ready_view}")
    else:
        logging.info("[CHECK] ready_signals_view 缺失或未配置。")

    logging.info(f"[CHECK] 近 N 天 BUY 总数：{total_buy_cnt}")
    logging.info(f"[CHECK] 最新信号日 BUY 数量：{latest_sig_date_cnt}")

    if skipped_weekly:
        logging.info("[CHECK] 周线环境：已跳过。")
        return
    written = weekly_status.get("written", 0)
    if written:
        logging.info(f"[CHECK] 周线指标 OK（写入 {written} 条）。")
    else:
        logging.info("[CHECK] 周线指标缺失，请检查周线指标生成流程。")


def main(
    *,
    skip_fetch: bool = False,
    skip_strategy: bool = False,
    skip_weekly: bool = False,
    asof_date: str | None = None,
) -> None:
    ensure_schema()
    if not skip_fetch:
        AshareApp().run()
    strategy_runner = MA5MA20StrategyRunner()
    if not skip_strategy:
        # 策略是否执行由 config.yaml: strategy_ma5_ma20_trend.enabled 控制
        strategy_runner.run()

    monitor_runner = MA5MA20OpenMonitorRunner()
    repo = monitor_runner.repo
    params = monitor_runner.params
    ready_view = str(params.ready_signals_view or "").strip() or None
    signal_table = str(getattr(strategy_runner.params, "signal_events_table", "") or "").strip()

    weekly_status: dict = {"written": 0}
    skipped_weekly = bool(skip_weekly)
    if not skipped_weekly:
        asof_date = _parse_asof_date(asof_date)
        try:
            weekly_status = run_weekly_market_indicator(
                start_date=asof_date,
                end_date=asof_date,
                mode="incremental",
            )
        except Exception as exc:  # noqa: BLE001
            logging.warning(f"[WARN] 周线环境生成失败：{exc}")

    breakdown = []
    if ready_view and repo._table_exists(ready_view):  # noqa: SLF001
        breakdown = _load_sig_date_breakdown(repo, ready_view, params.strategy_code)
    if not breakdown and signal_table:
        breakdown = _load_sig_date_breakdown(repo, signal_table, params.strategy_code)

    latest_trade_date = repo._resolve_latest_trade_date(ready_view=ready_view)  # noqa: SLF001
    if not latest_trade_date and signal_table:
        latest_trade_date = _resolve_latest_sig_date(repo, signal_table, params.strategy_code)

    latest_sig_date_buy_cnt = breakdown[0]["count"] if breakdown else 0
    buy_cnt_total_recent_n_days = sum(item.get("count", 0) for item in breakdown)

    report = {
        "latest_trade_date": latest_trade_date,
        "latest_sig_date_buy_cnt": latest_sig_date_buy_cnt,
        "buy_cnt_total_recent_n_days": buy_cnt_total_recent_n_days,
        "buy_sig_date_breakdown": breakdown[:7],
        "weekly_env_status": weekly_status if not skipped_weekly else {"skipped": True},
    }

    output_dir = Path("output")
    output_dir.mkdir(parents=True, exist_ok=True)
    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = output_dir / f"prep_report_{ts}.json"
    with report_path.open("w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    logging.info(f"[REPORT] 准备报告：已写入 {report_path.as_posix()}")

    _self_check(
        repo,
        ready_view=ready_view,
        total_buy_cnt=buy_cnt_total_recent_n_days,
        latest_sig_date_cnt=latest_sig_date_buy_cnt,
        weekly_status=weekly_status,
        skipped_weekly=skipped_weekly,
    )


if __name__ == "__main__":
    main()

# 验收自检（最小）
# 1) python -m py_compile ashare/open_monitor_repo.py run_open_monitor_scheduler.py run_index_weekly_channel.py start.py
# 2) 周末/非交易日：python run_open_monitor_scheduler.py --once
#    - monitor_date 回落到最近交易日，open_monitor 可正常构建环境
# 3) python start.py
#    - 报告中的 BUY 总数与 SQL 查询一致，周线 run_id= WEEKLY_{asof_date} 存在

================================================================================
FILE: tests/conftest.py
================================================================================

﻿import os
from typing import Dict

import pytest
from sqlalchemy import create_engine, text

from ashare.db import DatabaseConfig, MySQLWriter


def _mysql_available() -> bool:
    return bool(os.getenv("MYSQL_HOST") and os.getenv("MYSQL_USER"))


@pytest.fixture(scope="session")
def mysql_env() -> Dict[str, object]:
    if not _mysql_available():
        pytest.skip("MYSQL_HOST and MYSQL_USER must be set to run DB tests.")

    original_db = os.environ.get("MYSQL_DB_NAME")
    created = False
    db_name = original_db
    if not db_name:
        db_name = f"ashare_test_{os.getpid()}"
        created = True
        os.environ["MYSQL_DB_NAME"] = db_name

    try:
        yield {"db_name": db_name, "created": created, "original_db": original_db}
    finally:
        if original_db is None:
            os.environ.pop("MYSQL_DB_NAME", None)
        else:
            os.environ["MYSQL_DB_NAME"] = original_db


@pytest.fixture(scope="session")
def mysql_writer(mysql_env):
    cfg = DatabaseConfig.from_env()
    writer = MySQLWriter(cfg)
    try:
        yield writer
    finally:
        writer.dispose()
        if mysql_env["created"]:
            server_engine = create_engine(cfg.server_url(), future=True)
            with server_engine.begin() as conn:
                conn.execute(text(f"DROP DATABASE IF EXISTS `{cfg.db_name}`"))
            server_engine.dispose()

================================================================================
FILE: tests/test_config.py
================================================================================

﻿from pathlib import Path

import pytest

from ashare.config import CONFIG_FILE_ENV, ProxyConfig, get_section, load_config


def _write_config(tmp_path: Path, content: str) -> Path:
    path = tmp_path / "config.yaml"
    path.write_text(content, encoding="utf-8")
    return path


def test_load_config_returns_empty_when_missing(monkeypatch, tmp_path):
    load_config.cache_clear()
    missing = tmp_path / "missing.yaml"
    monkeypatch.setenv(CONFIG_FILE_ENV, str(missing))
    assert load_config() == {}
    load_config.cache_clear()


def test_load_config_from_env_path(monkeypatch, tmp_path):
    path = _write_config(tmp_path, "database:\n  host: 127.0.0.1\n")
    monkeypatch.setenv(CONFIG_FILE_ENV, str(path))
    load_config.cache_clear()
    data = load_config()
    assert data["database"]["host"] == "127.0.0.1"
    load_config.cache_clear()


def test_get_section_raises_on_invalid_type(monkeypatch, tmp_path):
    path = _write_config(tmp_path, "proxy: bad\n")
    monkeypatch.setenv(CONFIG_FILE_ENV, str(path))
    load_config.cache_clear()
    with pytest.raises(ValueError):
        get_section("proxy")
    load_config.cache_clear()


def test_proxyconfig_from_env_and_config(monkeypatch, tmp_path):
    path = _write_config(
        tmp_path,
        "proxy:\n  http: http://cfg\n  https: http://cfgs\n",
    )
    monkeypatch.setenv(CONFIG_FILE_ENV, str(path))
    monkeypatch.setenv("ASHARE_HTTP_PROXY", "http://env")
    monkeypatch.delenv("ASHARE_HTTPS_PROXY", raising=False)
    load_config.cache_clear()
    proxy = ProxyConfig.from_env()
    assert proxy.http == "http://env"
    assert proxy.https == "http://cfgs"
    load_config.cache_clear()

================================================================================
FILE: tests/test_convert.py
================================================================================

﻿from ashare.utils.convert import to_float


def test_to_float_handles_none_and_empty():
    assert to_float(None) is None
    assert to_float("") is None
    assert to_float("-") is None
    assert to_float("None") is None


def test_to_float_parses_numbers():
    assert to_float("1,234.5") == 1234.5
    assert to_float(10) == 10.0
    assert to_float(3.5) == 3.5


def test_to_float_nan_inf():
    assert to_float(float("nan")) is None
    assert to_float(float("inf")) is None

================================================================================
FILE: tests/test_db_schema.py
================================================================================

﻿import pandas as pd
import pytest
from sqlalchemy import inspect, text

from ashare.db import DatabaseConfig, MySQLWriter
from ashare.schema_manager import SchemaManager


@pytest.mark.requires_db
def test_schema_manager_ensure_all(mysql_writer):
    manager = SchemaManager(mysql_writer.engine, db_name=mysql_writer.config.db_name)
    manager.ensure_all()

    inspector = inspect(mysql_writer.engine)
    assert inspector.has_table("history_daily_kline")
    assert inspector.has_table("strategy_signal_events")
    assert inspector.has_table("strategy_open_monitor_eval")


@pytest.mark.requires_db
def test_mysql_writer_write_dataframe(mysql_writer):
    df = pd.DataFrame(
        [
            {"code": "000001", "value": 1.0},
            {"code": "000002", "value": 2.0},
        ]
    )
    table = "test_write_dataframe"
    mysql_writer.write_dataframe(df, table, if_exists="replace")
    with mysql_writer.engine.begin() as conn:
        row = conn.execute(text(f"SELECT COUNT(*) AS cnt FROM `{table}`")).mappings().first()
    assert int(row["cnt"]) == 2

================================================================================
FILE: tests/test_indicator_utils.py
================================================================================

﻿import pandas as pd

from ashare.indicator_utils import consecutive_true


def test_consecutive_true():
    series = pd.Series([True, True, False, True, False, True, True, True])
    result = consecutive_true(series).tolist()
    assert result == [1, 2, 0, 1, 0, 1, 2, 3]

================================================================================
FILE: tests/test_logger.py
================================================================================

﻿import logging

import pytest

from ashare.utils import logger as logger_mod


@pytest.fixture(autouse=True)
def _reset_logger():
    logger = logging.getLogger("ashare")
    for handler in list(logger.handlers):
        logger.removeHandler(handler)
        try:
            handler.close()
        except Exception:
            pass
    if hasattr(logger, "_ashare_configured"):
        delattr(logger, "_ashare_configured")
    logger.setLevel(logging.NOTSET)
    logger.propagate = True
    yield
    for handler in list(logger.handlers):
        logger.removeHandler(handler)
        try:
            handler.close()
        except Exception:
            pass
    if hasattr(logger, "_ashare_configured"):
        delattr(logger, "_ashare_configured")


def test_parse_level():
    assert logger_mod._parse_level("10") == 10
    assert logger_mod._parse_level("info") == logging.INFO
    assert logger_mod._parse_level("notalevel") == logging.INFO


def test_setup_logger_writes_to_dir(monkeypatch, tmp_path):
    monkeypatch.delenv("ASHARE_LOG_DIR", raising=False)
    monkeypatch.delenv("ASHARE_LOG_LEVEL", raising=False)
    monkeypatch.setattr(logger_mod, "get_section", lambda name: {})

    log = logger_mod.setup_logger(log_dir=tmp_path)
    log.info("hello")
    assert (tmp_path / "ashare.log").exists()

================================================================================
FILE: tests/test_ma5_ma20_trend_strategy.py
================================================================================

﻿import pandas as pd

from ashare.ma5_ma20_trend_strategy import _atr, _macd, _rsi, _split_exchange_symbol


def test_split_exchange_symbol():
    assert _split_exchange_symbol("sh.600000") == ("sh", "600000")
    assert _split_exchange_symbol("600000") == ("", "600000")
    assert _split_exchange_symbol("") == ("", "")


def test_macd_and_atr_shapes():
    close = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    dif, dea, hist = _macd(close)
    assert len(dif) == len(close)
    assert len(dea) == len(close)
    assert len(hist) == len(close)

    high = pd.Series([2, 3, 4, 5, 6])
    low = pd.Series([1, 2, 3, 4, 5])
    preclose = pd.Series([1, 2, 3, 4, 5])
    atr = _atr(high, low, preclose)
    assert len(atr) == len(high)


def test_rsi_range():
    close = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 8, 7, 6, 5, 4])
    rsi = _rsi(close)
    assert rsi.dropna().between(0, 100).all()

================================================================================
FILE: tests/test_market_indicator_builder.py
================================================================================

﻿import pandas as pd

from ashare.market_indicator_builder import MarketIndicatorBuilder


def test_resolve_daily_regime_pullback_fast_drop():
    df = pd.DataFrame(
        {
            "status": ["PULLBACK", "RISK_ON"],
            "score_raw": [2, 3],
            "pullback_mode": ["FAST_DROP", None],
        }
    )
    result = MarketIndicatorBuilder._resolve_daily_regime(df)
    assert result["regime"] == "PULLBACK"
    assert abs(result["position_hint"] - 0.3) < 1e-6


def test_resolve_breadth_metrics():
    df = pd.DataFrame(
        {
            "ma20": [1, 1, None],
            "ma60": [1, None, 1],
            "above_ma20": [True, False, False],
            "above_ma60": [True, False, True],
            "risk_off_flag": [True, False, True],
            "macd_hist": [1, 1, 1],
            "dev_ma20_atr": [0.1, 0.2, 0.3],
        }
    )
    result = MarketIndicatorBuilder._resolve_breadth_metrics(df)
    assert abs(result["breadth_pct_above_ma20"] - 0.5) < 1e-6
    assert abs(result["breadth_pct_above_ma60"] - 1.0) < 1e-6
    assert result["breadth_risk_off_ratio"] is not None


def test_resolve_daily_zone_breakdown():
    row = pd.Series({"regime": "BREAKDOWN", "bb_pos": 0.5, "bb_width": 0.1})
    zone = MarketIndicatorBuilder._resolve_daily_zone(row)
    assert zone["daily_zone_id"] == "DZ_BREAKDOWN"
    assert zone["daily_cap_multiplier"] == 0.2

================================================================================
FILE: tests/test_market_regime.py
================================================================================

﻿import pandas as pd

from ashare.market_regime import MarketRegimeClassifier


def test_market_regime_risk_on():
    dates = pd.date_range("2025-01-01", periods=6, freq="D")
    df = pd.DataFrame(
        {
            "code": ["sh.000001"] * len(dates),
            "date": dates,
            "close": [10, 10.5, 11, 11.5, 12, 12.5],
        }
    )
    classifier = MarketRegimeClassifier(breakdown_window=3, effective_breakdown_days=2)
    result = classifier.classify(df, short=2, mid=3, long=4)
    assert result.regime in {"RISK_ON", "PULLBACK"}
    assert result.position_hint is not None

================================================================================
FILE: tests/test_monitor_rules.py
================================================================================

﻿from ashare.monitor_rules import MonitorRuleConfig, build_default_monitor_rules
from ashare.open_monitor_eval import merge_gate_actions
from ashare.open_monitor_rules import DecisionContext, RuleEngine, Rule, RuleResult


def test_monitor_rule_config_normalizes_percentages():
    cfg = MonitorRuleConfig.from_config(
        {
            "max_gap_up_pct": 5,
            "limit_up_trigger_pct": 0.1,
            "enable_gap_up": "false",
        }
    )
    assert abs(cfg.max_gap_up_pct - 0.05) < 1e-6
    assert abs(cfg.limit_up_trigger_pct - 10.0) < 1e-6
    assert cfg.enable_gap_up is False


def test_build_default_rules_quote_missing():
    cfg = MonitorRuleConfig()
    rules = build_default_monitor_rules(cfg, Rule=Rule, RuleResult=RuleResult)
    ctx = DecisionContext(price_now=None)
    engine = RuleEngine(merge_gate_actions)
    engine.apply(ctx, rules)
    result = ctx.export_result()
    assert result.action == cfg.quote_missing_action
    assert cfg.quote_missing_reason in result.action_reason

================================================================================
FILE: tests/test_open_monitor_eval.py
================================================================================

﻿import json

import pandas as pd

from ashare.open_monitor_eval import (
    OpenMonitorEvaluator,
    compute_runup_metrics,
    evaluate_runup_breach,
    merge_gate_actions,
)


def test_merge_gate_actions_priority():
    assert merge_gate_actions("ALLOW", "WAIT") == "WAIT"
    assert merge_gate_actions("GO") == "ALLOW"
    assert merge_gate_actions("stop", "allow") == "STOP"


def test_compute_runup_metrics():
    metrics = compute_runup_metrics(10, asof_close=12, live_high=11, sig_atr14=1)
    assert metrics.runup_ref_price == 12
    assert metrics.runup_ref_source == "max(asof_close, live_high)"
    assert metrics.runup_from_sigclose == 2
    assert metrics.runup_from_sigclose_atr == 2


def test_evaluate_runup_breach():
    metrics = compute_runup_metrics(10, asof_close=12, live_high=12, sig_atr14=1)
    breached, reason = evaluate_runup_breach(metrics, runup_atr_max=1.0, runup_atr_tol=0.1)
    assert breached is True
    assert "runup_from_sigclose_atr" in (reason or "")


def test_extract_rule_hit_names():
    hits = [{"name": "limit_up"}, {"id": "runup_breach"}]
    raw = json.dumps(hits)
    names = OpenMonitorEvaluator._extract_rule_hit_names(raw)
    assert "LIMIT_UP" in names
    assert "RUNUP_BREACH" in names


def test_build_board_map_from_strength():
    df = pd.DataFrame(
        [
            {"board_name": "A", "board_code": "001", "rank": 1, "chg_pct": 2.0},
            {"board_name": "B", "board_code": "002", "rank": 10, "chg_pct": -1.0},
        ]
    )
    board_map = OpenMonitorEvaluator.build_board_map_from_strength(df)
    assert board_map["A"]["status"] == "strong"
    assert board_map["002"]["status"] in {"weak", "neutral"}

================================================================================
FILE: tests/test_open_monitor_rules.py
================================================================================

﻿import json

from ashare.open_monitor_eval import merge_gate_actions
from ashare.open_monitor_rules import (
    DecisionContext,
    MarketEnvironment,
    Rule,
    RuleEngine,
    RuleResult,
    _action_rank,
)


def test_action_rank_priority():
    assert _action_rank("STOP") < _action_rank("WAIT")
    assert _action_rank("SKIP") < _action_rank("EXECUTE")
    assert _action_rank("UNKNOWN") > _action_rank("EXECUTE")


def test_market_environment_from_snapshot():
    snapshot = {
        "env_final_gate_action": " wait ",
        "env_final_cap_pct": "0.5",
        "env_final_reason_json": {"k": "v"},
        "env_index_snapshot_hash": "abc",
        "index_score": "3.2",
        "regime": "RISK_ON",
        "position_hint": "0.7",
        "weekly_asof_trade_date": "2025-01-03",
        "weekly_risk_level": "LOW",
        "weekly_scene_code": "SCENE_A",
    }
    env = MarketEnvironment.from_snapshot(snapshot)
    assert env.gate_action == "WAIT"
    assert env.position_cap_pct == 0.5
    assert env.index_snapshot_hash == "abc"
    assert env.score == 3.2
    assert env.position_hint == 0.7
    assert env.weekly_scene == "SCENE_A"


def test_rule_engine_action_override():
    rules = [
        Rule(
            id="QUOTE_MISSING",
            category="ACTION",
            severity=10,
            predicate=lambda ctx: True,
            effect=lambda ctx: RuleResult(reason="no quote", action_override="SKIP"),
        )
    ]
    ctx = DecisionContext(entry_exposure_cap=1.0)
    engine = RuleEngine(merge_gate_actions)
    engine.apply(ctx, rules)
    result = ctx.export_result()
    assert result.action == "SKIP"
    assert result.state == "INVALID"
    assert result.entry_exposure_cap == 0.0
    hits = json.loads(result.rule_hits_json)
    assert hits[0]["name"] == "QUOTE_MISSING"


def test_env_overlay_wait_promotes_action():
    rules = [
        Rule(
            id="ENV_WAIT",
            category="ENV_OVERLAY",
            severity=5,
            predicate=lambda ctx: True,
            effect=lambda ctx: RuleResult(reason="env", env_gate_action="WAIT"),
        )
    ]
    ctx = DecisionContext(entry_exposure_cap=0.8)
    engine = RuleEngine(merge_gate_actions)
    engine.apply(ctx, rules)
    result = ctx.export_result()
    assert result.action == "WAIT"
    assert result.state == "PENDING"

================================================================================
FILE: tests/test_strategy_candidates_db.py
================================================================================

﻿import datetime as dt

import pandas as pd
import pytest
from sqlalchemy import text

from ashare.strategy_candidates import StrategyCandidatesService


@pytest.mark.requires_db
def test_strategy_candidates_signal_scan(mysql_writer):
    asof_date = dt.date(2025, 1, 10)

    with mysql_writer.engine.begin() as conn:
        conn.execute(
            text(
                """
                CREATE TABLE IF NOT EXISTS history_daily_kline (
                    code VARCHAR(20) NOT NULL,
                    date DATE NOT NULL
                )
                """
            )
        )
        conn.execute(text("DELETE FROM history_daily_kline"))
        conn.execute(
            text(
                """
                INSERT INTO history_daily_kline (code, date)
                VALUES
                  ('sh.000001', '2025-01-10'),
                  ('sh.000001', '2025-01-09'),
                  ('sh.000001', '2025-01-08')
                """
            )
        )
        conn.execute(
            text(
                """
                CREATE TABLE IF NOT EXISTS a_share_top_liquidity (
                    code VARCHAR(20) NOT NULL,
                    date DATE NOT NULL
                )
                """
            )
        )
        conn.execute(text("DELETE FROM a_share_top_liquidity"))
        conn.execute(
            text(
                """
                INSERT INTO a_share_top_liquidity (code, date)
                VALUES ('000001', '2025-01-10')
                """
            )
        )
        conn.execute(
            text(
                """
                CREATE TABLE IF NOT EXISTS strategy_signal_events (
                    code VARCHAR(20) NOT NULL,
                    sig_date DATE NOT NULL,
                    strategy_code VARCHAR(32),
                    signal VARCHAR(32),
                    final_action VARCHAR(32),
                    valid_days INT
                )
                """
            )
        )
        conn.execute(text("DELETE FROM strategy_signal_events"))
        conn.execute(
            text(
                """
                INSERT INTO strategy_signal_events
                    (code, sig_date, strategy_code, signal, final_action, valid_days)
                VALUES
                    ('000001', '2025-01-10', 'MA5_MA20_TREND', 'BUY', NULL, 3)
                """
            )
        )

    service = StrategyCandidatesService(db_writer=mysql_writer)
    df = service._load_signal_candidates(asof_date)
    assert not df.empty
    assert df.iloc[0]["code"] == "000001"

    merged = service._merge_candidates(asof_date, ["000001"], df)
    assert "is_liquidity" in merged.columns
    assert "has_signal" in merged.columns
    assert int(merged.iloc[0]["has_signal"]) == 1

================================================================================
FILE: tests/test_weekly_channel_regime.py
================================================================================

﻿import pandas as pd

from ashare.weekly_channel_regime import _to_weekly_ohlcv, WeeklyChannelClassifier


def test_to_weekly_ohlcv_aggregates():
    df = pd.DataFrame(
        [
            {"code": "sh.000001", "date": "2025-01-06", "open": 1, "high": 2, "low": 1, "close": 2, "volume": 100},
            {"code": "sh.000001", "date": "2025-01-07", "open": 2, "high": 3, "low": 2, "close": 3, "volume": 150},
            {"code": "sh.000001", "date": "2025-01-13", "open": 3, "high": 4, "low": 3, "close": 4, "volume": 200},
            {"code": "sh.000001", "date": "2025-01-14", "open": 4, "high": 5, "low": 4, "close": 5, "volume": 250},
        ]
    )
    wk = _to_weekly_ohlcv(df)
    assert len(wk) == 2
    assert wk.iloc[0]["volume"] == 250
    assert wk.iloc[1]["volume"] == 450


def test_weekly_channel_classifier_returns_payload():
    df = pd.DataFrame(
        [
            {"code": "sh.000001", "date": "2025-01-06", "open": 1, "high": 2, "low": 1, "close": 2, "volume": 100},
            {"code": "sh.000001", "date": "2025-01-07", "open": 2, "high": 3, "low": 2, "close": 3, "volume": 150},
            {"code": "sh.000001", "date": "2025-01-08", "open": 3, "high": 4, "low": 3, "close": 3.5, "volume": 120},
            {"code": "sh.000001", "date": "2025-01-09", "open": 3.5, "high": 4, "low": 3, "close": 3.2, "volume": 130},
            {"code": "sh.000001", "date": "2025-01-10", "open": 3.2, "high": 4, "low": 3, "close": 3.8, "volume": 140},
            {"code": "sh.000001", "date": "2025-01-13", "open": 3.8, "high": 4.2, "low": 3.7, "close": 4.0, "volume": 160},
        ]
    )
    classifier = WeeklyChannelClassifier(lrc_length=3, ma_fast=2, ma_slow=3)
    result = classifier.classify(df)
    assert result.state is not None
    assert "sh.000001" in result.detail

================================================================================
FILE: tests/test_weekly_env_builder.py
================================================================================

﻿from ashare.weekly_env_builder import WeeklyEnvironmentBuilder


def test_merge_gate_actions():
    assert WeeklyEnvironmentBuilder._merge_gate_actions("ALLOW", "WAIT") == "WAIT"
    assert WeeklyEnvironmentBuilder._merge_gate_actions("go") == "ALLOW"


def test_resolve_env_weekly_gate_policy():
    env_context = {
        "weekly_gating_enabled": True,
        "weekly_risk_level": "MEDIUM",
        "weekly_structure_status": "FORMING",
    }
    policy = WeeklyEnvironmentBuilder.resolve_env_weekly_gate_policy(env_context)
    assert policy == "ALLOW_SMALL"


def test_derive_gate_action():
    assert WeeklyEnvironmentBuilder._derive_gate_action("BREAKDOWN", 0.5) == "STOP"
    assert WeeklyEnvironmentBuilder._derive_gate_action("RISK_OFF", 0.5) == "WAIT"
    assert WeeklyEnvironmentBuilder._derive_gate_action("RISK_ON", 0.2) == "ALLOW"


def test_resolve_weekly_zone():
    scenario = {
        "weekly_risk_level": "HIGH",
        "weekly_plan_a_exposure_cap": 0.2,
        "weekly_direction_confirmed": False,
    }
    zone = WeeklyEnvironmentBuilder._resolve_weekly_zone(scenario, "WAIT", "")
    assert zone["weekly_zone_id"] == "WZ0_RISK_OFF"
    assert zone["weekly_zone_score"] == 10

================================================================================
FILE: tests/test_weekly_pattern_system.py
================================================================================

﻿import pandas as pd

from ashare.weekly_pattern_system import _clip_text, _fit_regression, WeeklyPlanSystem


def test_clip_text():
    text = "a b  c"
    assert _clip_text(text, 3) == "a b"


def test_fit_regression():
    slope, intercept = _fit_regression([(0, 1.0), (1, 2.0)])
    assert slope is not None
    assert intercept is not None


def test_prepare_df_sorts_and_adds_columns():
    system = WeeklyPlanSystem()
    bars = [
        {"week_end": "2025-01-10", "open": 2, "high": 3, "low": 1, "close": 2.5, "volume": 100},
        {"week_end": "2025-01-03", "open": 1, "high": 2, "low": 1, "close": 1.5, "volume": 90},
    ]
    df = system._prepare_df(bars)
    assert not df.empty
    assert df.iloc[0]["week_end"] < df.iloc[1]["week_end"]
    assert "ma_fast" in df.columns
    assert "atr14" in df.columns

================================================================================
FILE: tool/export_db_snapshot.py
================================================================================

"""
export_db_snapshot.py

导出 MySQL 数据库快照（表清单 / 字段 / 行数 / 样例数据 / 可选日期范围），用于给 LLM 理解你当前已有的数据。

特点：
- 默认从项目根目录 config.yaml 读取数据库配置（顶层 database.*）
- 也支持通过环境变量 ASHARE_CONFIG_FILE 指定配置文件路径
- 生成两份文件：
  1) tool/output/db_snapshot_YYYYMMDD_HHMMSS.md
  2) tool/output/db_snapshot_YYYYMMDD_HHMMSS.json
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import quote_plus

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

try:
    import yaml  # type: ignore
except Exception as e:  # pragma: no cover
    raise RuntimeError(
        "缺少依赖 pyyaml，无法读取 config.yaml。请先安装：pip install pyyaml"
    ) from e


# ----------------------------
# Config / DB Connect
# ----------------------------

@dataclass
class DBConfig:
    host: str = "127.0.0.1"
    port: int = 3306
    user: str = "root"
    password: str = ""
    db_name: str = "ashare"

    def sqlalchemy_url(self) -> str:
        pwd = quote_plus(self.password) if self.password else ""
        cred = self.user if not pwd else f"{self.user}:{pwd}"
        # charset 兼容中文字段/注释；pool_pre_ping 提高连接稳定性
        return f"mysql+pymysql://{cred}@{self.host}:{self.port}/{self.db_name}?charset=utf8mb4"


def _project_root() -> Path:
    # .../AShare/tool/export_db_snapshot.py -> parents[1] == .../AShare
    return Path(__file__).resolve().parents[1]


def _default_config_path() -> Path:
    root = _project_root()
    # 优先 config.yaml，其次 config.yml
    p1 = root / "config.yaml"
    p2 = root / "config.yml"
    return p1 if p1.exists() else p2


def _load_yaml(path: Path) -> Dict[str, Any]:
    if not path.exists():
        raise FileNotFoundError(f"未找到配置文件：{path}")
    with path.open("r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}
    if not isinstance(data, dict):
        raise ValueError(f"配置文件格式不正确（顶层不是 dict）：{path}")
    return data


def _load_db_config_from_yaml(cfg: Dict[str, Any]) -> DBConfig:
    section = cfg.get("database") or {}
    if not isinstance(section, dict):
        section = {}

    host = os.getenv("MYSQL_HOST") or section.get("host") or "127.0.0.1"
    port_raw = os.getenv("MYSQL_PORT") or section.get("port") or 3306
    user = os.getenv("MYSQL_USER") or section.get("user") or "root"
    password = os.getenv("MYSQL_PASSWORD")
    if password is None:
        password = section.get("password") or ""
    db_name = os.getenv("MYSQL_DB_NAME") or section.get("db_name") or "ashare"

    try:
        port = int(port_raw)
    except Exception:
        port = 3306

    return DBConfig(host=str(host), port=port, user=str(user), password=str(password), db_name=str(db_name))


def _connect_engine(config_path: Optional[Path] = None) -> Tuple[Engine, Path, DBConfig]:
    # 1) 明确传入 --config
    # 2) 环境变量 ASHARE_CONFIG_FILE
    # 3) 项目根目录 config.yaml / config.yml
    env_cfg = os.environ.get("ASHARE_CONFIG_FILE")
    cfg_path = config_path or (Path(env_cfg) if env_cfg else _default_config_path())

    cfg = _load_yaml(cfg_path)
    db_cfg = _load_db_config_from_yaml(cfg)

    url = db_cfg.sqlalchemy_url()
    engine = create_engine(url, future=True, pool_pre_ping=True)

    # quick ping
    with engine.connect() as conn:
        conn.execute(text("SELECT 1"))

    return engine, cfg_path, db_cfg


# ----------------------------
# Snapshot helpers
# ----------------------------

_DATE_CANDIDATE_COLS = [
    "trade_date",
    "date",
    "dt",
    "day",
    "period",
    "statDate",
    "pubDate",
    "publish_date",
    "datetime",
]


def _fetch_tables(engine: Engine, db_name: str) -> List[Dict[str, Any]]:
    """
    使用 information_schema.tables 获取表/视图清单与近似行数、注释。
    """
    sql = text(
        """
        SELECT
            TABLE_NAME AS table_name,
            TABLE_TYPE AS table_type,
            IFNULL(TABLE_ROWS, 0) AS approx_rows,
            IFNULL(TABLE_COMMENT, '') AS table_comment
        FROM information_schema.TABLES
        WHERE TABLE_SCHEMA = :db
        ORDER BY TABLE_NAME
        """
    )
    with engine.connect() as conn:
        df = pd.read_sql(sql, conn, params={"db": db_name})
    return df.to_dict(orient="records")


def _fetch_columns(engine: Engine, db_name: str, table: str) -> List[Dict[str, Any]]:
    sql = text(
        """
        SELECT
            COLUMN_NAME AS column_name,
            COLUMN_TYPE AS column_type,
            IS_NULLABLE AS is_nullable,
            COLUMN_DEFAULT AS column_default,
            IFNULL(COLUMN_COMMENT, '') AS column_comment
        FROM information_schema.COLUMNS
        WHERE TABLE_SCHEMA = :db AND TABLE_NAME = :tbl
        ORDER BY ORDINAL_POSITION
        """
    )
    with engine.connect() as conn:
        df = pd.read_sql(sql, conn, params={"db": db_name, "tbl": table})
    return df.to_dict(orient="records")


def _safe_quote_ident(name: str) -> str:
    # MySQL 反引号转义
    return "`" + name.replace("`", "``") + "`"


def _estimate_row_count(engine: Engine, db_name: str, table: str) -> int:
    # 近似行数已经在 tables 里拿到；这里提供一个兜底的精确 count（可能慢）
    sql = text(f"SELECT COUNT(1) AS cnt FROM {_safe_quote_ident(table)}")
    with engine.connect() as conn:
        row = conn.execute(sql).mappings().first()
    return int(row["cnt"]) if row and row.get("cnt") is not None else 0


def _pick_date_column(columns: List[Dict[str, Any]]) -> Optional[str]:
    col_names = [str(c.get("column_name", "")) for c in columns]
    for cand in _DATE_CANDIDATE_COLS:
        if cand in col_names:
            return cand
    # 再做一次宽松匹配（例如 tradeDate / TRADE_DATE）
    lower_map = {c.lower(): c for c in col_names}
    for cand in _DATE_CANDIDATE_COLS:
        if cand.lower() in lower_map:
            return lower_map[cand.lower()]
    return None


def _fetch_date_range(engine: Engine, table: str, date_col: str) -> Tuple[Optional[str], Optional[str]]:
    sql = text(
        f"SELECT MIN({_safe_quote_ident(date_col)}) AS mn, MAX({_safe_quote_ident(date_col)}) AS mx "
        f"FROM {_safe_quote_ident(table)}"
    )
    with engine.connect() as conn:
        row = conn.execute(sql).mappings().first()

    def _to_str(v: Any) -> Optional[str]:
        if v is None:
            return None
        if isinstance(v, (dt.date, dt.datetime)):
            return v.isoformat()
        return str(v)

    mn = _to_str(row["mn"]) if row else None
    mx = _to_str(row["mx"]) if row else None
    return mn, mx


def _fetch_sample(engine: Engine, table: str, limit: int) -> List[Dict[str, Any]]:
    if limit <= 0:
        return []
    sql = text(f"SELECT * FROM {_safe_quote_ident(table)} LIMIT :n")
    with engine.connect() as conn:
        df = pd.read_sql(sql, conn, params={"n": int(limit)})

    # 统一转成可 JSON 化
    records: List[Dict[str, Any]] = []
    for rec in df.to_dict(orient="records"):
        clean: Dict[str, Any] = {}
        for k, v in rec.items():
            if isinstance(v, (dt.datetime, dt.date)):
                clean[k] = v.isoformat()
            elif pd.isna(v):
                clean[k] = None
            else:
                clean[k] = v
        records.append(clean)
    return records


# ----------------------------
# Export
# ----------------------------

def _write_markdown(out_path: Path, meta: Dict[str, Any], tables: List[Dict[str, Any]]) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        f.write("# Database Snapshot for LLM\n\n")
        f.write(f"- exported_at: {meta.get('exported_at')}\n")
        f.write(f"- config_path: {meta.get('config_path')}\n")
        f.write(f"- db: {meta.get('db_name')}\n")
        f.write(f"- host: {meta.get('host')}:{meta.get('port')}\n")
        f.write(f"- user: {meta.get('user')}\n")
        f.write(f"- tables: {len(tables)}\n\n")

        f.write("## Tables\n\n")
        f.write("| table | type | approx_rows | date_col | date_min | date_max | comment |\n")
        f.write("|---|---:|---:|---|---|---|---|\n")
        for t in tables:
            f.write(
                f"| {t.get('table_name')} | {t.get('table_type')} | {t.get('approx_rows')} | "
                f"{t.get('date_col') or ''} | {t.get('date_min') or ''} | {t.get('date_max') or ''} | "
                f"{(t.get('table_comment') or '').replace('|',' ')} |\n"
            )

        f.write("\n---\n\n")
        for t in tables:
            f.write(f"## {t.get('table_name')}\n\n")
            f.write(f"- type: {t.get('table_type')}\n")
            f.write(f"- approx_rows: {t.get('approx_rows')}\n")
            if t.get("exact_rows") is not None:
                f.write(f"- exact_rows: {t.get('exact_rows')}\n")
            if t.get("table_comment"):
                f.write(f"- comment: {t.get('table_comment')}\n")
            if t.get("date_col"):
                f.write(f"- date_col: {t.get('date_col')}\n")
                f.write(f"- date_min: {t.get('date_min')}\n")
                f.write(f"- date_max: {t.get('date_max')}\n")

            f.write("\n### Columns\n\n")
            f.write("| name | type | nullable | default | comment |\n")
            f.write("|---|---|---:|---|---|\n")
            for c in t.get("columns", []):
                f.write(
                    f"| {c.get('column_name')} | {c.get('column_type')} | {c.get('is_nullable')} | "
                    f"{c.get('column_default') if c.get('column_default') is not None else ''} | "
                    f"{(c.get('column_comment') or '').replace('|',' ')} |\n"
                )

            samples = t.get("sample_rows") or []
            if samples:
                f.write("\n### Sample Rows\n\n")
                f.write("```json\n")
                f.write(json.dumps(samples, ensure_ascii=False, indent=2))
                f.write("\n```\n")

            f.write("\n---\n\n")


def main() -> None:
    parser = argparse.ArgumentParser(description="Export MySQL database snapshot for LLM.")
    parser.add_argument(
        "--config",
        type=str,
        default="",
        help="配置文件路径（默认：环境变量 ASHARE_CONFIG_FILE；否则项目根目录 config.yaml/config.yml）",
    )
    parser.add_argument(
        "--outdir",
        type=str,
        default=str(Path(__file__).resolve().parent / "output"),
        help="输出目录（默认：tool/output）",
    )
    parser.add_argument(
        "--sample-rows",
        type=int,
        default=5,
        help="每张表导出多少行样例（默认：5；设为 0 则不导出样例）",
    )
    parser.add_argument(
        "--max-tables",
        type=int,
        default=0,
        help="最多导出多少张表（0 表示全部）",
    )
    parser.add_argument(
        "--exact-counts",
        action="store_true",
        help="对每张表执行 SELECT COUNT(1) 获取精确行数（可能很慢，不建议大表开启）",
    )
    parser.add_argument(
        "--no-date-range",
        action="store_true",
        help="不计算 MIN/MAX 日期范围（更快）",
    )

    args = parser.parse_args()
    config_path = Path(args.config) if args.config else None

    engine, used_cfg_path, db_cfg = _connect_engine(config_path=config_path)

    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    outdir = Path(args.outdir)
    md_path = outdir / f"db_snapshot_{ts}.md"
    json_path = outdir / f"db_snapshot_{ts}.json"

    meta: Dict[str, Any] = {
        "exported_at": dt.datetime.now().isoformat(timespec="seconds"),
        "config_path": str(used_cfg_path),
        "db_name": db_cfg.db_name,
        "host": db_cfg.host,
        "port": db_cfg.port,
        "user": db_cfg.user,
    }

    print("==== DB Snapshot Export ====")
    print(f"config: {used_cfg_path}")
    print(f"db: {db_cfg.user}@{db_cfg.host}:{db_cfg.port}/{db_cfg.db_name}")
    print(f"outdir: {outdir.resolve()}")
    print("============================")

    tables = _fetch_tables(engine, db_cfg.db_name)

    if args.max_tables and args.max_tables > 0:
        tables = tables[: int(args.max_tables)]

    enriched: List[Dict[str, Any]] = []
    for i, t in enumerate(tables, start=1):
        table_name = str(t.get("table_name"))
        table_type = str(t.get("table_type"))
        approx_rows = int(t.get("approx_rows") or 0)

        print(f"[{i}/{len(tables)}] {table_name} ({table_type}) ...")

        columns = _fetch_columns(engine, db_cfg.db_name, table_name)

        date_col = _pick_date_column(columns)
        date_min = None
        date_max = None
        if (not args.no_date_range) and date_col:
            try:
                date_min, date_max = _fetch_date_range(engine, table_name, date_col)
            except Exception as e:
                # 不中断导出
                date_min, date_max = None, None
                print(f"  [WARN] date range failed: {e}")

        exact_rows = None
        if args.exact_counts:
            try:
                exact_rows = _estimate_row_count(engine, db_cfg.db_name, table_name)
            except Exception as e:
                exact_rows = None
                print(f"  [WARN] exact count failed: {e}")

        sample_rows: List[Dict[str, Any]] = []
        if args.sample_rows and int(args.sample_rows) > 0:
            try:
                sample_rows = _fetch_sample(engine, table_name, int(args.sample_rows))
            except Exception as e:
                sample_rows = []
                print(f"  [WARN] sample fetch failed: {e}")

        enriched.append(
            {
                "table_name": table_name,
                "table_type": table_type,
                "approx_rows": approx_rows,
                "exact_rows": exact_rows,
                "table_comment": t.get("table_comment") or "",
                "columns": columns,
                "date_col": date_col,
                "date_min": date_min,
                "date_max": date_max,
                "sample_rows": sample_rows,
            }
        )

    outdir.mkdir(parents=True, exist_ok=True)

    # JSON
    with json_path.open("w", encoding="utf-8") as f:
        json.dump({"meta": meta, "tables": enriched}, f, ensure_ascii=False, indent=2)

    # Markdown
    _write_markdown(md_path, meta, enriched)

    print("\n✅ 导出完成")
    print(f"- Markdown: {md_path}")
    print(f"- JSON:     {json_path}")


if __name__ == "__main__":
    main()

================================================================================
FILE: tool/export_project.py
================================================================================

import os
from pathlib import Path
from datetime import datetime
from zoneinfo import ZoneInfo

ts = datetime.now(ZoneInfo("Asia/Singapore")).strftime("%Y%m%d_%H%M%S")

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
OUTPUT_DIR = SCRIPT_DIR / "output"
OUTPUT_FILE = OUTPUT_DIR / f"project_for_llm_{ts}.txt"

# 想导出的文件后缀（按需增减）
INCLUDE_EXT = {
    ".py",
    ".toml",
    ".md",
    ".txt",
    ".json",
    ".yml",
    ".yaml",
}

# 想忽略的目录（虚拟环境、git、缓存这些没必要给大模型看）
EXCLUDE_DIRS = {
    ".git",
    ".idea",
    "__pycache__",
    ".pytest_cache",
    "venv",
    ".venv",
    "dbn_trading_auto",
    "output",
}


def should_skip_dir(path: Path) -> bool:
    return any(part in EXCLUDE_DIRS for part in path.parts)


def main() -> None:
    files: list[Path] = []

    output_resolved = OUTPUT_FILE.resolve()
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    for root, dirs, filenames in os.walk(PROJECT_ROOT):
        root_path = Path(root)

        # 过滤不想要的目录
        if should_skip_dir(root_path.relative_to(PROJECT_ROOT)):
            # 清空 dirs，阻止继续往下走
            dirs[:] = []
            continue

        for name in filenames:
            p = root_path / name

            # ✅ 关键：跳过输出文件自身，防止“写的时候又读自己”导致内容重复
            if p.resolve() == output_resolved:
                continue

            if p.suffix.lower() in INCLUDE_EXT:
                files.append(p)

    files.sort()

    with OUTPUT_FILE.open("w", encoding="utf-8") as out:
        out.write(
            f"# Project dump for LLM\n"
            f"# Root: {PROJECT_ROOT}\n"
            f"# Total files: {len(files)}\n"
            f"\n"
        )

        for path in files:
            rel = path.relative_to(PROJECT_ROOT)
            out.write("\n")
            out.write("=" * 80 + "\n")
            out.write(f"FILE: {rel.as_posix()}\n")
            out.write("=" * 80 + "\n\n")

            try:
                with path.open("r", encoding="utf-8") as f:
                    out.write(f.read())
            except UnicodeDecodeError:
                out.write("# [SKIP] 非 UTF-8 文本文件，未导出内容。\n")

    print(f"导出完成，共 {len(files)} 个文件")
    print(f"输出文件: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

================================================================================
FILE: tool/export_zip_project.py
================================================================================

# tool/export_zip_project.py
# Python 3.13+
from __future__ import annotations

import argparse
import hashlib
import json
import os
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from zoneinfo import ZoneInfo
from zipfile import ZIP_DEFLATED, ZipFile, ZipInfo

# === 与旧导出脚本保持一致的“导出范围” ===
INCLUDE_EXT = {
    ".py",
    ".toml",
    ".md",
    ".txt",
    ".json",
    ".yml",
    ".yaml",
}

EXCLUDE_DIRS = {
    ".git",
    ".idea",
    "__pycache__",
    ".pytest_cache",
    "venv",
    ".venv",
    "dbn_trading_auto",
    "output",
}


@dataclass(frozen=True)
class FileEntry:
    rel_path: str
    size: int
    mtime: int
    sha256: str


def should_skip_dir(rel_dir: Path) -> bool:
    # 只要相对路径的任何一段命中 EXCLUDE_DIRS，就整段跳过（与旧脚本一致）
    return any(part in EXCLUDE_DIRS for part in rel_dir.parts)


def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()


def to_posix_relpath(base: Path, p: Path) -> str:
    return p.relative_to(base).as_posix()


def collect_files(project_root: Path, out_zip: Path) -> list[Path]:
    files: list[Path] = []
    project_root = project_root.resolve()
    out_zip_resolved = out_zip.resolve()

    for root, dirs, filenames in os.walk(project_root):
        root_path = Path(root)

        try:
            rel_dir = root_path.relative_to(project_root)
        except ValueError:
            # 理论上不应发生
            dirs[:] = []
            continue

        # 目录过滤：命中就阻止继续深入
        if rel_dir != Path(".") and should_skip_dir(rel_dir):
            dirs[:] = []
            continue

        # 同时把下一层 dirs 里命中的排除目录删掉（加速）
        kept_dirs: list[str] = []
        for d in dirs:
            if d in EXCLUDE_DIRS:
                continue
            kept_dirs.append(d)
        dirs[:] = kept_dirs

        for name in filenames:
            p = root_path / name

            # 跳过符号链接（避免不同机器解压差异）
            try:
                if p.is_symlink():
                    continue
                if not p.is_file():
                    continue
            except OSError:
                continue

            # 跳过输出 zip 自身（极端情况下同名覆盖/重复运行时可避免自包含）
            try:
                if p.resolve() == out_zip_resolved:
                    continue
            except OSError:
                pass

            # 只按后缀白名单导出（与旧脚本一致）
            if p.suffix.lower() not in INCLUDE_EXT:
                continue

            files.append(p)

    files.sort(key=lambda x: to_posix_relpath(project_root, x))
    return files


def zip_write_file(zf: ZipFile, project_root: Path, file_path: Path) -> FileEntry:
    rel_posix = to_posix_relpath(project_root, file_path)
    st = file_path.stat()
    mtime = int(st.st_mtime)
    size = int(st.st_size)
    digest = sha256_file(file_path)

    zi = ZipInfo(rel_posix)
    t = time.localtime(mtime)
    zi.date_time = (max(t.tm_year, 1980), t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)

    # 尝试保存权限位（在 *nix 更有意义，Windows 下也无害）
    try:
        zi.external_attr = (st.st_mode & 0xFFFF) << 16
    except Exception:
        pass

    # 二进制读取写入：不做任何编码/换行处理
    with file_path.open("rb") as f:
        data = f.read()
    zf.writestr(zi, data, compress_type=ZIP_DEFLATED)

    return FileEntry(rel_path=rel_posix, size=size, mtime=mtime, sha256=digest)


def build_manifest(entries: list[FileEntry], project_root: Path, out_zip: Path) -> dict:
    return {
        "tool": "tool/export_zip_project.py",
        "root": str(project_root.resolve()),
        "zip": str(out_zip.resolve()),
        "created_at": datetime.now(ZoneInfo("Asia/Singapore")).isoformat(),
        "python": sys.version,
        "include_ext": sorted(INCLUDE_EXT),
        "exclude_dirs": sorted(EXCLUDE_DIRS),
        "total_files": len(entries),
        "files": [
            {"path": e.rel_path, "size": e.size, "mtime": e.mtime, "sha256": e.sha256}
            for e in entries
        ],
    }


def main() -> int:
    script_dir = Path(__file__).resolve().parent      # .../tool
    project_root = script_dir.parent                  # 项目根
    output_dir = script_dir / "output"                # tool/output

    ts = datetime.now(ZoneInfo("Asia/Singapore")).strftime("%Y%m%d_%H%M%S")
    default_out_zip = output_dir / f"project_export_{ts}.zip"

    ap = argparse.ArgumentParser(
        description="Export project as zip (binary-preserved) with the same scope rules as export_project.py."
    )
    ap.add_argument("--src", type=str, default=str(project_root), help="Project root (default: project root).")
    ap.add_argument("--out", type=str, default=str(default_out_zip), help="Output zip path (default: tool/output/...).")
    ap.add_argument("--compresslevel", type=int, default=6, help="Zip deflate compresslevel 0-9 (default: 6).")
    args = ap.parse_args()

    src = Path(args.src).resolve()
    if not src.exists() or not src.is_dir():
        print(f"[ERROR] --src is not a directory: {src}", file=sys.stderr)
        return 2

    out_zip = Path(args.out).resolve()
    out_zip.parent.mkdir(parents=True, exist_ok=True)

    files = collect_files(src, out_zip)
    entries: list[FileEntry] = []

    with ZipFile(
        out_zip,
        mode="w",
        compression=ZIP_DEFLATED,
        compresslevel=max(0, min(int(args.compresslevel), 9)),
    ) as zf:
        for p in files:
            entries.append(zip_write_file(zf, src, p))

        manifest = build_manifest(entries, src, out_zip)
        zf.writestr("__manifest__.json", json.dumps(manifest, ensure_ascii=False, indent=2).encode("utf-8"))

    print(f"[OK] Wrote zip: {out_zip}")
    print(f"[OK] Files: {len(entries)}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

================================================================================
FILE: tool/test_akshare_network.py
================================================================================

from __future__ import annotations

import argparse
import csv
import json
import os
import time
from contextlib import contextmanager
from dataclasses import asdict, dataclass
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any, Callable

import pandas as pd

try:
    import akshare as ak
except ImportError:
    ak = None


@dataclass
class CaseResult:
    round: int
    case: str
    status: str
    row_count: int | None
    elapsed_ms: float
    error_msg: str | None
    used_date: str | None = None
    tries: int | None = None


@contextmanager
def _temp_env(overrides: dict[str, str | None]):
    """临时覆盖环境变量（用于对比：走代理 vs 不走代理）。"""

    old: dict[str, str | None] = {}
    for k, v in overrides.items():
        old[k] = os.environ.get(k)
        if v is None:
            os.environ.pop(k, None)
        else:
            os.environ[k] = v

    try:
        yield
    finally:
        for k, v in old.items():
            if v is None:
                os.environ.pop(k, None)
            else:
                os.environ[k] = v


def _print_proxy_env() -> None:
    keys = [
        "HTTP_PROXY",
        "HTTPS_PROXY",
        "http_proxy",
        "https_proxy",
        "NO_PROXY",
        "no_proxy",
    ]
    print("代理环境变量：")
    for k in keys:
        v = os.environ.get(k)
        if v:
            print(f"  {k}={v}")
        else:
            print(f"  {k}=<empty>")


def _eastmoney_push2_ping() -> pd.DataFrame:
    """直连测试：东方财富 push2 行情接口（模拟 AkShare 的底层请求）。"""

    try:
        import requests
    except ImportError as exc:  # noqa: BLE001
        raise ImportError("缺少 requests 依赖，无法进行 push2 连通性测试") from exc

    url = "https://82.push2.eastmoney.com/api/qt/clist/get"
    params = {
        "pn": 1,
        "pz": 1,
        "po": 1,
        "np": 1,
        "ut": "bd1d9ddb04089700cf9c27f6f7426281",
        "fltt": 2,
        "invt": 2,
        "fid": "f12",
        "fs": "m:0+t:6,m:0+t:80,m:1+t:2,m:1+t:23",
        "fields": "f12,f13,f14,f2,f3,f4",
    }
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
        "Accept": "application/json,text/plain,*/*",
    }

    resp = requests.get(url, params=params, headers=headers, timeout=10)
    resp.raise_for_status()
    data = resp.json() if resp.content else {}

    diff = (((data or {}).get("data") or {}).get("diff")) or []
    if not diff:
        return pd.DataFrame()

    first = diff[0] or {}
    return pd.DataFrame(
        [
            {
                "f12": first.get("f12"),
                "f13": first.get("f13"),
                "name": first.get("f14"),
                "latest": first.get("f2"),
                "pct": first.get("f3"),
                "chg": first.get("f4"),
            }
        ]
    )


def _ensure_df(value: Any) -> pd.DataFrame:
    if value is None:
        return pd.DataFrame()
    if isinstance(value, pd.DataFrame):
        return value
    try:
        return pd.DataFrame(value)
    except Exception:  # noqa: BLE001
        return pd.DataFrame()


def _count_rows(result: Any) -> int:
    if result is None:
        return 0
    if isinstance(result, pd.DataFrame):
        return int(len(result))
    if isinstance(result, list):
        total = 0
        for item in result:
            if isinstance(item, pd.DataFrame):
                total += len(item)
            else:
                total += len(_ensure_df(item))
        return int(total)
    return int(len(_ensure_df(result)))


def _run_case(func: Callable[[], Any]) -> tuple[str, int | None, float, str | None, Any]:
    t0 = time.perf_counter()
    try:
        result = func()
        elapsed_ms = (time.perf_counter() - t0) * 1000.0

        rows = _count_rows(result)
        if rows == 0:
            return "success_empty", 0, elapsed_ms, None, result

        return "success", rows, elapsed_ms, None, result
    except Exception as exc:  # noqa: BLE001
        elapsed_ms = (time.perf_counter() - t0) * 1000.0
        return "exception", None, elapsed_ms, repr(exc), None


def _date_str(d: date) -> str:
    return d.strftime("%Y-%m-%d")


def _date_compact(d: date) -> str:
    return d.strftime("%Y%m%d")


def _try_with_date_fallback(
    name: str,
    date_list: list[date],
    call_factory: Callable[[date], Callable[[], Any]],
) -> tuple[CaseResult, Any]:
    last_result: Any = None
    for i, d in enumerate(date_list, start=1):
        status, rows, elapsed_ms, err, result = _run_case(call_factory(d))
        last_result = result

        if status == "success":
            return (
                CaseResult(
                    round=0,
                    case=f"{name}_{_date_compact(date_list[0])}",
                    status=status,
                    row_count=rows,
                    elapsed_ms=elapsed_ms,
                    error_msg=None,
                    used_date=_date_str(d),
                    tries=i,
                ),
                result,
            )

        # 如果是空数据：有些接口当天确实可能为空，这里不当作异常；但如果是第一个日期为空，也继续尝试回退
        if status == "success_empty" and i < len(date_list):
            continue

        # 异常：继续尝试回退日期
        if status == "exception" and i < len(date_list):
            continue

        # 最后一次（无论 success_empty 还是 exception）
        return (
            CaseResult(
                round=0,
                case=f"{name}_{_date_compact(date_list[0])}",
                status=status,
                row_count=rows,
                elapsed_ms=elapsed_ms,
                error_msg=err,
                used_date=_date_str(d),
                tries=i,
            ),
            last_result,
        )

    # 理论上不会到这里
    return (
        CaseResult(
            round=0,
            case=f"{name}_{_date_compact(date_list[0])}",
            status="exception",
            row_count=None,
            elapsed_ms=0.0,
            error_msg="unknown",
            used_date=None,
            tries=None,
        ),
        None,
    )


def build_cases(
    test_day: date,
    test_symbol: str,
    *,
    with_spot: bool = False,
) -> list[tuple[str, Callable[[], Any]]]:
    """构造测试用例列表。

    - 默认只测试“日频/数据中心类接口”。
    - 当 with_spot=True 时，额外测试“实时行情”接口（会更慢、更容易触发限流）。
    """

    if ak is None:
        raise ImportError("akshare 未安装，无法运行 AkShare 自检脚本")

    cases: list[tuple[str, Callable[[], Any]]] = []

    # -------------------------
    # 网络连通性诊断：走代理 vs 不走代理
    # -------------------------
    no_proxy_hosts = "82.push2.eastmoney.com,push2.eastmoney.com,eastmoney.com"

    def push2_env_case() -> Any:
        return _eastmoney_push2_ping()

    def push2_no_proxy_case() -> Any:
        # 清掉常见代理环境变量，同时设置 no_proxy/NO_PROXY 让 requests/urllib 走直连
        with _temp_env(
            {
                "HTTP_PROXY": None,
                "HTTPS_PROXY": None,
                "http_proxy": None,
                "https_proxy": None,
                "NO_PROXY": no_proxy_hosts,
                "no_proxy": no_proxy_hosts,
            }
        ):
            return _eastmoney_push2_ping()

    cases.append(("eastmoney_push2_ping_env", push2_env_case))
    cases.append(("eastmoney_push2_ping_no_proxy", push2_no_proxy_case))

    # 这几个接口依赖“交易日”。遇到周末/节假日时，仅回退 1 天仍可能不是交易日；
    # 这里统一回退 7 天，尽量命中最近一个交易日。
    trade_day_candidates = [test_day - timedelta(days=i) for i in range(0, 7)]

    def lhb_case() -> CaseResult:
        res, _ = _try_with_date_fallback(
            name="lhb_detail_em",
            date_list=trade_day_candidates,
            call_factory=lambda d: (
                lambda: ak.stock_lhb_detail_em(
                    start_date=_date_compact(d), end_date=_date_compact(d)
                )
            ),
        )
        return res

    def margin_sse_case() -> CaseResult:
        res, _ = _try_with_date_fallback(
            name="margin_detail_sse",
            date_list=trade_day_candidates,
            call_factory=lambda d: (lambda: ak.stock_margin_detail_sse(date=_date_compact(d))),
        )
        return res

    # SZSE：允许当天为空，不强制回退（但也提供一次回退尝试）
    def margin_szse_case() -> CaseResult:
        res, _ = _try_with_date_fallback(
            name="margin_detail_szse",
            date_list=trade_day_candidates,
            call_factory=lambda d: (lambda: ak.stock_margin_detail_szse(date=_date_compact(d))),
        )
        return res

    cases.append((f"lhb_detail_em_{_date_compact(test_day)}", lhb_case))
    cases.append((f"margin_detail_sse_{_date_compact(test_day)}", margin_sse_case))
    cases.append((f"margin_detail_szse_{_date_compact(test_day)}", margin_szse_case))

    cases.append(("gdhs_summary_latest", lambda: ak.stock_zh_a_gdhs(symbol="最新")))
    cases.append((f"gdhs_detail_{test_symbol}", lambda: ak.stock_zh_a_gdhs_detail_em(symbol=test_symbol)))

    # 新增：用项目里的 AkshareDataFetcher 跑一遍 batch_get_shareholder_count_detail
    def fetcher_batch_case() -> Any:
        # 为了支持“直接运行这个脚本文件”，这里把项目根目录塞进 sys.path
        import sys

        root = Path(__file__).resolve().parents[1]
        if str(root) not in sys.path:
            sys.path.insert(0, str(root))

        from ashare.akshare_fetcher import AkshareDataFetcher  # noqa: WPS433

        fetcher = AkshareDataFetcher()
        return fetcher.batch_get_shareholder_count_detail([test_symbol])

    cases.append((f"fetcher_batch_gdhs_detail_{test_symbol}", fetcher_batch_case))

    # -------------------------
    # 可选：实时行情（会更慢、更容易被限流）
    # -------------------------
    if with_spot:
        cases.append(("spot_zh_a_spot_em_env", lambda: ak.stock_zh_a_spot_em()))

        def spot_no_proxy_case() -> Any:
            with _temp_env(
                {
                    "HTTP_PROXY": None,
                    "HTTPS_PROXY": None,
                    "http_proxy": None,
                    "https_proxy": None,
                    "NO_PROXY": no_proxy_hosts,
                    "no_proxy": no_proxy_hosts,
                }
            ):
                return ak.stock_zh_a_spot_em()

        cases.append(("spot_zh_a_spot_em_no_proxy", spot_no_proxy_case))

    return cases


def main() -> None:
    parser = argparse.ArgumentParser(
        prog="test_akshare_network",
        description=(
            "AkShare 接口稳定性自检脚本：包含数据中心类接口 +（可选）实时行情接口，"
            "并提供东方财富 push2 连通性（走代理/不走代理）对比测试。"
        ),
    )
    parser.add_argument("--rounds", type=int, default=1, help="测试轮数（默认 1）")
    parser.add_argument("--symbol", type=str, default="600000", help="测试股票代码（默认 600000）")
    parser.add_argument(
        "--with-spot",
        action="store_true",
        help="额外测试 AkShare 实时行情接口（stock_zh_a_spot_em，较慢且可能触发限流）",
    )
    parser.add_argument(
        "--sleep",
        type=float,
        default=0.5,
        help="每轮之间休眠秒数（默认 0.5，避免频繁请求触发限流）",
    )

    args = parser.parse_args()

    rounds = max(1, int(args.rounds))
    test_day = date.today()
    test_symbol = str(args.symbol).strip() or "600000"
    with_spot = bool(args.with_spot)
    sleep_s = max(0.0, float(args.sleep))

    print("==== AkShare 接口稳定性自检 ====")
    print(f"执行时间：{datetime.now():%Y-%m-%d %H:%M:%S}")
    print(f"测试轮数：{rounds}")
    print(f"实时行情：{'ON' if with_spot else 'OFF'}")
    print("================================\n")

    _print_proxy_env()
    print(
        "\n提示：如果你在实时行情/东财接口看到 ProxyError，通常是系统/环境代理不可用或被断开；"
        "脚本里会对比 env vs no_proxy 两种方式，帮助你快速定位问题。\n"
    )
    print(f"测试日期：{_date_str(test_day)}（用于龙虎榜/两融接口）")
    print(f"测试股票：{test_symbol}（用于股东户数明细接口）\n")

    out_dir = Path(__file__).resolve().parents[1] / "output"
    out_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = out_dir / f"akshare_network_report_{timestamp}.csv"
    json_path = out_dir / f"akshare_network_report_{timestamp}.json"

    all_results: list[CaseResult] = []

    for r in range(1, rounds + 1):
        print(f"\n===== Round {r}/{rounds} =====")

        cases = build_cases(test_day=test_day, test_symbol=test_symbol, with_spot=with_spot)
        for name, fn in cases:
            # 兼容两种：普通接口（返回 DF）/ 特殊接口（返回 CaseResult）
            print(f"[RUN ] {name} ... ", end="", flush=True)

            t0 = time.perf_counter()
            try:
                maybe = fn()
            except Exception as exc:  # noqa: BLE001
                elapsed_ms = (time.perf_counter() - t0) * 1000.0
                result = CaseResult(
                    round=r,
                    case=name,
                    status="exception",
                    row_count=None,
                    elapsed_ms=elapsed_ms,
                    error_msg=repr(exc),
                    used_date=None,
                    tries=None,
                )
                all_results.append(result)
                print(f"exception (error={result.error_msg})")
                continue

            if isinstance(maybe, CaseResult):
                # 日期回退那种 case
                maybe.round = r
                all_results.append(maybe)
                if maybe.status == "success":
                    print(f"success (rows={maybe.row_count}, used_date={maybe.used_date}, tries={maybe.tries})")
                elif maybe.status == "success_empty":
                    print(f"success_empty (rows=0, used_date={maybe.used_date}, tries={maybe.tries})")
                else:
                    print(f"exception (used_date={maybe.used_date}, tries={maybe.tries}, error={maybe.error_msg})")
                continue

            # 普通 case：统计行数 + 状态
            status, rows, elapsed_ms, err, _ = _run_case(lambda: maybe)
            result = CaseResult(
                round=r,
                case=name,
                status=status,
                row_count=rows,
                elapsed_ms=elapsed_ms,
                error_msg=err,
                used_date=None,
                tries=1,
            )
            all_results.append(result)

            if status == "success":
                print(f"success (rows={rows})")
            elif status == "success_empty":
                print("success_empty (rows=0)")
            else:
                print(f"exception (error={err})")

        if sleep_s > 0 and r < rounds:
            time.sleep(sleep_s)

    with json_path.open("w", encoding="utf-8") as f:
        json.dump([asdict(x) for x in all_results], f, ensure_ascii=False, indent=2)

    # 总结
    ok = sum(1 for x in all_results if x.status in ("success", "success_empty"))
    fail = sum(1 for x in all_results if x.status == "exception")
    print("\n================================")
    print(f"完成：ok={ok}, fail={fail}")
    print(f"CSV: {csv_path}")
    print(f"JSON: {json_path}")
    print("================================")


if __name__ == "__main__":
    main()

================================================================================
FILE: tool/test_baostock_network.py
================================================================================

from __future__ import annotations

"""
Baostock 核心接口网络与稳定性测试脚本

用途：
- 登录 Baostock，一次性跑一圈常用 A 股接口；
- 记录每个接口是否成功、耗时、返回行数；
- 把结果保存到当前目录下 output/baostock_core_ashare_test_*.csv，方便长期对比。

使用方法（在项目根目录）：
    python -m ashare.test_baostock_network --loops 3
或在 ashare 目录下：
    python test_baostock_network.py --loops 3
"""

import argparse
import time
import traceback
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List

import baostock as bs
import pandas as pd


def _resultset_to_df(rs: Any) -> pd.DataFrame:
    """通用 ResultSet -> DataFrame 转换。"""
    data_list: List[List[str]] = []
    # Baostock 的错误码是字符串，'0' 表示成功
    while (getattr(rs, "error_code", None) == "0") and rs.next():
        data_list.append(rs.get_row_data())
    fields = getattr(rs, "fields", None)
    if fields is not None:
        return pd.DataFrame(data_list, columns=fields)
    return pd.DataFrame(data_list)


def _run_case(
    name: str,
    func: Callable[..., Any],
    kwargs: Dict[str, Any],
    round_idx: int,
) -> Dict[str, Any]:
    """执行单个接口测试，捕获异常并统计耗时、行数等信息。"""
    t0 = time.perf_counter()
    status = "unknown"
    error_code = None
    error_msg = ""
    row_count = None

    try:
        rs = func(**kwargs)
    except Exception as e:  # 网络异常、参数错误等
        elapsed_ms = (time.perf_counter() - t0) * 1000
        return {
            "round": round_idx,
            "case": name,
            "status": "exception",
            "error_code": None,
            "error_msg": repr(e),
            "row_count": None,
            "elapsed_ms": round(elapsed_ms, 2),
        }

    # 尝试读取 error_code / error_msg
    error_code = getattr(rs, "error_code", None)
    error_msg = getattr(rs, "error_msg", "")

    try:
        df = _resultset_to_df(rs)
        row_count = len(df)
        status = "success" if error_code == "0" else "api_error"
    except Exception as e:
        status = "read_failed"
        error_msg = f"{error_msg}; read_failed={repr(e)}"

    elapsed_ms = (time.perf_counter() - t0) * 1000
    return {
        "round": round_idx,
        "case": name,
        "status": status,
        "error_code": error_code,
        "error_msg": error_msg,
        "row_count": row_count,
        "elapsed_ms": round(elapsed_ms, 2),
    }


def build_test_cases() -> List[Dict[str, Any]]:
    """构造一批典型 A 股接口的测试用例。"""
    today = date.today()
    end = today.strftime("%Y-%m-%d")
    start_1y = (today - timedelta(days=365)).strftime("%Y-%m-%d")
    start_30d = (today - timedelta(days=30)).strftime("%Y-%m-%d")
    start_5y = (today - timedelta(days=365 * 5)).strftime("%Y-%m-%d")
    # 基本面数据用稍微滞后的完整财报年份，这样成功概率更高
    fin_year = today.year - 2
    if fin_year < 2007:
        fin_year = 2007

    cases: List[Dict[str, Any]] = [
        # --- 核心：证券列表 / 基本信息 ---
        {
            "name": "query_all_stock_today",
            "func": bs.query_all_stock,
            "kwargs": {},
        },
        {
            "name": "query_stock_basic_sh600000",
            "func": bs.query_stock_basic,
            "kwargs": {"code": "sh.600000"},
        },
        # --- 核心：指数 & 股票 K 线 ---
        {
            "name": "history_k_sh000001_daily_1y",
            "func": bs.query_history_k_data_plus,
            "kwargs": {
                "code": "sh.000001",
                "fields": "date,code,open,high,low,close,preclose,volume,amount,pctChg",
                "start_date": start_1y,
                "end_date": end,
                "frequency": "d",
            },
        },
        {
            "name": "history_k_sz000001_5min_30d",
            "func": bs.query_history_k_data_plus,
            "kwargs": {
                "code": "sz.000001",
                # 分钟线字段：注意多了 time，去掉 preclose、pctChg
                "fields": "date,time,code,open,high,low,close,volume,amount,adjustflag",
                "start_date": start_30d,
                "end_date": end,
                "frequency": "5",
            },
        },
        # --- 交易日历 ---
        {
            "name": "trade_dates_last_5y",
            "func": bs.query_trade_dates,
            "kwargs": {"start_date": start_5y, "end_date": end},
        },
        # --- 财务数据：盈利 / 成长 / 偿债 / 现金流 ---
        {
            "name": "profit_data_sh600000",
            "func": bs.query_profit_data,
            "kwargs": {"code": "sh.600000", "year": str(fin_year), "quarter": 4},
        },
        {
            "name": "growth_data_sh600000",
            "func": bs.query_growth_data,
            "kwargs": {"code": "sh.600000", "year": str(fin_year), "quarter": 4},
        },
        {
            "name": "balance_data_sh600000",
            "func": bs.query_balance_data,
            "kwargs": {"code": "sh.600000", "year": str(fin_year), "quarter": 4},
        },
        {
            "name": "cash_flow_data_sh600000",
            "func": bs.query_cash_flow_data,
            "kwargs": {"code": "sh.600000", "year": str(fin_year), "quarter": 4},
        },
    ]

    # 某些环境可能没有升级到最新 baostock，个别接口不存在时在运行阶段兜底
    return cases


def main(argv: List[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description="Baostock 接口网络 / 稳定性测试脚本")
    parser.add_argument(
        "--loops",
        type=int,
        default=1,
        help="重复测试轮数（默认 1 轮，建议 3~5 轮观察稳定性）",
    )
    args = parser.parse_args(argv)

    print("==== Baostock 接口稳定性自检 ====")
    print(f"执行时间：{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"测试轮数：{args.loops}")
    print("================================\n")

    # 登录
    print("登录 Baostock ...")
    lg = bs.login()
    print(f"login error_code={lg.error_code}, error_msg={lg.error_msg}")
    if lg.error_code != "0":
        print("登录失败，无法继续测试，请检查网络或 baostock 安装。")
        return

    all_results: List[Dict[str, Any]] = []
    cases = build_test_cases()

    try:
        for round_idx in range(1, args.loops + 1):
            print(f"\n===== Round {round_idx}/{args.loops} =====")
            for case in cases:
                name = case["name"]
                func = case["func"]
                kwargs = case["kwargs"]

                # 防止老版本 baostock 缺少某些接口
                if not callable(func):
                    print(f"[SKIP] {name}: 接口不可用（非可调用对象）")
                    continue

                print(f"[RUN ] {name} ...", end="", flush=True)
                try:
                    result = _run_case(name, func, kwargs, round_idx)
                except Exception:
                    # 理论上 _run_case 已经兜底，这里只是双保险
                    traceback.print_exc()
                    continue

                all_results.append(result)
                print(
                    f" {result['status']} "
                    f"(code={result['error_code']}, rows={result['row_count']}, "
                    f"{result['elapsed_ms']} ms)"
                )
    finally:
        print("\n登出 Baostock ...")
        bs.logout()

    if not all_results:
        print("没有任何结果，可能所有接口都执行失败。")
        return

    df = pd.DataFrame(all_results)
    print("\n===== 汇总统计 =====")
    print(df["status"].value_counts())
    print("\n按接口维度统计：")
    print(df.groupby(["case", "status"]).size())

    base_dir = Path(__file__).resolve().parent
    out_dir = base_dir / "output"
    out_dir.mkdir(exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = out_dir / f"baostock_core_ashare_test_{ts}.csv"
    df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"\n详细结果已保存到: {out_path}")


if __name__ == "__main__":
    main()
